<!DOCTYPE html>
<!--[if lt IE 7 ]>
<html class="ie ie6" lang="en" class="no-js"> <![endif]-->
<!--[if IE 7 ]>
<html class="ie ie7" lang="en" class="no-js"> <![endif]-->
<!--[if IE 8 ]>
<html class="ie ie8" lang="en" class="no-js"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!-->
<html lang="en" class="no-js"> <!--<![endif]-->
<head>
    <meta charset="utf-8">
    <title>PyCon.DE 2017 Demystifying Deep Learning Frameworks : Generic Function Approximation using Tensorflow</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="PyCon.DE 2017 Karlsruhe - is the traditional meeting point for people who work and play with Python. Hosted in Germany and held in the English language, the conference welcomes Python professionals and enthusiasts from Central Europe and around the world.">
    <meta name="author" content="PyCon.DE">

    <!-- Styles -->
    <link href="../../../assets/css/bootstrap.css?h=bc7d127a" rel="stylesheet">
    <link href="../../../assets/css/main.css?h=5452214a" rel="stylesheet">

    <!-- Google Fonts -->
    <link href='//fonts.googleapis.com/css?family=Dosis:300,400,500,600,700' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Open+Sans:400italic,300,400,700' rel='stylesheet'
          type='text/css'>

    <!-- IE 9 Fallback-->
    <!--[if IE 9]>
    <link href="" rel="stylesheet">
    <![endif]-->

    <!-- HTML5 shim, for IE6-8 support of HTML5 elements and IE Fallback-->
    <!--[if lt IE 9]>
    <script src=""></script>
    <![endif]-->

    <!-- Fav and touch icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="../../../assets/ico/apple-touch-icon-144-precomposed.png?h=15ea1e3b">
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="../../../assets/ico/apple-touch-icon-114-precomposed.png?h=88450737">
    <link rel="apple-touch-icon-precomposed" sizes="72x72" href="../../../assets/ico/apple-touch-icon-72-precomposed.png?h=92748be5">
    <link rel="apple-touch-icon-precomposed" href="../../../assets/ico/apple-touch-icon-57-precomposed.png?h=9375b3f7">
    <link rel="shortcut icon" href="../../../assets/ico/favicon.ico?h=80408e57">

<!-- place this in your head tag -->
<script src='https://js.tito.io/v1' async></script>


<!-- for basic styles include a link to our stylesheet. -->
<!-- fancy something more adventurous? Copy it and roll your own! -->
<link rel="stylesheet" type="text/css" href='https://css.tito.io/v1.1' />
<link rel="alternate" title="Atom Feed for PyConDE 2017" type="application/atom+xml" href="/feed.xml">

</head>
<body id="top" class="fullscreen-image-background">

<!-- NAVBAR -->
<div class="navbar navbar-fixed-top navbar-light">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-nav">
                <span class="sr-only">Menu</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="../../../" class="navbar-brand">
                PyCon.DE 2017 &amp; PyData Karlsruhe
            </a>
        </div>
        <nav class="navbar-collapse collapse" id="main-nav" role="navigation">
            <ul class="nav navbar-nav navbar-right">



  <li><a href="../../../blog/">NEWS</a></li>

  <li class="active"><a href="../../../schedule/">SCHEDULE</a></li>

  <li><a href="../../../tickets/">TICKETS</a></li>

  <li><a href="../../../venue/">VENUE</a></li>

  <li><a href="../../../code-of-conduct/">CODE OF CONDUCT</a></li>

  <li><a href="../../../sponsoring/">SPONSORING</a></li>

  <li><a href="../../../team/">TEAM</a></li>


            </ul>
        </nav>
    </div>
</div>
<!-- END NAVBAR -->

<!-- BODY CONTENT -->



<!-- MAIN CONTENT -->
<div id="main-content" class="main-content">

    <div class="section">
        <div class="container">
            <div class="section-content">
            <!-- place this where you want the widget to appear -->
            <div class="row blog-content">
                <h1>Demystifying Deep Learning Frameworks : Generic Function Approximation using Tensorflow</h1>
<p><div class="avatar">
<img src="https://secure.gravatar.com/avatar/b783d342a504b68f376b5cc4893edfc9?s=500" alt="">
<strong><a href="https://www.linkedin.com/in/siddrai/">Siddhartha Rai</a></strong></p>
<p>Siddhartha is an innovate Data Scientist and Engineer with a strong background in managing technical teams. He is solution driven and goal focussed. He has built strong business partnerships and products in his 15 + year career in software development and research. He is passionate about using the predictive power of data to solve complex industry problem and create robust business processes. He is experienced in several machine learning techniques and algorithms and related data engineering pipelines. Siddhartha excels at analyzing copious amounts of data, communicating its significance and impact to the business, designing and developing impactful predictive models, tools and products to ensure spectacular business results. Siddhartha has led and mentored A-star teams in financial and retail sectors. He is passionate about technology and believes that current business processes can leverage and benefit significantly from the use of data.
</div></p>
<h2>Abstract</h2>
<p><em>Tags:</em> analytics python data-science machine learning ai deep learning analytics python data-science machine learning</p>
<p>This talk attempts to demystify some aspects of Deep Learning using Tensorflow, a popular deep learning framework. Under the hood, Tensorflow is essentially a way to approximate functions. We show how Tensorflow can be used as a generic function approximation tool, and some approaches to tune it.</p>
<h2>Description</h2>
<h1>Tensorflow as a function approximation tool</h1>
<p>Neural networks(and deep learning networks) are essentially tools for function approximation. A <em>function approximator</em> is a tool that given training data X (consisting of multiple independent variables), and output data y (a single output variable), approximates the relationship between X and y as a mathematical function. In this talk, we show how Tensorflow can be used as a function approximation tool, and some approaches for tuning Tensorflow for this purpose.</p>
<h2>Modeling Function Approximation in Tensorflow</h2>
<p>Given training data X, and a output value y, we wish to learn the <em>best</em>  mathematical relationship  between X and y, The term <em>best</em> here is a relative term, and depends on how <em>different</em> the output from this function(y_hat) is as compared to the actual output y. In this talk we show how to use Tensorflow, a deep learning framework, to model arbitrary functions.</p>
<p>For example, given a single training variable x, and an output variable y, the relationship between x and y <em>could</em> be linear, e.g. <em>y + a x + b</em>, or it <em>could</em> be more complex e.g. <em>y = sin(π x) + cos(π x/2) + sin(π x/4)  + cos (π x/8)</em>. One of the key goals while developing a generic function approximation model is to ensure that the model does not make any assumption on the relationship between X, and y, and instead <em>learns</em> this relationship as part of the model training.</p>
<h2>Modeling Approach</h2>
<p>We use the following approach to learn the relationship between X and y in Tensorflow-</p>
<ol>
<li>We first define the structure of the deep learning network that we would like to use to model the generic relationship between X, and y. </li>
<li>We define the activation functions that we would need to use in the model</li>
<li>We then define the criteria for measuring the difference between the predicted output and the actual output </li>
<li>We define the approach used for solving the optimization problem defined in the above steps </li>
<li>We compute the output of our function approximator using Tensorflow primitives</li>
</ol>
<h2>Tuning Choices </h2>
<p>Each of the above stages presents several tuning choices, for example -</p>
<ol>
<li>How deep and how wide should the network be</li>
<li>Which activation function to use</li>
<li>Which optimization method to use for arriving at the best function approximation</li>
</ol>
<h2>Implementation </h2>
<p>The following is an implementation of a generic function approximator that takes inputs _in<em>x</em>, and _in<em>y</em>, and <em>learns</em> a deep network to model the relationship between _in<em>x</em> and _in<em>y</em>. The parameters _layer1_output, layer2_output,layer3<em>output</em> define the structure of the deep network, and the tuning parameter <em>iterations</em> is the number of iterations of the optimiztaion algorithm that are executed, before the output is evaluated. The optimization technique used can also be parameterized, for example,  it could be a simple <em>GradientDescentOptimizer</em>, or  <em>AdamOptimizer</em>, each of which have further tuning parameters.</p>
<pre><code>def eval_multi_stage(layer1_output, layer2_output,layer3_output, iterations, in_x, in_y):
    in_x = np.reshape(in_x, (-1,1))
    in_y = np.reshape(in_y, (-1,1))
    sess = tf.InteractiveSession()
    x = tf.placeholder(tf.float32, shape=[None, 1], name = "x12")
    y_ = tf.placeholder(tf.float32, shape=[None, 1], name = "y12_")
    W_fc1 = weight_variable([1, layer1_output])
    b_fc1 = bias_variable([layer1_output])
    y_1 = tf.nn.relu(tf.matmul(x, W_fc1) + b_fc1)
    W_fc2 = weight_variable([layer1_output, layer2_output])
    b_fc2 = bias_variable([layer2_output])
    y_2 = tf.nn.relu(tf.matmul(y_1, W_fc2) + b_fc2)
    W_fc3 = weight_variable([layer2_output, layer3_output])
    b_fc3 = bias_variable([layer3_output])
    y_3 = tf.matmul(y_2, W_fc3) + b_f3
    cross_entropy = tf.reduce_mean(tf.square(in_y - y_3))
    train_step = tf.train.AdamOptimizer(epsilon = 1e-5, learning_rate=0.01).minimize(cross_entropy) 
    sess.run(tf.initialize_all_variables())
    for i in range(iterations):
        train_step.run(feed_dict={x:in_x, y_: in_y})
    y_hat = output.eval(feed_dict={x:in_x , y_: in_y})
    return np.reshape(y_hat, [-1])
</code></pre>
<h2>Results</h2>
<p>The results of approximating a mathematical function are best described in terms of -</p>
<ol>
<li>How closely the approximated output matches the given input. </li>
<li>The effect that the different tuning choices described above have on the quality of the approximation</li>
<li>The effect that the tuning choices have on the computation time spent before the solution converges </li>
</ol>
<p>The results from applying these functions on different types of functions are shown in the following links (please click for the plots) -</p>
<ul>
<li><a href="https://drive.google.com/file/d/0B-oAF_WTZ7kGUG5rZExSMTVqY28/view?usp=sharing">  Cubic function approximated by a single layer neural network</a> </li>
</ul>
<p>The first, and the simplest approximation that we demonstrate is that of a cubic function approximated by a single stage neural network. The cubic function that captures the actual relationship in the training set is <em>y = (x - 2) <em> (x - 4) </em> (x-6)</em>. As expected, the approximation is linear (the green line). The input used for training, here is represented by the blue curve, a.k.a <em>Actual Data</em>. The results of approximating using a single layer neural network is represented by the line <em>Fitted Function</em>. The line represented by <em>Fitted Function</em> is therefore an approximation of the above cubic function that is <em>learnt</em> by the deep network based on the data that it was trained on.</p>
<ul>
<li><a href="https://drive.google.com/file/d/0B-oAF_WTZ7kGdW1XREJDR01aOFk/view?usp=sharing"> Cubic Function Approximated by a 3 layer deep Neural Network </a> </li>
</ul>
<p>This curve shows the results of approximating our cubic function using a neural network that is 3 layers deep. Here we see that the blue curve of actual values is almost hidden by the approximation function - the red curve(deep learning network's oputput) of approximated values. The approximation here is done using a 3 layer network, whose first 2 stages have 32 nodes each.  This example suggests that the network defined here has done an <em>almost perfect</em> job at learning the function using the training data that it was provided.</p>
<ul>
<li><a href="https://drive.google.com/file/d/0B-oAF_WTZ7kGZ3VNU194eFl4UUE/view?usp=sharing">Exponential Function Approximation</a></li>
</ul>
<p>We extend the example, by approximating an exponential function<em>(y = exp(x))</em> using the same network as above.  The same 3 layer deep network, does a good job at approximating the exponential function as well. Here again, the number of nodes in the first 2 layers is 32.</p>
<ul>
<li><a href="https://drive.google.com/file/d/0B-oAF_WTZ7kGaHZ1SHpwVmJzeU0/view?usp=sharing">Quassi-periodic Function Approximation (Trend)</a>  </li>
</ul>
<p>We use the same network as in the second example, now to approximate a quassi-periodic function. The relationship in the training set is _ y = sin(π x) + cos(π x/2) + sin(π x/4)  + cos (π x/8). This quassi-periodic function here , for example, could represent a time-series function with <em>seasonality</em> and <em>trend</em> components - the <em>trend</em> being defined as the overall variation across the different cycles,(or seasons) , and the <em>seasonality</em> defined as the cyclical variation within the cycles. We notice from this plot, that the 3 layer network with 32 nodes in each of the 1st 2 layers represented by the red curve of fitted values, follows the overall trend in the actual value. We conclude therefore that the network, does a reasonably good job at capturing the <em>trend</em>. However the <em>seasonality</em> is not captured well by this network, as there are significant differences between the blue and red curves within the cycles.</p>
<ul>
<li><p><a href="https://drive.google.com/file/d/0B-oAF_WTZ7kGRXJoRlFJY1NXYm8/view?usp=sharing">Quassi-periodic Function Approximation (Seasonality - part 1)</a><br>
We enrich the deep learning network used above by increasing the number of nodes in each of the 1st 2 stages from 32 to 64, and we notice that this improves the ability of the network to capture the seasonality of the data. This network captures the seasonality for more than half of the trained range.</p>
</li>
<li><p><a href="https://drive.google.com/file/d/0B-oAF_WTZ7kGTWtzaVJvOWtzQlk/view?usp=sharing">Quassi-periodic Function Approximation (Seasonality - part 2)</a><br>
We extend the network further to increase the number of nodes in the 1st 2 layers from 64 to 128, and we observe that the seasonality component is captured in much better manner by the network. And finally by increasing the number of nodes in 1st 2 layers from 128 to 256, the network learns the behavior of the function in the entire trained range. The results for this are shown <a href="https://drive.google.com/file/d/0B-oAF_WTZ7kGTVJxSGlGTHpnWWM/view?usp=sharing">  here.</a></p>
</li>
</ul>
<p>In the last example(256 nodes in 1st 2 layers), the blue curve of actual values, is almost covered by the approximated values, except at the right boundary. This suggests that the approximation is good in the range of values for which it has been trained, and no guarantees can be made for the input values lying outside the trained range.  This is expected because in the absence of training data outside a given range, there is no reasonable way to approximate the output function.</p>
<p>We summarize that the same network that was used for approximating the cubic and exponential functions, with a very limited amount of tuning does a good job of approximating the quassi-periodic function, without any change to its other parameters. The tuning being talked about here is namely the increase in number of nodes in the first 2 layers of the deep network. Therefore the same structure of the network is capable of <em>leraning</em> multiple types of relationships - cubic, exponential, and quassi-periodic. The last example, also suggests that more complex function require a higher number of nodes while retaining the same deep learning structure, and the network can be tuned to achieve the necessary performance by increasing the number of nodes.</p>
<h2>Summary </h2>
<p>This talk covered the following topics  -</p>
<ol>
<li>How to use Tensorflow to model an arbitrary function </li>
<li>The model tuning choices </li>
<li>The effect of various tuning choices on the quality and convergence of the optimization algorithm. </li>
<li>Most importantly, this talk demonstrates that it is possible to use the same neural network to approximate different types of functions - cubic, quassi-periodic, and exponential. </li>
</ol>
<p>As practitioners and users of deep learning frameworks, this should give us the confidence and encouragement, that an appropriately  defined deep learning  network is capable of being a universal function approximator - i.e. even in the absence of any prior information between the relationship between the input and output, a deep learning network is capable of learning the relationship using the given training data.</p>

            </div>
        </div>
    </div>
</div>
<!-- END MAIN CONTENT -->

<!-- END BODY CONTENT -->

<!-- FOOTER -->
<footer id="footer" class="section ">
    <div class="container">
        <p class="copyright pull-left">&copy; 2017 | PyCON DE 2017 | <a
                href="mailto:info@pycon.de">info@pycon.de </a>
            <br/>
            <span class="small">Legal notice: This website is run by <a href="https://python-verband.org">Python Software Verband e.V.</a>, Schulstraße 20, 15366 Neuenhagen</span>
        </p>
        <ul class="list-inline pull-right social-icon">
            <li><a href="https://github.com/pysv" data-toggle="tooltip" data-placement="top" title="Github"><i
                    class="fa fa-github"></i></a></li>
        </ul>
        <ul class="list-inline pull-right social-icon">
            <li><a href="https://twitter.com/pyconde" data-toggle="tooltip" data-placement="top" title="Twitter"><i
                    class="fa fa-twitter"></i></a></li>
        </ul>
    </div>
</footer>
<!-- END FOOTER -->

<div class="back-to-top">
    <a href="index.html#top"><i class="fa fa-chevron-up"></i></a>
</div>

<!-- JAVASCRIPT -->
<script src="../../../assets/js/jquery-2.1.0.min.js?h=7e40e55d"></script>
<script src="../../../assets/js/bootstrap.min.js?h=5814e91b"></script>
<script src="../../../assets/js/jquery.scrollTo-1.4.3.1-min.js?h=2b7bb6c2"></script>
<script src="../../../assets/js/jquery.localscroll-1.2.7-min.js?h=7b6589a6"></script>
<script src="../../../assets/js/jquery.easing.min.js?h=c08b816a"></script>
<script src="../../../assets/js/jquery.parallax-1.1.3.js?h=673006df"></script>
<script src="../../../assets/js/google-map.js?h=752c1fc0"></script>
<script src="../../../assets/js/imagesloaded.pkgd.js?h=fe505d9e"></script>
<script src="../../../assets/js/jquery.isotope.js?h=cdde97ee"></script>
<script src="../../../assets/js/wow.js?h=2d558fee"></script>
<script src="../../../assets/js/jquery.flexslider-min.js?h=4e07d04e"></script>
<!-- <script src="../../../assets/js/jpreloader.min.js?h=d49c72a3"></script>-->
<script src="../../../assets/js/jquery.magnific-popup.min.js?h=9a45cea8"></script>
<script src="../../../assets/js/loop.js?h=07df16a4"></script>
</body>
</html>

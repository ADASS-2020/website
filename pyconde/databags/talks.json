{
    "wed-11:30-cubus": {
        "code": "3HXB7K",
        "title": "Your first NLP project: peaks and pitfalls of unstructured data",
        "state": "confirmed",
        "abstract": "If you are looking for short, practical recipes  for different natural language processing use cases in Python, this talk is for you!",
        "description": "Natural Language Processing improves the quality of your text data for future analysis and increases the accuracy of your machine learning model. It\u2019s important to know what goes into the bag of words and what are some potential do's and don'ts of text pre-processing.\r\nWhich text normalization steps are necessary and which ones are \u201cnice-to-have\u201d? Why is classic NLP still relevant in the age of Deep Learning? What metrics can be used to compare word frequencies and what can machine learning algorithms do with those numbers? This NLP talk provides answers to these questions and more! You'll see three examples of NLP pipelines using spaCy: sentiment analysis and emoji in tweets, named entity recognition in Yelp reviews, and multilingual topic modeling for news articles.",
        "duration": "00:45",
        "speakers": "Anna Widiger",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "NLP",
        "domain": "some",
        "tweet": "Your first NLP project: peaks and pitfalls of unstructured data by Anna Widiger",
        "the_speakers": [
            {
                "name": "Anna Widiger",
                "biography": "Anna Widiger has a B.A. degree in Computational Linguistics. She\u2019s been doing NLP since her very first programming assignment, specializing in Russian morphology, German syntax, cross-lingual named entity recognition, topic modeling and natural language understanding. She likes Pi, pies and PyPeople.",
                "homepage": "http://",
                "@twitter": ""
            }
        ],
        "popularity": 4,
        "slot_time": "wed-11:30",
        "slot_date": "wednesday 11:30",
        "slot_code": "wed-11:30-cubus",
        "room": "cubus",
        "slug": "your-first-nlp-project-peaks-and-pitfalls-of-unstructured-data"
    },
    "wed-11:30-media": {
        "code": "YGB9LX",
        "title": "Modern asynchronous programming",
        "state": "confirmed",
        "abstract": "People are starting to use asyncio today, the talk covers common design mistakes and shares best practices.\r\nThe summary of asyncio usage for 5 years from the library maintainer.",
        "description": "I started to take part of `asyncio` development at ancient times when the library had called `Tulip`.\r\nWe made a bunch of libraries under `aio-libs` umbrella: `aiohttp` as the most popular asyncio web client and server library, drivers for `PostgreSQL`, `Redis`, `Kafka`, `ElasticSearch`, `Mongo` etc.\r\nasyncio proved its reliability, and now it is quite famous: thousands of GitHub projects are based on asyncio, very many companies use asyncio as a basement for their infrastructure.\r\nThe overall design is good but we made several minor mistakes, most of them are fixed by Python 3.6 and 3.7.\r\nBy the talk, I want to share my asyncio knowledge and my current understanding of best practices learned the hard way.",
        "duration": "00:45",
        "speakers": "Andrew Svetlov",
        "submission_type": "Talk",
        "pyskill": "expert",
        "domains": "Parallel Programming, Web",
        "domain": "expert",
        "tweet": "a talk about common asyncio mistakes and best practices",
        "the_speakers": [
            {
                "name": "Andrew Svetlov",
                "biography": "Andrew is the Python Core Developer.\r\nFor last years he is concentrated on asyncio and derived technologies.\r\nCo-Author of aiohttp, aio-libs team lead, asyncio maintainer.",
                "homepage": "http://",
                "@twitter": "andrew_svetlov"
            }
        ],
        "popularity": 5,
        "slot_time": "wed-11:30",
        "slot_date": "wednesday 11:30",
        "slot_code": "wed-11:30-media",
        "room": "media",
        "slug": "modern-asynchronous-programming"
    },
    "wed-11:30-lounge": {
        "code": "PGWRFX",
        "title": "reticulate: R interface to Python",
        "state": "confirmed",
        "abstract": "This talk presents the R package reticulate introduced by RStudio in 2018. It allows to import Python modules, source scripts, convert and manipulate objects and use a Python repl in R.",
        "description": "Python and R are the preferred languages for data science. In 2018, RStudio introduced its package reticulate and clearly demonstrates that it favours to join forces. Both languages have strengths and weaknesses. Tools to combine the strengths will enable easier collaboration in projects and more possibilities to succeed. Using Python from R gives R users wider access to functions and makes it easier for Python beginners to just run scripts and being able to collaborate in Python projects.\r\nThe talk will show the possibilities of reticulate:\r\nThe main part starts with demonstrating the Python interpreter within R. It will show how to source Python scripts as well as install and import modules. Then it will deal with the most important types of Python objects, how they are represented in R and how to further manipulate them. Thereby, a special focus is on using Python for data science. In addition, it will be presented how Conda environments can be created and used from R.  A further application will be the creation of reports with Markdown and LaTeX where R and Python can be used within one document and share objects. A last topic is about showing the possibilities for easier development in RStudio (help regarding Python functions, auto completion).",
        "duration": "00:45",
        "speakers": "Jens Bruno Wittek",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Artificial Intelligence, Algorithms, Big Data, Deep Learning & Artificial Intelligence, Data Science, NLP, Machine Learning, Visualisation",
        "domain": "expert",
        "tweet": "reticulate enables using Python within R: import Python modules, source scripts, convert and manipulate objects, use a Python repl and more",
        "the_speakers": [
            {
                "name": "Jens Bruno Wittek",
                "biography": "Jens Bruno Wittek has been working as a data scientist at AKKA Digital (previously known as GIGATRONIK) in Stuttgart for nearly two years. He conducts projects involving predictive maintenance for car parts and for lasers, text classification and others. Before, he got a Master in statistics at Bielefeld University and then was on loan at Daimler TSS. Since 2009 Jens Bruno Wittek gained expert level in R and now is using both R and Python in business projects involving data management, statistics, machine learning, visualisation and more.",
                "homepage": "http://https://www.akka-digital.com/"
            }
        ],
        "popularity": 3,
        "slot_time": "wed-11:30",
        "slot_date": "wednesday 11:30",
        "slot_code": "wed-11:30-lounge",
        "room": "lounge",
        "slug": "reticulate-r-interface-to-python"
    },
    "wed-11:30-lecture": {
        "code": "KF3BUL",
        "title": "Introduction and practical experience about Quantum Computing using the Python libraries from IBM and Google",
        "state": "confirmed",
        "abstract": "Provide an introduction of Quantum Computing and give experience about Quantum Computing experiments using the Python libraries from IBM (QISKIT) and Google (CIRQ).",
        "description": "As publicly announced in early 2018, Daimler AG has started cooperations with IBM and Google on Quantum Computing. When doing concrete experiments with the Quantum Computing cloud based offerings, two different Python libraries provided by IBM and Google are used. They are named QISKIT in the case of IBM and CIRQ in the case of Google. The experiments with both libraries are handled using appropriate Jupyter Notebooks.\r\nThis talk gives a brief introduction on Quantum Computing, specifically on Quantum Computers based on transmon-based QBits. This is followed by an introduction of the both Python libraries that are used. Then some details about the Jupyter notebooks that are used are given. The talk will finish with some demos and an overview about the most important practical experiences with both Quantum Computing offerings.",
        "duration": "00:45",
        "speakers": "Dr. Andreas Riegg",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Jupyter, Programming, Python, Science",
        "domain": "some",
        "tweet": "Practical experience on Quantum Computing experiments using the Python libraries from IBM (QISKIT) and Google (CIRQ).",
        "the_speakers": [
            {
                "name": "Dr. Andreas Riegg",
                "biography": "Since 1991 Manager IT Innovation at Daimler AG.",
                "homepage": "http://www.daimler.com",
                "@twitter": ""
            }
        ],
        "popularity": 3,
        "slot_time": "wed-11:30",
        "slot_date": "wednesday 11:30",
        "slot_code": "wed-11:30-lecture",
        "room": "lecture",
        "slug": "introduction-and-practical-experience-about-quantum-computing-using-the-python-libraries-from-ibm-and-google"
    },
    "wed-11:30-openhub": {
        "code": "BQSZ3X",
        "title": "Pandas IO Tools: Reading and Writing DataFrames as Files and Databases",
        "state": "confirmed",
        "abstract": "Learn to use the right tools to read and write your DataFrames as pickle, CSV, JSON, Msgpack, HTML, Excel, HDF5, Parquet, and a PostgreSQL database.",
        "description": "Reading a CSV file in Pandas can be as easy as `dfr = pd.read_csv('filename.csv')` and work each time, but still could return unexpected results. You may have to deal with ambiguous timestamps, broken timezone handling, obscure NaN notation, non-standard numbers representation, language-specific formats, or heterogeneous value types. The worst case is when it handles these problems somehow automatically and returns wrong data without warning.\r\n\r\nOn a series of examples we'll try to import real-world cases, discuss the problems and find a stable way to handle them. After CSV we'll have a look at most other formats supported by Pandas IO tools, such as pickle, JSON, Msgpack, HTML, Excel, HDF5, Parquet and a PostgreSQL database.\r\n\r\nThe participants will need Python 3.x and a recent Pandas installation. Jupyter Notebook may be useful but not necessary. A repository with all code examples and test data will be published before the conference.",
        "duration": "01:30",
        "speakers": "Miroslav \u0160ediv\u00fd",
        "submission_type": "Workshop",
        "pyskill": "basic",
        "domains": "Data Science, Jupyter, Science",
        "domain": "some",
        "tweet": "Pandas can read and write in many file formats. Don't content yourself with its defaults!",
        "the_speakers": [
            {
                "name": "Miroslav \u0160ediv\u00fd",
                "biography": "Using Python to make the sun shine and the wind blow. hjkl juggler and languages enthusiast. Living in the Europe/Berlin timezone, saving text files as UTF-8, typing on a standard US keyboard layout with a Compose key.",
                "@twitter": "@eumiro",
                "homepage": "http://"
            }
        ],
        "popularity": null,
        "slot_time": "wed-11:30",
        "slot_date": "wednesday 11:30",
        "slot_code": "wed-11:30-openhub",
        "room": "openhub",
        "slug": "pandas-io-tools-reading-and-writing-dataframes-as-files-and-databases"
    },
    "wed-12:20-cubus": {
        "code": "TMW9U3",
        "title": "Germany's next topic model",
        "state": "confirmed",
        "abstract": "The aim of this talk is to present a novel way of detecting topics that is especially suited for user generated content where topics are not as clearly separated as in the typical examples of Wikipedia or newsgroup articles. The basic idea is to compute a contextual similarity score that defines a network from which we can identify clusters through community detection.",
        "description": "Identifying topic models for user generated content like hotel reviews turns out to be difficult with the standard approach of LDA (Latent Dirichlet Allocation; Blei et al., 2003). Hotel review texts usually don't differ as much in the topics that are covered as is typical with other genres such as Wikipedia or newsgroup articles where there is commonly only a very small set of topics present in each document. \r\n\r\nTo this end, we developed our own approach to topic modeling that is especially tailored to non-edited texts like hotel reviews. The approach can be divided into three major steps. First, using the concept of second-order cooccurrences we define a contextual similarity score that enables us to identify words that are similar with respect to certain topics. This score allows us to build up a topic network where nodes are words and edges the contextual similarity between the words. With the help of algorithms from graph theory, like the Infomap algorithm (Rosvall and Bergstrom, 2008), we are able to detect clusters of highly connected words that can be identified as topics in our review texts. In a further step, we use these clusters and the respective words to get a topic similarity score for each word in the network. In other words, we transform a hard clustering of words into topics into a probability score of how likely a certain word belongs to a given topic/cluster. \r\n\r\nThe presentation is structured as follows:\r\n- short overview of existing topic modeling approaches\r\n- shortcomings of these approaches with respect to our domain (hotel review texts)\r\n- explaining the contextual similarity score and its relationship to word embeddings\r\n- topic modeling step through community detection\r\n- turning the hard clustering into a fuzzy topic model\r\n\r\nReferences:\r\nDavid M. Blei, Andrew Y. Ng, Michael I. Jordan: Latent dirichlet allocation. In: Journal of Machine Learning Research, Jg. 3 (2003), S. 993\u20131022, ISSN 1532-4435\r\nM. Rosvall and C. T. Bergstrom, Maps of information flow reveal community structure in complex networks, PNAS 105, 1118 (2008) http://dx.doi.org/10.1073/pnas.0706851105, http://arxiv.org/abs/0707.0609",
        "duration": "00:30",
        "speakers": "Thomas Mayer",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Artificial Intelligence, Deep Learning & Artificial Intelligence, Data Science, Networks, NLP, Machine Learning, Visualisation",
        "domain": "some",
        "tweet": "Topic modeling through community detection in a network of contextual similarities of words",
        "the_speakers": [
            {
                "name": "Thomas Mayer",
                "biography": "Thomas Mayer is an NLP Data Scientist at HolidayCheck, located in Munich.",
                "homepage": "http://",
                "@twitter": "@MayerThommy"
            }
        ],
        "popularity": 4,
        "slot_time": "wed-12:20",
        "slot_date": "wednesday 12:20",
        "slot_code": "wed-12:20-cubus",
        "room": "cubus",
        "slug": "germany-s-next-topic-model"
    },
    "wed-12:20-media": {
        "code": "EVT9PV",
        "title": "Developing ecommerce platform with Django Oscar",
        "state": "confirmed",
        "abstract": "In this talk I\u2019m going to describe from A to Z, how using Django and Django Oscar design complex modern ecommerce platform to buy, sell, supply and exchange goods.",
        "description": "Modern technologies has changed the way how we buy and sell in the Internet nowadays. Thus, we also constantly evolve the way how we build projects to sell and buy goods using our technologies of choice - Python and Django. Ease of customization and flexibility made Oscar application of my choice for building custom ecommerce platforms on top of Django, allowing to satisfy client requirements without necessity of implementing solution from scratch, since Oscar have ready-made applications for basket, checkout, shipping etc, available for the further tweak and refinement.\r\n\r\nI\u2019ve been using and contributing to Django Oscar since 2015, designed and supported various ecommerce projects in different fields (food, accessories, shoes) and sizes during this time. In this talk I\u2019m going to share some of the experience and conclusions about building complex ecommerce platform using Django, and eventually, Django Oscar.\r\n\r\nI will go through the next topics: \r\n* architecture approaches (synchronous app vs SPA); \r\n* frontend (VueJS, React etc); \r\n* API (Django REST Framework and Oscar-API); \r\n* payment method integration; \r\n* multiple currencies, exchange rate and conversion; \r\n* custom pricing policies (sale price, wholesale prices); \r\n* single-page checkout; \r\n* substandard discounting systems (buyers club, bonus points system); \r\n* user roles and permissions (users, admins, partners) and data management; \r\n* Oscar pros and cons (when to use and when not to use).",
        "duration": "00:30",
        "speakers": "Alexander Gaevsky",
        "submission_type": "Talk",
        "pyskill": "expert",
        "domains": "Business & Start-Ups, Code-Review, Django",
        "domain": "expert",
        "tweet": "Django consultant from Ukraine with 7yrs+ experience, Django-Oscar core developer and open-source contributor.",
        "the_speakers": [
            {
                "name": "Alexander Gaevsky",
                "biography": "Django consultant from Ukraine with 7yrs+ experience, Django-Oscar core developer and open-source contributor.",
                "homepage": "http://agaevsky.com",
                "@twitter": "agaevsky"
            }
        ],
        "popularity": 4,
        "slot_time": "wed-12:20",
        "slot_date": "wednesday 12:20",
        "slot_code": "wed-12:20-media",
        "room": "media",
        "slug": "developing-ecommerce-platform-with-django-oscar"
    },
    "wed-12:20-lounge": {
        "code": "NTCBZX",
        "title": "IoT using Python on Linux: Lessons Learned",
        "state": "confirmed",
        "abstract": "This talk is about our experience using Python on a Linux based IoT device, the problems we were facing and the lessons we learned.\r\nAn appliance using Bluetooth Low Energy, D-Bus, NetworkManager and a proprietary sensor interface - What could possibly go wrong?",
        "description": "In a distributed sensor network system with a Java based Cloud application, mobile apps and a proprietary radio protocol accompanying it we developed an IoT appliance that connects the existing radio infrastructure to the Cloud service developed in-house.\r\n\r\nUsing CPython 3.5 + Debian GNU/Linux 9 on an ARMv7 platform, we developed the following features:\r\n\r\n* Secure device <-> app communication via Bluetooth Low Energy using a custom encryption protocol\r\n* Enabling self healing network connectivity through the use of NetworkManager\r\n* Controlling the sensor network using proprietary hardware through a custom C library with Python bindings\r\n* Heavy use of D-Bus through pydbus\r\n* Interfacing with LEDs and buttons using the Linux GPIO subsystem\r\n* Internal state management through strictly composable interfaces\r\n* Secure remote software update\r\n\r\nOver the course of this project, we learned a lot about Test Driven Development of Python apps in teams and DevOps in the IoT space. We would now like to share our experience developing a Python application for a headless IoT device and the things we would liked to have known upfront.\r\n\r\nThe talk is held both by Matthias Schmidt (Senior Architect at diva-e)  and Thomas Keppler (Software Developer at diva-e).",
        "duration": "00:30",
        "speakers": "Thomas Keppler, Matthias Schmidt",
        "submission_type": "Talk",
        "pyskill": "professional",
        "domains": "DevOps, Infrastructure, Networks, Programming, Python",
        "domain": "not required",
        "tweet": "Developing an IoT appliance using Bluetooth Low Energy, D-Bus, NetworkManager and a proprietary sensor interface - What could possibly go wrong?",
        "the_speakers": [
            {
                "name": "Thomas Keppler",
                "biography": "Interested in technology since the age of 6, I started learning how to program using the traditional LAMP stack in the mid 2000s. From then on I was simply hooked on the field of application development! Developing software for platforms new and old slowly transformed from being a Hobby into a professional career.\r\nMy journey with Python 3 started in 2013 when I was searching for a platform independent way to write scripts for my sysadmin job at the time.",
                "homepage": "http://https://www.diva-e.com/",
                "@twitter": "@diva_excellence"
            },
            {
                "name": "Matthias Schmidt",
                "biography": "I'll come up with a detailed bio later. Bear with me.",
                "homepage": "http://diva-e.com",
                "@twitter": "https://twitter.com/diva_excellence"
            }
        ],
        "popularity": 4,
        "slot_time": "wed-12:20",
        "slot_date": "wednesday 12:20",
        "slot_code": "wed-12:20-lounge",
        "room": "lounge",
        "slug": "iot-using-python-on-linux-lessons-learned"
    },
    "wed-12:20-lecture": {
        "code": "NC7Z3X",
        "title": "Karabo - A control framework fueled by Python asyncio",
        "state": "confirmed",
        "abstract": "The Supervisory Control And Data Acquisition (SCADA) system 'Karabo' is briefly introduced focusing on the challenges and current solutions for a user-friendly Python based framework and interface at the European XFEL research facility.",
        "description": "The European XFEL in Hamburg is the world's largest X-ray Free Electron Laser (XFEL) in operation and opens up completely new research opportunities for scientists coming from all over the world to the facility to carry out experiments with the laser, which allows to create images or molecular movies to determine atomic positions with a temporal resolution of a few hundreds of femtoseconds.\r\n\r\nFor the Supervisory Control And Data Acquisition (SCADA) system in this research facility a new software control framework called 'Karabo' is actively developed accepting the challenge of the very high data rates and volumes by the large scientific detectors and numbers of slow control devices: when fully commissioned, the large scientific detectors will each produce up to 27,000 images per second, each image containing about a million pixels. \r\nThe core of the framework relies on an essentially event-based architecture using a central message broker and so-called pipelines, which are direct tcp connections for large data transfers.\r\nThere is significant complexity as a large number and variety of hardware and virtual devices for the\r\ndifferent beamlines and experiments need to be orchestrated together for various control and data acquisition tasks. Therefore, next to the programming language C++, the framework has been developed in Python based on the asyncio library providing a central event loop for managing the asynchronous i/o and concurrency tools. Reasons for the choice of Python include that it provides a strong asynchronous library, allows rapid software development, and is also widely regarded as a mediator language between\r\nscientists and software engineers. It strongly enhances external contributions from scientists for the integration of the necessary data analysis algorithms and control routines. In this talk, the challenges and current solutions for a user-friendly Python based framework and interface at the European XFEL research facility are presented.",
        "duration": "00:30",
        "speakers": "Dennis G\u00f6ries",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Business & Start-Ups, Infrastructure, Science",
        "domain": "some",
        "tweet": "Karabo - a new control and analysis framework for scientific facilities",
        "the_speakers": [
            {
                "name": "Dennis G\u00f6ries",
                "biography": "Senior Software Enginner at the European XFEL",
                "homepage": "http://",
                "@twitter": ""
            }
        ],
        "popularity": 2,
        "slot_time": "wed-12:20",
        "slot_date": "wednesday 12:20",
        "slot_code": "wed-12:20-lecture",
        "room": "lecture",
        "slug": "karabo-a-control-framework-fueled-by-python-asyncio"
    },
    "wed-14:00-cubus": {
        "code": "L8NSBN",
        "title": "From Wittgenstein to TensorFlow: The role of Domain Specific Languages and Language Design in Machine Learning",
        "state": "confirmed",
        "abstract": "What programming language do you use to develop Machine Learning (ML) and Artificial Intelligence (AI) systems? This is one of the most frequently asked question about my work. \r\nThe short answer: a mix of Scala, Python and F#\r\nThe long answer: DSLs are a hot topic and play a crucial role in many of the tasks Machine Learning (ML) and Artificial Intelligence (AI) systems need to tackle.",
        "description": "What programming language do you use to develop Machine Learning (ML) and Artificial Intelligence (AI) systems? This is one of the most frequently asked question about my work. \r\nThe short answer: a mix of Scala, Python and F#\r\nThe long answer: DSLs are a hot topic and play a crucial role in many of the tasks Machine Learning (ML) and Artificial Intelligence (AI) systems need to tackle.\r\nExpertise in DSLs is mission critical in ML and AI systems.",
        "duration": "00:45",
        "speakers": "Mattia Ferrini",
        "submission_type": "Talk",
        "popularity": null,
        "slot_time": "wed-14:00",
        "slot_date": "wednesday 14:00",
        "slot_code": "wed-14:00-cubus",
        "room": "cubus",
        "slug": "from-wittgenstein-to-tensorflow-the-role-of-domain-specific-languages-and-language-design-in-machine-learning"
    },
    "wed-14:00-media": {
        "code": "SVALYW",
        "title": "Performance evaluation of GANs in a semi-supervised OCR use case",
        "state": "confirmed",
        "abstract": "Even in the age of big data labelled data is a scarce resource in many machine learning use cases. We evaluate generative adversarial networks (GANs) at the task of extracting information from vehicle registrations under a varying amount of labelled data and compare the performance with supervised learning techniques. Using unlabelled data shows a significant improvement.",
        "description": "Online vehicle marketplaces are embracing artificial intelligence to ease the process of selling a vehicle on their platform. The tedious work of copying information from the vehicle registration document into some web form can be automated with the help of smart text spotting systems. The seller takes a picture of the document and the necessary information is extracted automatically.\r\n\r\nWe introduce the components of a text spotting system including the subtasks of object detection and character object recognition (OCR). In view of our use case, we elaborate on the challenges of OCR in documents with various distortions and artefacts which rule out off-the-shelve products for this task.\r\n\r\nAfter an introduction of semi-supervised learning based on generative adversarial networks (GANs), we evaluate the performance gains of this method compared to supervised learning. More specifically, for a varying amount of labelled data the accuracy of a convolution neural network (CNN) is compared to a GAN which uses additionally unlabelled data during the training phase.\r\n\r\nWe conclude that GANs significantly outperform classical CNNs in use cases with a lack of labelled data. Regarding our use case of extracting information from vehicle registration documents, we show that our text spotting system easily exceeds an accuracy of 99.5% thus making it applicable in a real-world use case.",
        "duration": "00:45",
        "speakers": "Florian Wilhelm",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Artificial Intelligence, Computer Vision, Deep Learning & Artificial Intelligence, Data Science, Machine Learning",
        "domain": "some",
        "tweet": "Performance evaluation of GANs in a semi-supervised OCR use case",
        "the_speakers": [
            {
                "name": "Florian Wilhelm",
                "biography": "I am a Data Scientist living in Cologne, Germany with a mathematical background. \r\nAfter my postdoctoral position I started as a Data Scientist at [Blue Yonder](http://www.blue-yonder.com/),\r\nthe leading platform provider for Predictive Applications and Big Data in the European market.\r\nRight now I enjoy working on innovative Data Science projects with experts every day at [inovex](https://www.inovex.de/en/). With more than five years of project experience in the field of Predictive & Prescriptive Analytics and Big Data, I have acquired profound knowledge in the domains of mathematical modelling, statistics, machine learning, high-performance computing and data mining.\r\n\r\nFor the last years I programmed mostly with the Python Data Science stack ([NumPy](http://www.numpy.org/), [SciPy](http://www.scipy.org/), [Scikit-Learn](http://scikit-learn.org/), [Pandas](http://pandas.pydata.org/), [Matplotlib](http://matplotlib.org/), [Jupyter](http://jupyter.org/), etc.) to which I also contributed several extensions.",
                "homepage": "http://https://inovex.de",
                "@twitter": "@FlorianWilhelm"
            }
        ],
        "popularity": 3,
        "slot_time": "wed-14:00",
        "slot_date": "wednesday 14:00",
        "slot_code": "wed-14:00-media",
        "room": "media",
        "slug": "performance-evaluation-of-gans-in-a-semi-supervised-ocr-use-case"
    },
    "wed-14:00-lounge": {
        "code": "QFNS3Z",
        "title": "Scalable Scientific Computing using Dask",
        "state": "confirmed",
        "abstract": "Pandas and NumPy are great tools to dive through data, do analysis and train machine learning models. They provide intuitive APIs and superb performance. Sadly they are both restricted to the main memory of a single machine and mostly also to a single CPU. Dask is a flexible tools for parallelizing NumPy and Pandas code on a single machine or a cluster.",
        "description": "Pandas and NumPy are great tools to dive through data, do analysis and train machine learning models. They provide intuitive APIs and superb performance. Sadly they are both restricted to the main memory of a single machine and mostly also to a single CPU. Once our code reaches these boundaries, we can utilize Dask to scale our code to multiple CPUs or even across a cluster.\r\n\r\nDask provides high-level Array, Bag, and DataFrame implementations that mimic the NumPy, lists, and Pandas APIs but operate in parallel on data that doesn't need to fit into main memory. In the low level, Dask provides dynamic task schedulers that execute task graphs in parallel. These execution engines power the high-level collections mentioned above but can also power custom, user-defined workloads.\r\n\r\nIn the workshop, we want to show how to turn typical Pandas and NumPy code into parallel/distributed code using dask.array and dask.dataframe. We will highlight things that can easily be transformed into dask code and other things that need a bit more thought. In addition, we will show the utilities that Dask provides us to inspect the execution graphs and the behaviour of our distributed code.",
        "duration": "01:30",
        "speakers": "Uwe L. Korn",
        "submission_type": "Workshop",
        "pyskill": "professional",
        "domains": "Algorithms, Big Data, Data Science, Parallel Programming, Python",
        "domain": "some",
        "tweet": "Parallelize Pandas and NumPy code using Dask/distributed",
        "the_speakers": [
            {
                "name": "Uwe L. Korn",
                "biography": "Uwe Korn is a Senior Data Scientist at the German RetailTec company Blue Yonder. His expertise is on building scalable architectures for machine learning services. Nowadays he focuses on the data engineering infrastructure that is needed to provide the building blocks to bring machine learning models into production. As part of his work to provide an efficient data interchange he became a core committer to the Apache Parquet and Apache Arrow projects.",
                "homepage": "http://https://uwekorn.com/",
                "@twitter": "@xhochy"
            }
        ],
        "popularity": null,
        "slot_time": "wed-14:00",
        "slot_date": "wednesday 14:00",
        "slot_code": "wed-14:00-lounge",
        "room": "lounge",
        "slug": "scalable-scientific-computing-using-dask"
    },
    "wed-14:00-lecture": {
        "code": "7ZNCP8",
        "title": "Cython to speed up your Python code",
        "state": "confirmed",
        "abstract": "Come and learn how to speed up and optimise your Python code with the Cython compiler.",
        "description": "[Cython](http://cython.org) is not only a very fast and comfortable way to talk to native code and libraries, it is also a widely used tool for speeding up Python code. The Cython compiler translates Python code to C or C++ code, and applies many static optimisations that make Python code run visibly faster than in the interpreter. But even better, it supports static type annotations that allow direct use of C/C++ data types and functions, which the compiler uses to convert and optimise the code into fast, native C. The tight integration of all three languages, Python, C and C++, makes it possible to freely mix Python features like generators and comprehensions with C/C++ features like native data types, pointer arithmetic or manually tuned memory management in the same code.\r\n\r\nThis talk by a core developer introduces the Cython compiler by interactive code examples, and shows how you can use it to speed up your real-world Python code. You will learn how you can profile a Python module and use Cython to compile and optimise it into a fast binary extension module. All of that, without losing the ability to run it through common development tools like code checkers or coverage test tools.",
        "duration": "00:45",
        "speakers": "Stefan Behnel",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Big Data, Infrastructure, Jupyter, Parallel Programming",
        "domain": "some",
        "tweet": "Stefan Behnel is show-casing Cython for speeding up Python code.",
        "the_speakers": [
            {
                "name": "Stefan Behnel",
                "biography": "Stefan is a long-time Python user and core developer of the well-known OSS projects Cython and lxml. He works on big data pipelines at TrustYou, encourages people to use Python and talk about it, and gives lectures and trainings on Cython programming and High-Performance Computing.",
                "homepage": "http://https://www.trustyou.com",
                "@twitter": ""
            }
        ],
        "popularity": 3,
        "slot_time": "wed-14:00",
        "slot_date": "wednesday 14:00",
        "slot_code": "wed-14:00-lecture",
        "room": "lecture",
        "slug": "cython-to-speed-up-your-python-code"
    },
    "wed-14:00-openhub": {
        "code": "VBC9GK",
        "title": "Finance, Sales and Business Development for Start-Ups",
        "state": "confirmed",
        "abstract": "Thinking about founding a Start-up, or you just did it? This workshop will give you an overview about the features and set up needed to start a company and to run it successfully. Furthermore, we will discuss how the financing of a start-up can be structured, how to get customers as well as how to set up the relevant structures for your company",
        "description": "Thinking about founding a Start-up, or you just did it? This workshop will give you an overview about the features and set up needed to start a company and to run it successfully. Furthermore, we will discuss how the financing of a start-up can be structured and what financing method can be used. Finally we discuss about sales methods, whatis used for what customer universe and how to structure it. Together we identify the idealtypical company vs. real life problems every founder is faced with. The structure of the workshop will be as follows:\r\n- Start-up ecosystem\r\n- Setting up a company:\r\n    -Ideal company set up:\r\n    -Team structure\r\n- Business Development\r\n    - Business model\r\n    - Processes and tasks\r\n    - KPIs and how to measure them\r\n    - Speed\r\n    - Infrastructure\r\n    - Resources\r\n    - Employees\r\n- Start-up Financing\r\n    - Friends, Fools and Family (FFF)\r\n    - Business Angels\r\n    - Venture Capitalists\r\n    - Banks\r\n    - Structured Finance\r\n- Sales\r\n    - What is sales?\r\n    - Who should do salesin a company\r\n    - What customers to address\r\n    - Sales Cycle\r\n    - Measure sales success\r\n    - Push vs Pull?",
        "duration": "01:30",
        "speakers": "Ingo Stegmaier",
        "submission_type": "Workshop",
        "pyskill": "basic",
        "domains": "Business & Start-Ups, Infrastructure",
        "domain": "some",
        "tweet": "financingandclients",
        "the_speakers": [
            {
                "name": "Ingo Stegmaier",
                "biography": "Ingo always had a passion for business models and entrepreneurial thinking. After his studies in Germany, Korea and the UK he was working for Morgan Stanley and the UBS in Munich where he was advising private clients and small corporations. Ingo made his passion into a profession is helping founders and start-ups in building up their company, get financing and industry customers.",
                "homepage": "http://www.koenigsweg.com",
                "@twitter": ""
            }
        ],
        "popularity": null,
        "slot_time": "wed-14:00",
        "slot_date": "wednesday 14:00",
        "slot_code": "wed-14:00-openhub",
        "room": "openhub",
        "slug": "finance-sales-and-business-development-for-start-ups"
    },
    "wed-14:50-cubus": {
        "code": "TEDNFL",
        "title": "Experiences from applying Convolutional Neural Networks for classifying 2D sensor data",
        "state": "confirmed",
        "abstract": "Applying deep neural networks on existing benchmark datasets is different than applying it on your company data where you have different requirements and constraints. This talk shows an example for applying a Convolutional Neural Network on real 2D sensor data, points out its challenges and provides hints.",
        "description": "When being the first in your company to apply a deep learning algorithm on your data you often have to overcome several obstacles. One challenge is to understand your data and to form a training and test dataset. Another one is to get your algorithms and its performance accepted and integrated in your existing processing workflow.\r\n\r\nConvolutional Neural Networks have become a standard tool in processing image data.  They have shown to reach human-level classification performance on some object recognition tasks.\r\n\r\n\r\nIn this talk I will present my experiences in getting started using a convolutional neural network for classification of 2D sensor data. I will point out the importance of understanding your data and give hints of how to select your train and test datasets according to the requirements. Furthermore, I will show how to get a feature extractor out of the classifier and how to visualize it.",
        "duration": "00:45",
        "speakers": "Matthias Peussner",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Artificial Intelligence, Computer Vision, Deep Learning & Artificial Intelligence, Machine Learning",
        "domain": "some",
        "tweet": "Experiences from applying Convolutional Neural Networks for classifying 2D Sensordata",
        "the_speakers": [
            {
                "name": "Matthias Peussner",
                "biography": "Matthias Peussner has studied applied system science at the Osnabr\u00fcck University, Germany. He worked several years as a software developer. Now he is working as scientist for ROSEN Technology & Research Center GmbH in Lingen, Germany. He has special interest in doing data science, developing machine learning models (deep learning) and creating software prototypes using Python.",
                "homepage": "http://www.rosen-group.com",
                "@twitter": ""
            }
        ],
        "popularity": 4,
        "slot_time": "wed-14:50",
        "slot_date": "wednesday 14:50",
        "slot_code": "wed-14:50-cubus",
        "room": "cubus",
        "slug": "experiences-from-applying-convolutional-neural-networks-for-classifying-2d-sensor-data"
    },
    "wed-14:50-media": {
        "code": "EQA7TS",
        "title": "Active Learning - Building Semi-supervised Classifiers when Labeled Data is not Available",
        "state": "confirmed",
        "abstract": "In situations where unlabeled data is abundant but labeled data cannot be obtained easily active learning\r\ncan be utilized to build classifiers by intelligently querying users for labels. This talks shows the \r\ncore concepts and algorithms of active learning based on a real-world project.",
        "description": "In many situations large datasets are available but unfortunately labeling is expensive and time consuming.  Active Learning is a concept for building classifiers by letting the algorithm choose the training data it uses. This achieves greater accuracy than just labeling a random subset of the available dataset. \r\n\r\nThe active learning algorithm selects some unlabeled data instances which are then labeled by a human\r\nannotator. Given this information a classifier is trained and new instances for the human annotator to label\r\nare selected. This iterative process tries to label as few instances as possible while achieving high classification accuracy. \r\n\r\nIn this talk I will give a general overview of the core concepts and techniques of active learning like\r\nalgorithms for selecting the queries and convergence criteria.",
        "duration": "00:45",
        "speakers": "Dr. Hendrik Niemeyer",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Artificial Intelligence, Algorithms, Data Science, Machine Learning, Python",
        "domain": "some",
        "tweet": "Learn how to select the optimal subset of data to label using active learning.",
        "the_speakers": [
            {
                "name": "Dr. Hendrik Niemeyer",
                "biography": "I am a Data Scientist working on applying machine learning solutions to sensor data from non-destructive testing of pipelines in order to detect and size internal damage.",
                "homepage": "http://",
                "@twitter": "@hniemeye"
            }
        ],
        "popularity": 4,
        "slot_time": "wed-14:50",
        "slot_date": "wednesday 14:50",
        "slot_code": "wed-14:50-media",
        "room": "media",
        "slug": "active-learning-building-semi-supervised-classifiers-when-labeled-data-is-not-available"
    },
    "wed-14:50-lecture": {
        "code": "KPY88U",
        "title": "Selinon - dynamic distributed task flows",
        "state": "confirmed",
        "abstract": "Selinon is a task flow manager that enhances Celery and gives you an ability to create advanced task flows serving large workflows in your cluster.",
        "description": "Have you ever tried to define and process complex workflows for data processing? If the answer is yes, you might have struggled to find the right framework for that. You've probably came across Celery - popular task flow management for Python. Celery is great, but it does not provide enough flexibility and dynamic features needed badly in complex flows. As we discovered all the limitations, we decided to implement Selinon.\r\n\r\nHave you ever tried to define and process complex workflows for data processing? If the answer is yes, you might have struggled to find the right framework for that. You've probably came across Celery - popular task flow management for Python. Celery is great, but it does not provide enough flexibility and dynamic features needed badly in complex flows. As we discovered all the limitations, we decided to implement Selinon.\r\n\r\nSelinon enhances Celery task flow management and allows you to create and model task flows in your distributed environment that can dynamically change behavior based on computed results in your cluster, automatically resolve tasks that need to be executed in case of selective task runs, automatic tracing mechanism and many others.",
        "duration": "00:45",
        "speakers": "Fridol\u00edn Pokorn\u00fd",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Big Data, Infrastructure, Parallel Programming, Programming, Python",
        "domain": "some",
        "tweet": "An advanced task flows in your cluster using Selinon by @fridex.",
        "the_speakers": [
            {
                "name": "Fridol\u00edn Pokorn\u00fd",
                "biography": "Software engineer at Red Hat with focus on Python, machine learning, scalable systems and big data processing.",
                "homepage": "http://https://fridex.github.io",
                "@twitter": "@fridex"
            }
        ],
        "popularity": 2,
        "slot_time": "wed-14:50",
        "slot_date": "wednesday 14:50",
        "slot_code": "wed-14:50-lecture",
        "room": "lecture",
        "slug": "selinon-dynamic-distributed-task-flows"
    },
    "wed-16:00-cubus": {
        "code": "F8NLRH",
        "title": "Case Study in Travel Business - Understanding agent connections using NetworkX",
        "state": "confirmed",
        "abstract": "When you make a search for a hotel room, do you know how many travel agents are searching for you at the same time? In this talk, we demonstrate how to use the millions of searches a sourcing company received to build a network of travel agents and finding the main hubs among them using NetworkX.",
        "description": "Network analysis is getting more and more attention in Business Intelligence, people hope to get information out of the structure of an organization or a communication network. In this talk, we use the hotel room search requests from travel agents, including online public website, B2C, B2B and B2B2C, to build a relational network among them. By using this network as an example, we demonstrate how insights can be extract by studying network properties.\r\n\r\nIn the first half of the talk, we will explain how the network is built using NetworkX, an open-source python library that is designed for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. When 2 agents are making the same search at the same time , a link ( or an \u201cedge\u201d in network analysts terms) is made pointing form the initial searcher to the subsequent searcher. Using a list of these searches, a directed graph is built. We will also demonstrate how to pick the biggest connected component out form the graph. In the second half, with the graphs created, we show how different functions of NetworkX can be used to study the graphs. By compare the graph properties of our graph to the other popular network graphs, we can get the insight of how the network was created. Also by studying the graphs, we can understand the behavior of the agents and can even figure out which agents are acting as main hubs in the network.\r\n\r\nThis talk is for people who are interested in network analysis and would like to see how it can be used in a business case. Audiences with any level of python experience can learn some basic concept of network analysis work and how it can be applied to provide business insights.",
        "duration": "00:30",
        "speakers": "Cheuk Ting Ho",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Algorithms, Networks, Python",
        "domain": "not required",
        "tweet": "Do you know how many travel agents are searching for your hotel bookings at the same time? In this talk, we build a network of travel agents and finding the main hubs using NetworkX.",
        "the_speakers": [
            {
                "name": "Cheuk Ting Ho",
                "biography": "After spending 5 years doing research in theoretical physics at Hong Kong University of Science and Technology, Cheuk has transferred her analytical and logical skills in natural science and built a career in data science. Cheuk is now a Data Scientist in Hotelbeds Groups which is one of the biggest worldwide wholesaler in travel business.\r\n\r\nCheuk constantly contributes to the community by giving AI and deep learning workshops, volunteering at Datakind for charities. At the same time participate open source projects. Cheuk has also been a guest speaker at University of Oxford and Queen Mary University of London. Believing in gender equality, Cheuk is currently a co-organizer of AI club for Gender Minorities to support Tech Diversity and Inclusion.",
                "homepage": "http://group.hotelbeds.com/",
                "@twitter": "@cheukting_ho"
            }
        ],
        "popularity": 3,
        "slot_time": "wed-16:00",
        "slot_date": "wednesday 16:00",
        "slot_code": "wed-16:00-cubus",
        "room": "cubus",
        "slug": "case-study-in-travel-business-understanding-agent-connections-using-networkx"
    },
    "wed-16:00-media": {
        "code": "7BJQJB",
        "title": "A Day Has Only 24\u00b11 Hours: import pytz",
        "state": "confirmed",
        "abstract": "Handle easily all those timezone issues your system knows about. Fear those it doesn't.",
        "description": "On the last Sunday of October \u201cwe get one more hour of sleep\u201d but may spend much more time debugging code dealing with the timezones, daylight saving time shifts and datetime stuff in general.\r\n\r\nWe'll look at a few pitfalls you may encounter when working with datetimes in Python. We'll discover the `pytz` module and explain why `pytz.all_timezones` contains over 500 individual timezones. We'll also find the reason why `pytz` is not part of the standard Python, why it gets updated so often and why even that won't solve all your problems.",
        "duration": "00:30",
        "speakers": "Miroslav \u0160ediv\u00fd",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Data Science, Science",
        "domain": "some",
        "tweet": "A day has only 24\u00b11 hours: import pytz and handle easily all those timezone issues your system knows about. Fear those it doesn't.",
        "the_speakers": [
            {
                "name": "Miroslav \u0160ediv\u00fd",
                "biography": "Using Python to make the sun shine and the wind blow. hjkl juggler and languages enthusiast. Living in the Europe/Berlin timezone, saving text files as UTF-8, typing on a standard US keyboard layout with a Compose key.",
                "@twitter": "@eumiro",
                "homepage": "http://"
            }
        ],
        "popularity": 4,
        "slot_time": "wed-16:00",
        "slot_date": "wednesday 16:00",
        "slot_code": "wed-16:00-media",
        "room": "media",
        "slug": "a-day-has-only-241-hours-import-pytz"
    },
    "wed-16:00-lounge": {
        "code": "SFSLZG",
        "title": "Script, Library, or Executable: You can have it all!",
        "state": "confirmed",
        "abstract": "Sometimes a stand-alone script contains useful code to share across projects so you refactor into an importable library. After that library gains traction some less tech-savvy users want the functionality in a GUI.  It can be difficult for beginner or intermediate Python developers to structure a Python package that can provide a good interface to CLI users, developers, and GUI lovers.  This talk will describe one potential project layout, API guidelines, and tools to easily grow a project to support all of these.",
        "description": "I will describe one possible way to achieve this using the following features:\r\n\r\n\t* Argparse based CLI script\r\n\t* Using Gooey to wrap CLI API for a GUI\r\n\t* Pyinstaller to package with no external dependencies\r\n\r\n### Package goals\r\nWe want to create a package layout that can support a CLI interface, an importable library, and a GUI all while sharing as much code as possible.\r\n\r\nAlthough text and graphical interfaces are very different we can provide a consistent API with careful consideration.  This way users can easily use our library or either interface without starting all over again.\r\n\r\n## API guidelines\r\n\t- Use the same terminology in each entry point (CLI/GUI)\r\n\t- Allow all the same inputs to be passed to a single functional-entry point when imported as a library\r\n\t\r\n### Step 1: CLI\r\nFirst we will layout a single-file CLI script using argparse similar to the Unix `wc` tool that takes a text file and outputs the following information:\r\n\r\n\t1. Number of words\r\n\t2. Number of lines\r\n\t3. Rough estimate of reading time\r\n\r\nWe'll discuss the `__name__ == __main__` Python idiom, separating the argument parsing from the main function, and why keeping as little as possible in `__main__` is better for reusability.\r\n\r\nThere are several pros and cons to providing others with a single-file script.  It's easy to develop and simple to read, but it requires any user to have the correct version of Python installed. It's also difficult for other developers to reuse the code in their own projects or deploy to PyPI.\r\n\r\n### Step 2: Add library interface\r\nNext we'll take our single-file script and expand it into a basic Python package using a main folder, __init__.py, and a cli.py script to expose the same CLI as before.\r\n\r\nWe'll discuss how to restructure the main and parsing functions from step 1 into an public API defined by the __init__.py that exposes the same CLI functionality as a library.\r\n\r\nWe won't dive into setup.py at all, but there will be links and a brief description on the various tools to layout a package such as cookiecutter and setup.py.\r\n\r\n### Step 3: Add Gooey interface\r\nThe [Gooey](https://github.com/chriskiehl/Gooey) project can easily expose a CLI as GUI with a few decorators.  We'll discuss briefly how to use Gooey and some of the extra functionality it provides to create more advanced GUIs.\r\n\r\nWe'll also give a simple mental model for how it maps argparse argument types to GUI widgets.\r\n\r\n### Step 4: PyInstaller\r\nUntil step 3 all we required of users was a working Python 3 installation.  However, adding Gooey requires users to have a working Python installer, the Gooey library, and wxPython.  Typically GUIs are meant for higher-level users so asking them to install all of this to benefit from our little app is too much.\r\n\r\nInstead we'll see how we run PyInstaller on our entry script to package up all our dependencies **including** Python itself into a simple executable.  We'll briefly discuss the build and dist output folders from PyInstaller along with the ability to use it to package all sorts of complicated Python applications using Qt, Numpy, etc.\r\n\r\nEnd-users in management don't even have to know we used Python!",
        "duration": "00:30",
        "speakers": "Luke Lee",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Programming, Python",
        "domain": "not required",
        "tweet": "Script, Library, or Executable: You can have it all!",
        "the_speakers": [
            {
                "name": "Luke Lee",
                "biography": "Over the last 13 years, Luke Lee has professionally written software for applications ranging from Python desktop and web applications to embedded C\r\ndrivers for Solid State Disks.  Currently, he writes Python desktop and web applications for clients in the Oil and Education industries.\r\n\r\nHis enthusiasm for Python is emphasized throughout his presentations at several Python related conferences including Pycon, PyTexas, and PyArkansas.  He also maintains a technical blog at www.lukelee.me/blog",
                "homepage": "http://",
                "@twitter": "durden20"
            }
        ],
        "popularity": 3,
        "slot_time": "wed-16:00",
        "slot_date": "wednesday 16:00",
        "slot_code": "wed-16:00-lounge",
        "room": "lounge",
        "slug": "script-library-or-executable-you-can-have-it-all"
    },
    "wed-16:00-lecture": {
        "code": "V3UZWL",
        "title": "Achieving Resilient Code with Integration Tests",
        "state": "confirmed",
        "abstract": "Writing good and isolated integration tests can be challenging. We will see in this talk how to reach this goal, using Pytest and Docker Compose.",
        "description": "You are maybe like me: I never learned at school how to write tests. My teachers gave me at first a broad overview of computer history. Then, they explained me some basic design patterns. And to finish, I often had to write more or less basic programs, to validate and demonstrate my skills. Not the kind of code I would be really proud of today: the procrastinator monkey living in my head at this time was more thinking about planning my summer holidays, rather than writing Ninja code!\r\n\r\nAnd to make things worse, my studies focused on network and system engineering. Not software architecture. Funny story, because I decided to become programmer a couple of years later\u2026\r\n\r\nWhat I realize now is that I don\u2019t have as much time as before to learn. And in a world driven by business, where time is money, and where tradeoffs are the rule, there is rarely enough money to write both shiny new features and a complete test suite.\r\n\r\nPeople who practice Test-Driven Development know how complicated it can be to write proper tests. TDD is often discouraging at first: the learning curve is steep. But this problem also exists in the testing world in general. Because writing good tests is hard, many beginners get headaches trying to reach this goal. How to convince project managers to have more time for writing tests in these conditions\u2026\r\n\r\nBut \u201cle jeu en vaut la chandelle\u201d as we say in French (\"the juice is worth the squeeze\"). Well tested applications are not only easier to maintain and extend. They also have in general a better API. That\u2019s what we will see in this talk, by focusing on how to write integration tests. Our journey will begin with a presentation of different testing strategies. We will then jump to the practical part, using Pytest, interface testing , dependency injections and stubs, amongst many others. And because we want to add nice buzzwords on our resume after PyConDE, we will finish this talk by automating the whole with Docker Compose.",
        "duration": "00:30",
        "speakers": "Alexandre Figura",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Programming, Python",
        "domain": "not required",
        "tweet": "Inject more love in your integration tests <3",
        "the_speakers": [
            {
                "name": "Alexandre Figura",
                "biography": "Alexandre is a French Python developer, living since 2 years in Berlin. In his short career, he already worked with more than 20 different nationalities. Over the years, he gained experience in Web Programming and System/Network Engineering. But his favorite topic is testing: he likes to test everything possible, to make applications more resilient and easier to maintain.",
                "homepage": "http://",
                "@twitter": ""
            }
        ],
        "popularity": 4,
        "slot_time": "wed-16:00",
        "slot_date": "wednesday 16:00",
        "slot_code": "wed-16:00-lecture",
        "room": "lecture",
        "slug": "achieving-resilient-code-with-integration-tests"
    },
    "wed-16:00-openhub": {
        "code": "8WXEH8",
        "title": "Build a modern data infrastructure",
        "state": "confirmed",
        "abstract": "### Data is the new oil \ud83d\udee2\ufe0f\r\n\r\nSo you want to build a data driven company or create machine learnings models, but guess what?\r\nYou need a proper data infrastructure to support it!\r\n\r\nDuring this workshop you will learn how to create one using OSS tools and the best practices that could fit both batch and stream processing.",
        "description": "During this tutorial you will learn how to create a scalable and reliable data infrastructure using OSS libraries.\r\n\r\nGiven the limited amount of time we will focus mainly on the batch side but some useful insights will be shared about stream processing and OLAP.\r\n\r\nThe first part will introduce the main concepts while the second will be focused on *building stuff*.\r\n\r\n### Tutorial outline\r\n\r\n#### First part\r\n\r\n- Why bother?\r\n- Unified data warehouse\r\n- Stream VS Batch (Fast VS Slow data)\r\n- Kafka? I'd rather use Redis\r\n- Short term storage VS Long term storage\r\n- Airflow\r\n\r\n#### Second part\r\n\r\n- Build a Python consumer\r\n- Use Airflow to move things around\r\n- Use Airflow to train machine learning models\r\n- What's next?\r\n\r\n### Tools that we are going to use\r\n\r\n- Redis streams\r\n- Scylla\r\n- Airflow\r\n- Docker\r\n- Pandas and Parquet\r\n- Python \u2764\ufe0f\r\n\r\n### Prerequisites\r\n\r\n- If I say `clone this repo locally and download these docker images` you don't freak out\r\n- A good knowledge of Python",
        "duration": "01:30",
        "speakers": "Christian Barra",
        "submission_type": "Workshop",
        "pyskill": "expert",
        "domains": "Big Data, Data Science, DevOps, Infrastructure, Jupyter, Web",
        "domain": "some",
        "tweet": "Build a modern data infrastructure using microservices, Airflow and Docker",
        "the_speakers": [
            {
                "name": "Christian Barra",
                "biography": "I do Python, conferences and often play with data.",
                "homepage": "http://chrisbarra.xyz",
                "@twitter": "@christianbarra"
            }
        ],
        "popularity": null,
        "slot_time": "wed-16:00",
        "slot_date": "wednesday 16:00",
        "slot_code": "wed-16:00-openhub",
        "room": "openhub",
        "slug": "build-a-modern-data-infrastructure"
    },
    "wed-16:35-cubus": {
        "code": "A9HL7W",
        "title": "Binder - lowering the bar to sharing interactive software",
        "state": "confirmed",
        "abstract": "In this talk I will introduce the audience to the concepts and ideas behind the Binder project. I will showcase examples from the community to illustrate use-cases and show off the power of Binder using the mybinder.org service.",
        "description": "The Binder project drastically lowers the bar to sharing and re-using software. As a user wanting to try out someone else\u2019s work I only have to click a single link. As the author preparing a binder-ready project is much easier than having to support many different platforms and for many projects involves little additional work.\r\n\r\nIn this talk I will introduce the audience to the concepts and ideas behind the Binder project. I will showcase examples from the community to illustrate use-cases and show off the power of Binder.\r\n\r\nThree pieces of software power Binder: [repo2docker](http://repo2docker.readthedocs.io/en/latest/), [BinderHub](https://binderhub.readthedocs.io/en/latest/) and [JupyterHub](http://jupyterhub.readthedocs.io/en/stable/). Using an example repository I will go through the steps required to make a repository binder-ready and what happens when a user launches it. At each step I will illustrate the role that each of the three software components play and how they interact.\r\n\r\nBinder is a project created by its community. I will present pathways for getting involved with the community.\r\n\r\nTo wrap up I will highlight plans for future developments and features of Binder.",
        "duration": "00:30",
        "speakers": "Tim Head",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Community, Data Science, DevOps, Jupyter, Science, Web",
        "domain": "some",
        "tweet": "Lowering the bar to sharing interactive software, listen to @betatim talk about the ins and outs of how mybinder.org works",
        "the_speakers": [
            {
                "name": "Tim Head",
                "biography": "Tim Head builds data driven products for clients all around the world, from startups to UN organisations. His company www.wildtreetech.com specialises in digital products that leverage machine-learning and deploying custom JupyterHub setups.\r\n\r\nTim contributes to the Binder project and helped create scikit-optimize. When he isn\u2019t traveling he trains for triathlons.",
                "homepage": "http://www.wildtreetech.com",
                "@twitter": "@betatim"
            }
        ],
        "popularity": 5,
        "slot_time": "wed-16:35",
        "slot_date": "wednesday 16:35",
        "slot_code": "wed-16:35-cubus",
        "room": "cubus",
        "slug": "binder-lowering-the-bar-to-sharing-interactive-software"
    },
    "wed-16:35-media": {
        "code": "YMMKKW",
        "title": "Python Dependency Management",
        "state": "confirmed",
        "abstract": "While `there should be one-- and preferably only one --obvious way to do it`, there are\r\nmultiple for managing Python dependencies. Let's have a look at the current state of\r\ndependency management in Python and some of the tools available.",
        "description": "For a long time there were `pip` and `virtualenv` which were used together with `requirements.txt`\r\nfiles to manage Python dependencies. Nowadays there are various other tools that help you improve\r\nthe workflow.\r\n\r\nWe will have a look at popular projects like\r\n* [pip-compile](https://github.com/jazzband/pip-tools)\r\n* [hatch](https://github.com/ofek/hatch)\r\n* [poetry](https://poetry.eustace.io/)\r\n* [Pipenv](https://docs.pipenv.org/)\r\n\r\nAfter the talk you will be able to decide for yourself which approach suits your usecases best and\r\ndon't have to rely on rants postet on reddit.",
        "duration": "00:30",
        "speakers": "Patrick Muehlbauer",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Infrastructure",
        "domain": "some",
        "tweet": "Python Dependency Management",
        "the_speakers": [
            {
                "name": "Patrick Muehlbauer",
                "biography": "Patrick is a Software Engineer at [Blue Yonder](https://tech.blue-yonder.com/). He cares for resilient services and scalable architectures, which he did for quite some time in Blue Yonder's platform team. Nowadays he is responsible for Blue Yonder's customer facing UI services.",
                "homepage": "http://",
                "@twitter": "@tmuxbee"
            }
        ],
        "popularity": 4,
        "slot_time": "wed-16:35",
        "slot_date": "wednesday 16:35",
        "slot_code": "wed-16:35-media",
        "room": "media",
        "slug": "python-dependency-management"
    },
    "wed-16:35-lounge": {
        "code": "US3HQJ",
        "title": "Productionizing your ML code seamlessly",
        "state": "confirmed",
        "abstract": "Nowadays, it's easy to build a model and play with data in a notebook, but hard to bring the code to production. This talk will aim to answer:\r\n1. What does running an ML model in production involve? \r\n2. How to improve your development workflow to make the path to production easier?",
        "description": "Data science and Machine Learning are hot topics right now for Software Engineers and beyond. There are a lot of python tools that allow you to hack together a notebook to quickly get insight on your data, or train a model to predict or classify. Or you might have inherited some data wrangling and modeling {Jupyter/Zeppelin} notebook code from someone else, like the resident data scientist. \r\n\r\nThe code works on test data, when you run the cells in the right order (skipping cell 22), and you believe that the insight gained from this work would be a valuable game changer. But now how do you take this experimental code into production, and keep it up-to-date with a regular retraining schedule? And what do you need to do after that, to ensure that it remains reliable and brings value in the long term?\r\n\r\nThese will be the questions this talk will answer, focusing on 2 main themes:\r\nWhat does running an ML model in production involve?\r\nHow to improve your development workflow to make the path to production easier?\r\n\r\n\r\nThis talk will draw examples from real projects at Yelp, like migrating a pandas/sklearn classification project into production with pyspark, while aiming to give advice that is not dependent on specific frameworks, or tools, and is useful for listeners from all backgrounds.",
        "duration": "00:30",
        "speakers": "Lauris Jullien",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Artificial Intelligence, Data Science, Machine Learning",
        "domain": "some",
        "tweet": "Nowadays, it's easy to build a model and play with data in a notebook, but hard to bring the code to production. This talk will aim to answer: What does running an ML model in production involve? How to improve your development workflow to make the path to production easier?",
        "the_speakers": [
            {
                "name": "Lauris Jullien",
                "biography": "I always love playing with data and ML problems. Computer Vision at University, robotics later and now for Yelp!",
                "homepage": "http://",
                "@twitter": ""
            }
        ],
        "popularity": 3,
        "slot_time": "wed-16:35",
        "slot_date": "wednesday 16:35",
        "slot_code": "wed-16:35-lounge",
        "room": "lounge",
        "slug": "productionizing-your-ml-code-seamlessly"
    },
    "wed-16:35-lecture": {
        "code": "M8KBBJ",
        "title": "Testing in Python - The Big Picture",
        "state": "confirmed",
        "abstract": "Software testing is of course very important when it comes to quality assurance. But good testing strategies can also make our lives as developers easier. In this talk, we will take a look at different aspects of software testing and find out what the Python ecosystem has to offer for our testing needs.",
        "description": "Any team developing and maintaining software - be it free and open source or commercial - employs one form of software testing or another. But what are the different kinds of tests in our tool boxes? And how are they best used? In this talk we'll take a look around and try to answer these questions. \r\n\r\nFirst, we'll examine the basic concepts of testing: Everyone has probably at least heard about *unit tests*, but are they all you need? *Performance tests* can help you find out how well your product performs under load and detect bottle necks early on. *Manual testing* is often looked down upon, since it's not automated, but is it always a bad idea? And what even _is_ *mutation testing*?\r\n\r\nWe'll also get to know a lot of the amazing testing tools from the Python ecosystem. Find out what the best test runner is (Spoiler alert: it's pytest). Learn how to make writing test more fun and less work using tools like mock, Faker and factory_boy. Measure the quality of your test suite using coverage.py.\r\n\r\nBut no tool is the right one for any situation. We'll also talk about when and how to use each of the tools, while debunking common misconceptions and demonstrating best practices.",
        "duration": "00:30",
        "speakers": "Niklas Meinzer",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Programming, Python",
        "domain": "not required",
        "tweet": "Think software testing is dull and boring? Want to improve the quality of your product? Then \"Testing in Python - The big picture\" might be for you!",
        "the_speakers": [
            {
                "name": "Niklas Meinzer",
                "biography": "A Python programmer and open source enthusiast for years, I am now the lead developer in a medical software company. Like everyone in the software field, I benefit greatly from many open source tools available to us. Therefore I try to give back whenever I can, leaving a contribution here and there. I'm mainly involved with the coala.io project, a code quality analyzer for all language where I'm currently mentoring a Google Summer of Code student. I absolutely love going to conferences to meet old friends and make new ones. If you've seen my talk and have questions or comments and wonder if you should discuss them with me, the answer is: yes!",
                "homepage": "http://https://niklas-meinzer.de/",
                "@twitter": "NiklasMM"
            }
        ],
        "popularity": 4,
        "slot_time": "wed-16:35",
        "slot_date": "wednesday 16:35",
        "slot_code": "wed-16:35-lecture",
        "room": "lecture",
        "slug": "testing-in-python-the-big-picture"
    },
    "wed-17:10-cubus": {
        "code": "HSVZUS",
        "title": "Pyccel, a Fortran static compiler for scientific High-Performance Computing",
        "state": "confirmed",
        "abstract": "Presenting Pyccel, a source-to-source Python-Fortran, and DSL enabling HPC capabilities.",
        "description": "*Pyccel* is a new **static compiler** for Python that uses **Fortran** as backend language while enabling High-Performance Computing **HPC** capabilities.\r\n\r\nFortran is a computer language for scientific programming that is tailored for efficient run-time execution on a wide variety of processors. Even if the *2003* and *2008* standards added major improvements like *OOP, Coarrays, Submodules, do concurrent*, etc ... they are not covered by all available compilers. Moreover, the Fortran developer still suffers from the lack of **meta-programming** compared to **C++** ones. Therefore, it is more and more difficult for applied mathematicians and computational physicists to write applications at the *state of art* (targeting CPUs, GPUs, MICs) while implementing complicated algorithms or numerical schemes.\r\n\r\nPyccel can be used in two cases:\r\n\r\n- accelerate Python code by converting it to Fortran and calling *f2py*,\r\n- generate portable HPC Fortran codes from a DSL using the Python syntax.\r\n\r\nIn order to achieve the second point, we developed an internal DSL for *types* and *macros*. The later is used to map sentences based on *mpi4py*, *scipy.linalg.blas or lapack* onto the appropriate calls in Fortran. Moreover, two parsers, for *OpenMP* and *OpenACC*, were added too, allowing for explicit parallelism through the use of pragmas.\r\n\r\nLast but not least, Pyccel is an extension of **Sympy**. Actually, it converts a Python code to symbolic expressions/trees, from a Full Syntax Tree (*RedBaron*), then annotates the new AST using types or different settings provided by the user.\r\n\r\nIn this talk, after a brief description of Pyccel, I will show different applications including Finite Elements (1d, 2d, 3d), Semi-Lagrangian schemes (4d), Kronecker linear solvers, diagnostics for 5D kinetic simulations and Machine Learning for Partial Differential Equations.",
        "duration": "00:15",
        "speakers": "Dr. Ing. Ratnani Ahmed",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Artificial Intelligence, Algorithms, Astronomy, Parallel Programming, Programming, Python, Science",
        "domain": "not required",
        "tweet": "Accelerate your Python code using Pyccel",
        "the_speakers": [
            {
                "name": "Dr. Ing. Ratnani Ahmed",
                "biography": "- *since 09/2014* Max-Planck Institut f\u00fcr Plasmaphysik IPP, Garching bei M\u00fcnchen, Germany\r\nMHD group leader since 2016\r\n\r\n- *10/2013 - 09/2014* Universit\u00e9 de Nice Sophia-Antipolis, Nice FRANCE\r\nPostDoc\r\n\r\n- *10/2011 - 10/2013* C.E.A./DSM/IRFM/SIPP, Cadarache FRANCE\r\nPostDoc, EuroFusion fellowship\r\n\r\n- *09/2008 - 10/2011* INRIA / IRMA - Institut de Recherche en Math\u00e9matiques Avanc\u00e9es, Strasbourg FRANCE\r\nPhd \r\n\r\n- *02/2006 - 09/2007* FERMAT - Risk Management Software Provider FRANCE\r\nEngineer ( Basel II Project - Credit Risk )",
                "homepage": "http://",
                "@twitter": ""
            }
        ],
        "popularity": 2,
        "slot_time": "wed-17:10",
        "slot_date": "wednesday 17:10",
        "slot_code": "wed-17:10-cubus",
        "room": "cubus",
        "slug": "pyccel-a-fortran-static-compiler-for-scientific-high-performance-computing"
    },
    "wed-17:10-media": {
        "code": "PA9FZK",
        "title": "Distributed Hyperparameter search with sklearn and kubernetes",
        "state": "confirmed",
        "abstract": "In this talk, I will show how you can harness the scheduling of kubernetes for distributing hyperparameter search with sklearn onto a cluster of nodes. This can be achieved quite easily and with just a few changes to the original code, so the Data Scientist won't be bothered by complex kubernetes internals.",
        "description": "While sklearn provides a good interface to do hyperparameter search on large & complex model (pipelines), doing these can take up a lot of time. The traditional way usually includes one beefy machine and a lot of waiting. In other cases, people tend to \u201cmanually\u201d schedule parameter ranges between nodes, but that can also be problematic since these won't talk to each other. Kubernetes itself is currently the most prominent scheduler and shines at distributing task, but is a pretty complex system in itself. \r\n\r\nIn this talk, I will show how you can harness the scheduling of kubernetes for distributing hyperparameter search with sklearn onto a cluster of nodes. This can be achieved quite easily and with just a few changes to the original code, so the Data Scientist won't be bothered by complex kubernetes internals.",
        "duration": "00:15",
        "speakers": "Jakob Karalus",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Algorithms, Big Data, Data Science, DevOps, Infrastructure, Machine Learning",
        "domain": "some",
        "tweet": "How to use to do hyperparameter search with sklearn ontop of kuberneter",
        "the_speakers": [
            {
                "name": "Jakob Karalus",
                "biography": "Data Scientist/Consultant for codecentric.",
                "homepage": "http://",
                "@twitter": "@krallistic"
            }
        ],
        "popularity": 3,
        "slot_time": "wed-17:10",
        "slot_date": "wednesday 17:10",
        "slot_code": "wed-17:10-media",
        "room": "media",
        "slug": "distributed-hyperparameter-search-with-sklearn-and-kubernetes"
    },
    "wed-17:10-lounge": {
        "code": "XX8KJD",
        "title": "How type annotations make your code better",
        "state": "confirmed",
        "abstract": "The real world examples on how type annotations make your code better and less complex for other developers.",
        "description": "Type annotations still not received that amount of popularity, that should. People still finding them hard and sometimes ambiguous to use. But if you start new project in Python in 2018 you should consider using type annotations in your code and this short talk describes why.\r\n\r\nI'll go over examples, where type annotations helped my team to create less complex code, and how using type annotations changing your mind for projecting & implementing features for your project.",
        "duration": "00:15",
        "speakers": "Igor Davydenko",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Infrastructure, Web",
        "domain": "some",
        "tweet": "Type annotations make your code less complex",
        "the_speakers": [
            {
                "name": "Igor Davydenko",
                "biography": "Developing web applications with Python for 10+ years. Trying to make them as maintainable as possible. Using latest Python in production, cause what to wait for?",
                "homepage": "http://https://igordavydenko.com/",
                "@twitter": "playpausenstop"
            }
        ],
        "popularity": 3,
        "slot_time": "wed-17:10",
        "slot_date": "wednesday 17:10",
        "slot_code": "wed-17:10-lounge",
        "room": "lounge",
        "slug": "how-type-annotations-make-your-code-better"
    },
    "wed-17:10-lecture": {
        "code": "3RMPDC",
        "title": "About going Open-Source",
        "state": "confirmed",
        "abstract": "Why should you work for free?\r\nThis talk is about one of the best decisions in my life... Starting to get into Open-Source.\r\nYou will get a gentle introduction on what makes Open-Source so satisfying and the ways you can use to get started with Open-Source.\r\nWhether you're already a full grown developer or just starting out, this talk will probably give you some ideas and the motivation to really become passionate.\r\nTo close this off, you will get a little impression on how important an Open-Source portfolio is to your career and I'll give some neat hints to get your projects to the next level.\r\nSo join me and start Your journey!",
        "description": "I will start of talking about why \"working for free\" became an important part of the developer culture all around the world and talk about why companies should try to share as many tools as possible.\r\nI'll give some examples of tools that were open sourced and got immense traction leading to the projects growing with an intense speed.\r\nI'll try to take away the \"fear\" of open sourcing tools that cost ten of thousands of dollars to create.\r\nThen I'll describe how my journey with open source looks like and start talking about how you can get your feet into contributing to known projects.\r\nAfter that I will close off with some hints on doing and contributing to open source projects that worked out great for me.\r\nThey will also get an impression on what opportunities will show up if they put some of their time into doing OS.\r\n\r\nThe audience can be mixed, but it's way more interesting for people that are either new to programming or have never worked with open source.\r\n\r\nIn a perfect world, some of the listeners would leave the presentation with the motivation and hard goal to start with OS as soon as possible.",
        "duration": "00:15",
        "speakers": "Tim",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Business & Start-Ups",
        "domain": "some",
        "tweet": "Why should you \"work\" for free? About getting started with OpenSource work.",
        "the_speakers": [
            {
                "name": "Tim",
                "biography": "Passionate learner and developer. Studying computer science at the Media University. Looking forward to working with ingenious teams on challenging projects.",
                "homepage": "http://",
                "@twitter": "@timigrossmann"
            }
        ],
        "popularity": 3,
        "slot_time": "wed-17:10",
        "slot_date": "wednesday 17:10",
        "slot_code": "wed-17:10-lecture",
        "room": "lecture",
        "slug": "about-going-open-source"
    },
    "thu-10:30-cubus": {
        "code": "HBNNF8",
        "title": "Big Data Systems Performance: The Little Shop of Horrors",
        "state": "confirmed",
        "abstract": "The confusion around terms such as like NoSQL, Big Data, Data Science, SQL, Spark, and Data Lakes often creates more fog than clarity.  In my presentation, I will show that often at least three design dimensions are cluttered and confused in discussions when it comes to data management and how understanding these dfimensions helps you making your applications several orders of magnitude faster.",
        "description": "The confusion around terms such as like NoSQL, Big Data, Data Science, Spark, SQL, and Data Lakes often creates more fog than clarity. However, clarity about the underlying technologies is crucial to designing the best technical solution in any field relying on huge amounts of data including data science, machine learning, but also more traditional analytical systems such as data integration, data warehousing, reporting, and OLAP.\r\n\r\nIn my presentation, I will show that often at least three dimensions are cluttered and confused in discussions when it comes to data management: First, buzzwords (labels & terms like \"big data\", \"AI\", \"data lake\"); second, data design patterns (principles & best practices like: selection push-down, materialization, indexing); and Third, software platforms (concrete implementations & frameworks like: Python, DBMS, Spark, and NoSQL-systems).\r\n\r\nOnly by keeping these three dimensions apart, it is possible to create technically-sound architectures in the field of big data analytics.\r\n\r\nI will show concrete examples, which through a simple redesign and wise choice of the right tools and technologies, run thereby up to 1000 times faster. This in turn triggers tremendous savings in terms of development time, hardware costs, and maintenance effort.",
        "duration": "00:45",
        "speakers": "Jens Dittrich",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Algorithms, Big Data, Data Science, Infrastructure, Parallel Programming, Programming, Python, Science",
        "domain": "some",
        "tweet": "Big Data Systems Performance: The Little Shop of Horrors",
        "the_speakers": [
            {
                "name": "Jens Dittrich",
                "biography": "Jens Dittrich is a Full Professor of Computer Science in the area of Databases, Data Management, and Big Data Analytics at Saarland University, Germany. Previous affiliations include U Marburg, SAP AG, and ETH Zurich. He received an Outrageous Ideas and Vision Paper Award at CIDR 2011, a BMBF VIP Grant in 2011, a best paper award at VLDB 2014, three CS teaching awards in 2011, 2013, and 2018, as well as several presentation awards including a qualification for the interdisciplinary German science slam finals in 2012 and three presentation awards at CIDR (Conference on Innovative Data systems Research, 2011, 2013, and 2015). He has been a PC member and area chair/group leader of prestigious international database conferences and journals such as PVLDB/VLDB, SIGMOD, ICDE, and VLDB Journal. He is a member of the scientific advisory board of Software AG. He was a keynote speaker at VLDB 2017: \u201cDeep Learning (m)eats Databases\u201d (http://bit.ly/DL_meats_DB) and will also be speaking at DEEM@SIGMOD (Data Management for End to End Machine learning, http://deem-workshop.org/). At Saarland University he co-organizes the Data Science Summer School.\r\n\r\nHis research focuses on big data analytics including in particular: data analytics on large datasets, scalability, main-memory databases, database indexing, reproducibility, and scalable data science. He enjoys coding data science problems in Python, in particular using the keras and tensorflow libraries for Deep Learning. Since 2017 he has been working on a start-up at the intersection of data science and databases (http://daimond.ai). He teaches some of his classes as flipped classrooms (https://www.youtube.com/user/jensdit) and tweets at https://twitter.com/jensdittrich.",
                "homepage": "http://https://bigdata.uni-saarland.de/",
                "@twitter": "@jensdittrich"
            }
        ],
        "popularity": 3,
        "slot_time": "thu-10:30",
        "slot_date": "thursday 10:30",
        "slot_code": "thu-10:30-cubus",
        "room": "cubus",
        "slug": "big-data-systems-performance-the-little-shop-of-horrors"
    },
    "thu-10:30-media": {
        "code": "DPYMZZ",
        "title": "Deep Learning with PyTorch for more Fun and Profit (Part II)",
        "state": "confirmed",
        "abstract": "There are all these great articles and blog posts about Deep Learning describing all that awesome stuff.  - Is it all that easy? Let's check!\r\n\r\nWe'll look into: style transfer (making a picture look like painting), speech generation (like Siri or Alexa) and text generation (writing a story).\r\nIn this talk I'll describe the whole journey: A fun ride from the idea to the very end including all the struggles, failures and successes.",
        "description": "There are all these great blog posts about Deep Learning describing all that awesome stuff.  - Is it all that easy? Let's check!\r\nThis is part 2 of on ongoing series of adventures in Deep Learning for fun, research and business.\r\n\r\nWe'll look into: style transfer (making a picture look like painting), speech generation (like Siri or Alexa) and text generation (writing a story).\r\nIn this talk I'll describe the whole journey: A fun ride from the idea to the very end including all the struggles, failures and successes.\r\nSteps, we'll cover:\r\n* The data challenge: get the data ready\r\n* Have it run on your Mac with PyTorch and an eGPU\r\n* Creating a character-level language models with an Recurrent Neural Network\r\n* Creating a text generator\r\n* Creating artwork",
        "duration": "00:45",
        "speakers": "Alexander CS Hendorf",
        "submission_type": "Talk",
        "the_speakers": [
            {
                "name": "Alexander CS Hendorf",
                "biography": "Alexander' professional career was always about digitalisation: starting from vinyl records in the nineties to to advanced data analytics nowadays. He's program chair of Europe's main Python conference EuroPython, one of the 25 mongoDB masters, organiser of PyConDE 2017 and a regular contributor to the tech community. He has spoken at many international conferences in Silicon Valley, New York, London, Florence or Paris. He's a partner at K\u00f6nigsweg consultancy for digitalisation, high-tech and data science."
            }
        ],
        "popularity": 5,
        "slot_time": "thu-10:30",
        "slot_date": "thursday 10:30",
        "slot_code": "thu-10:30-media",
        "room": "media",
        "slug": "deep-learning-with-pytorch-for-more-fun-and-profit-part-ii"
    },
    "thu-10:30-lounge": {
        "code": "RR9WTS",
        "title": "Machine Learning as a Service: How to deploy ML Models as APIs without going nuts",
        "state": "confirmed",
        "abstract": "Deploying a machine learning model as an API is not a trivial task. Requires understanding of some web framework, worrying about data validation, authentication and deploying etc. Even after the API is ready, figuring out how to use it in the client is an added pain. \r\n\r\nThis talk takes the participants though the journey of building a machine learning model and deploying it as API.",
        "description": "Often, the most convenient way to deploy a machine model is an API. It allows accessing it from various programming environments and also decouples the development and deployment of the models from its use.\r\n\r\nHowever, building an good API is hard. It involves many nitty-gritties and many of them need to repeated everytime an API is built. Also, it is very important to have a client library so that the API can be easily accessed. If you every plan to use it from Javascript directly, then you need to worry about cross-origin-resource-sharing etc. All things add up and building APIs for machine very tedious.\r\n\r\nIn this talk demonstrates how deploying machine learning models an APIs can be made fun by using right programming abstractions. This presents couple of opensource libraries [firefly][] and [rorolite][] which are built for this very purpose.\r\n\r\n[firefly]: https://firefly-python.readthedocs.io/en/latest/\r\n[rorolite]: https://rorodata.github.io/rorolite/",
        "duration": "00:30",
        "speakers": "Anand Chitipothu",
        "submission_type": "Talk",
        "pyskill": "expert",
        "domains": "Machine Learning, Python, Web",
        "domain": "expert",
        "tweet": "Deploying machine learning models can't be any simpler than this.",
        "the_speakers": [
            {
                "name": "Anand Chitipothu",
                "biography": "Anand has been crafting beautiful software since a decade and half. He\u2019s now building a data science platform, rorodata, which he recently co-founded. He regularly conducts advanced programming courses through Pipal Academy. He is co-author of web.py, a micro web framework in Python. He has worked at Strand Life Sciences and Internet Archive.",
                "homepage": "http://https://anandology.com/",
                "@twitter": "anandology"
            }
        ],
        "popularity": 3,
        "slot_time": "thu-10:30",
        "slot_date": "thursday 10:30",
        "slot_code": "thu-10:30-lounge",
        "room": "lounge",
        "slug": "machine-learning-as-a-service-how-to-deploy-ml-models-as-apis-without-going-nuts"
    },
    "thu-10:30-lecture": {
        "code": "NJUTQM",
        "title": "Bonobo, Airflow and Grafana to visualize your business",
        "state": "confirmed",
        "abstract": "Build a simple business intelligence dashboard using python and open-source tools.",
        "description": "Zero-to-one hands-on introduction to building a business dashboard using Bonobo ETL, Airflow, and a bit of Grafana (because graphs are cool). Although the opposite is better, there is no need of prior knowledge about any of those tools.\r\n\r\nAfter a short introduction about the tools, we'll go through the following topics, using the real data of a small SaaS software:\r\n\r\n* Plan (What data do we need to see?)\r\n* Implement (How do we quickly get those graphs up?)\r\n* Monitor (Are you sure your data's still there?)\r\n* Iterate (How do we move on from there?)\r\n\r\nOne can expect to be able to build a similar system at the end of the talk in a few days (of course, the implementation is only a small part of this process, data is what really matters).\r\n\r\n\u00abMetrics you watch tend to improve over time\u00bb",
        "duration": "00:45",
        "speakers": "Romain Dorgueil",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Business & Start-Ups, Data Science, Visualisation, Web",
        "domain": "some",
        "tweet": "Build a business dashboard using Bonobo, Airflow and Grafana.",
        "the_speakers": [
            {
                "name": "Romain Dorgueil",
                "biography": "Developer, sysadmin, technical team builder, founder, advisor.\r\n\r\nCreator of Bonobo ETL and a bunch of other python libraries.\r\n\r\nSometimes, I play go and make music, but not at the same time.",
                "homepage": "http://https://makersquad.fr/",
                "@twitter": "@rdorgueil"
            }
        ],
        "popularity": 3,
        "slot_time": "thu-10:30",
        "slot_date": "thursday 10:30",
        "slot_code": "thu-10:30-lecture",
        "room": "lecture",
        "slug": "bonobo-airflow-and-grafana-to-visualize-your-business"
    },
    "thu-10:30-openhub": {
        "code": "9P7C3X",
        "title": "Understanding Neural Networks by Playing Games",
        "state": "confirmed",
        "abstract": "The objective of this workshop is to get the participants to understand CNNs by playing the simple Chrome T-Rex game. An intuitive and simple to follow tutorial that requires no prior knowledge.",
        "description": "Playing the Chrome T-Rex involves generating training data by actually playing the game in Chrome offline mode. We need roughly 300-500 images for training and this can be easily generated by playing the game just 5 times.\r\nWe then work through the basics of Neural Networks and CNNs and then actually training and testing a model. \r\nThis model can then be deployed easily within the browser and participants can see the model playing the game (with reasonable accuracy) - all within a manner of an hour.\r\nWe then take this one step further by trying to understand what the model and how it correlates with how they played the game in the first place. This helps to build an understanding of how humans think and what the model actually learns and looks for in the image.",
        "duration": "01:30",
        "speakers": "Sidharth Ramachandran",
        "submission_type": "Workshop",
        "pyskill": "basic",
        "domains": "Artificial Intelligence, Deep Learning & Artificial Intelligence",
        "domain": "some",
        "tweet": "Understanding Neural Networks by Playing Games!",
        "the_speakers": [
            {
                "name": "Sidharth Ramachandran",
                "biography": "Data Scientist focused on solving real business problems through innovative data products",
                "homepage": "http://",
                "@twitter": "sidhusmart"
            }
        ],
        "popularity": null,
        "slot_time": "thu-10:30",
        "slot_date": "thursday 10:30",
        "slot_code": "thu-10:30-openhub",
        "room": "openhub",
        "slug": "understanding-neural-networks-by-playing-games"
    },
    "thu-11:20-cubus": {
        "code": "S3AXGV",
        "title": "ZODB: The Graph Database for PythonDevelopers",
        "state": "confirmed",
        "abstract": "The ZODB is a mature graph database written in Python and optimized in C. Just subclass off of class Persistent Object, and Persistent Container, and your objects, graphs and applications become persistent. This talk teaches you what you need to know to start using a pythonic graph database.",
        "description": "You can see the current version of the slides at \r\nhttps://pythonlinks.info/presentations/zodbtalk.pdf\r\n\r\nI invite you to first watch the full but slightly earlier version of the talk at \r\nPythonLinks.info/zodb\r\n\r\nAnd then read the following summary to see what else is being added to the talk. \r\n\r\n\r\nThe ZODB is a mature graph database written in Python and optimized in C.  Just subclass off of class Persistent Object and Persistent Container, and your objects, graphs and applications become persistent. \r\n\r\nThe market for Graph Databases has recently exploded, as evidenced by over $200 Million invested in graph database companies.  Most of the graph databases are written in Java.  \r\n\r\nIf you are a Python developer, you will find much greater productivity using a graph database written in Python, than one written in statically bound Java.  You cannot add or remove an attribute to an object at run-time in a statically typed language. Furthermore, the major Java databases constrain you to one of several persistent data types.  Persistent Python, supported by the ZODB allows you to make any Python data structure persistent. Publishing JSON, YAML and Pickles are well supported.  GraphQL is conceptually very close to the ZODB schema approach. \r\n\r\nOkay, the ZODB is interesting, but is it risky?  The ZODB is mature, rock solid and well supported. The ZODB is quite heavily used in the Plone world.   Just the government of Brazil has over 100 websites using the ZODB.  That includes the President's office, parliament and many other governmet offices.   Recently the ZODB has been reengineered.  It now supports thousands of write transactions per second. \r\n\r\nThe major applications of graph databases are fraud detection, social networks and computer networks.  NLP is an interesting application area. \r\n\r\nThe talk reviews the basic concepts of traversal and views on objects.  \r\n\r\nIt is important to understand the basics of how objects are stored on disk. Objects are pickled.  There are multiple ways to store those pickles.     When using File Storage, the objects in a transaction are appended to he end of the database files.   When using relstorage, a record is created with the object id, the version number, and the pickle.  The talk reviews how objects are distributed across multiple Python processes.  With ZEO the pickles are served across the network. Connections are encrypted.  The talk also discusses how to build real-time (chat and iOT) applications using the MQTT message broker with the ZODB.  \r\n\r\nPerformance, scalability, and number of objects,  are all discussed.  Comparisons are made to traditional relational databases. \r\n\r\nThe ZODB Demo makes it very easy to start building your own applications on top of the ZODB. You can start by customizing the TreeLeaf, TreeBranch and TreeRoot classes and their templates. You get CRUD for free. \r\n \r\nThe demo includes traditional relational CRUD, Create, Read, Update, and Delete.  But it also includes the extended graph CRUD.  Rename a Leaf or Branch.  Cut and paste leaves or branches, copy and paste leaves or branches.  View  and restore historic versions are demonstrated.\r\n\r\nOf course the real reason to use a graph database is to improve the user experience.  A basic concept in human factors is to limit lists to 7 items.  That is why librarians use hierarchy.  The Panama Papers journalists said a graph database was more intuitive.  Have you ever selected your country from a list of 150 countries.  Much better to use a hierarchical list.  Have you ever used a Google map with thousands of pins.  Much better to have one page for each city. \r\n\r\nAnd of course the most important reason for using a graph database is not what the software does, but how it changes how we humans think about our problems, and how we make decisions.  Graph databases enable a different approach to distributing applications across the network.  They encourage a different approach to managing the git development process.  They enable a different set of decisions to be made.  \r\n\r\nBy the end of this talk, readers should have a much better appreciation for the rich but little known and under appreciated ZODB ecosystem.",
        "duration": "00:30",
        "speakers": "Christopher Lozinski",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Web",
        "domain": "some",
        "tweet": "ZODB: The GraphDatabase for Python Developers",
        "the_speakers": [
            {
                "name": "Christopher Lozinski",
                "biography": "Christopher lozinski is a serial entrepreneur and MIT graduate.  Rather than seek venture capital he moved his ompany to Poland. MQTT is used for hierarchicalchat onPythonLinks.info.",
                "homepage": "http://PythonLinks.info",
                "@twitter": "@PythonLinks"
            }
        ],
        "popularity": 3,
        "slot_time": "thu-11:20",
        "slot_date": "thursday 11:20",
        "slot_code": "thu-11:20-cubus",
        "room": "cubus",
        "slug": "zodb-the-graph-database-for-pythondevelopers"
    },
    "thu-11:20-media": {
        "code": "TUMAN8",
        "title": "Reproducibility, and Selection Bias in Machine Learning",
        "state": "confirmed",
        "abstract": "In this talk I will provide a solid introduction to the topics of reproducibility and selection bias, with examples taken from the biomedical research, in which reliability of estimator models is paramount.",
        "description": "_Reproducibility_ - the ability to recompute results \u2014 and _replicability_\u2014\r\nthe chances other experimenters will achieve a consistent result[1]- are\r\namong the main important beliefs of the _scientific method_.\r\n\r\nSurprisingly, these two aspects are often\r\nunderestimated or not even considered when setting up scientific\r\nexperimental pipelines. In this, one of the main threat to replicability\r\nis the _selection bias_, that is the\r\nerror in choosing the individuals or groups to take part in a study.\r\nSelection bias may come in different flavours: the selection of the\r\npopulation of samples in the dataset (_sample bias_);\r\nthe selection of features used by the learning models,\r\nparticularly sensible in case of high dimensionality; the selection\r\nof hyper parameter best performing on specific dataset(s).\r\nIf not properly considered, the selection bias may strongly affect the\r\nvalidity of derived conclusions, as well as the reliability of the learning\r\nmodel.\r\n\r\nIn this talk I will provide a solid introduction to the topics of\r\nreproducibility and selection bias, with examples taken from the\r\nbiomedical research, in which reliability is paramount.\r\n\r\nFrom a more technological perspective, to date the scientific Python\r\necosystem still misses tools to consolidate the experimental pipelines in\r\nin research, that can be used together with Machine and Deep learning frameworks\r\n(e.g. `sklearn` and `keras`).\r\nIn this talk, I will present `reproducible-lern`, a new Python frameworks for\r\nreproducible research to be used for machine and deep learning.\r\n\r\nDuring the talk, the main features of the framework will be presented,\r\nalong with several examples, technical insights and implementation\r\nchoices to be discussed with the audience.\r\n\r\nThe talk is intended for *intermediate* PyData researchers and practitioners.\r\nBasic prior knowledge of the main Machine Learning concepts is assumed\r\nfor the first part of the talk.\r\nOn the other hand, good proficiency with the Python language and with\r\nscientific python libraries (e.g. `numpy`, `sklearn`) are required for\r\nthe second part.\r\n\r\n\r\n\r\n--\r\n[1](http://www.pnas.org/content/112/6/1645.full)\r\n_Reproducible research can still be wrong: Adopting a prevention approach_ by\r\nJeffrey T. Leek, and Roger D. Peng\r\n\r\n[2](https://www.cancer.gov/publications/dictionaries/cancer-terms?CdrID=44087)\r\nDictionary of Cancer Terms -> \"selection bias\"",
        "duration": "00:30",
        "speakers": "Valerio Maggio",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Algorithms, Machine Learning, Science",
        "domain": "some",
        "tweet": "Reproducibility and selection bias in Machine Learning",
        "the_speakers": [
            {
                "name": "Valerio Maggio",
                "biography": "Valerio Maggio has a Ph.D. in Computational Science from the University of Naples \u201cFederico II\u201d and he is currently a Postdoc researcher in the FBK \u2013 MPBA team in Trento, Italy.  His research interests are focused on Machine Learning, Deep Learning and Big Data Analysis. \r\nValerio is very much involved in the scientific Python community, and he is an active speaker at many Python conference including EuroPython and EuroScipy. \r\nHe uses Python as the mainstream language for his deep/machine learning code, \r\nmaking an intensive use of (especially) Numpy-based packages (e.g. scikit-learn) to crunch, munge, and analyse data.\r\nValerio is a member of the Italian Python community, and the lead organiser of the PyData Italy conferences, held in Florence since 2015. He also enjoys playing basketball and drinking tea.",
                "homepage": "http://",
                "@twitter": "@leriomaggio"
            }
        ],
        "popularity": 4,
        "slot_time": "thu-11:20",
        "slot_date": "thursday 11:20",
        "slot_code": "thu-11:20-media",
        "room": "media",
        "slug": "reproducibility-and-selection-bias-in-machine-learning"
    },
    "thu-11:20-lounge": {
        "code": "WSMVVU",
        "title": "Satellite Image Segmentation Photovoltaic Potential Estimation",
        "state": "confirmed",
        "abstract": "Using Google Cloud Machine Learning Engine to built a model that help in estimating photovoltaic system design. An end-to-end python based deep learning application.",
        "description": "The used technologies are python based and include:\r\nMongoDB\r\ntensorflow\r\nFlask\r\ngoogle.cloud python API\r\n\r\nA dataset of labelled satellite images is created. \r\nSeveral networks are trained and tested on this dataset.\r\nThe network is deployed on a production server.\r\n\r\nThe results of the classification/segmentaion are used to feed python based photovotlaic simulation libaries. The output is displayed and the results (the potential) evaluated.",
        "duration": "00:30",
        "speakers": "Johannes Oos",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Artificial Intelligence, Computer Vision, Deep Learning & Artificial Intelligence, Machine Learning, Science",
        "domain": "some",
        "tweet": "solarPotential",
        "the_speakers": [
            {
                "name": "Johannes Oos",
                "biography": "About the Author:\r\n\u2022\t10 years of experience in the solar industry (majorly Europe and East Africa)\r\n\u2022\t3 years of experience in Software Development and Artificial Intelligence\r\n\u2022\tPresentation @ Geopython 2018 on Classification of Satellite Images\r\n\u2022\tMasterthesis on the Estimation of the Potential of Roof Top Solar Systems in Luxemburg\r\n\u2022\tDiplomathesis on Measurement and Simulation of a solar pumping station in Egypt",
                "homepage": "http://",
                "@twitter": ""
            }
        ],
        "popularity": 3,
        "slot_time": "thu-11:20",
        "slot_date": "thursday 11:20",
        "slot_code": "thu-11:20-lounge",
        "room": "lounge",
        "slug": "satellite-image-segmentation-photovoltaic-potential-estimation"
    },
    "thu-11:20-lecture": {
        "code": "ASWMKL",
        "title": "Python with and without Pants",
        "state": "confirmed",
        "abstract": "This is a talk about building, packaging, and deploying Python code with Pants and PEX (https://www.pantsbuild.org).",
        "description": "What is the best outfit for comfortable, but highly productive programming at home? While this is definitely an important question, this talk will focus on a topic that is slightly more controversial: monorepos and their build tools. Specifically, the talk will have a closer look at Pants (https://www.pantsbuild.org).\r\n\r\nPants is a build system for large or rapidly growing code bases. It supports all stages of a typical build ( bootstrapping, dependency resolution, compilation, linting, ...) and allows users to organize their code via targets for binaries, libraries, and tests. For Python programmers, pants is especially interesting, as it makes the manipulation and distribution of hermetically sealed Python environments painless - so called PEXes.\r\n\r\nThe talk will motivate Pants and its usage in the context of a large company-wide monorepo. It will then focus on important Python-centric features, and shortly explain how those work under the hood. The talk will conclude with a discussion of usecases for Pants outside of a monorepo, i.e. for the rest of us.",
        "duration": "00:30",
        "speakers": "Stephan Erb",
        "submission_type": "Talk",
        "pyskill": "expert",
        "domains": "DevOps, Infrastructure",
        "domain": "some",
        "tweet": "Python with and without Pants - Building and packaging Python with Pants and PEX",
        "the_speakers": [
            {
                "name": "Stephan Erb",
                "biography": "Stephan Erb is a software engineer driven by the goal to make Blue Yonder's data scientists more productive. Stephan holds a master's degree in computer science from the Karlsruhe Institute of Technology. He is a PMC member of the Apache Aurora project and tweets at @ErbStephan.",
                "@twitter": "@ErbStephan"
            }
        ],
        "popularity": 5,
        "slot_time": "thu-11:20",
        "slot_date": "thursday 11:20",
        "slot_code": "thu-11:20-lecture",
        "room": "lecture",
        "slug": "python-with-and-without-pants"
    },
    "thu-11:55-cubus": {
        "code": "8Y7JHD",
        "title": "Interactive Visualization of Traffic Data using Bokeh",
        "state": "confirmed",
        "abstract": "How can you use Python to create an interactive and comprehensive visualization\r\nof Traffic or other GIS Data on a map? In this talk, we will show how one can\r\nuse the tools of the Python Data Analysis and Visualization landscape to\r\nachieve this goal.",
        "description": "This talk covers the creation of highly interactive and dynamic visualization\r\n(as HTML) using Python, that can still be opened with any modern browser. Using\r\nreal-world examples we will show our usual workflow for processing and creating\r\nvisualizations using Bokeh. The following topics will be covered:\r\n\r\n* Quick Introduction into Transport Modelling\r\n* Performing GIS Data Analysis and Processing with GeoPandas, Pandas and Dask\r\n* Plotting Points, Lines and Polygons on Geographic Maps using Bokeh\r\n* How to use Javascript-Callbacks to make the plot interactive and dynamic,\r\nbut still exportable to static HTML\r\n* Visualizing huge amounts of data on a map with DataShader",
        "duration": "00:30",
        "speakers": "Dr. Patrik Hlobil",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Data Science, Visualisation, Web",
        "domain": "some",
        "tweet": "How can you use Python to create an interactive and comprehensive visualization of Traffic or other GIS Data on a map? Join this talk, where we will show how one can use the tools of the Python Data Analysis and Visualization landscape to achieve this goal.",
        "the_speakers": [
            {
                "name": "Dr. Patrik Hlobil",
                "biography": "Dr. Patrik Hlobil is Traffic Data Analysist and Transport Modeller at PTV AG.\r\nHe helps with the analysis and interpretation of new Traffic Data Sources and\r\ntheir integration in Traffic Products and Software.\r\nBefore working at PTV Patrik Hlobil studied Physics at the Karlsruhe Insitute\r\nof Technology with focus on the theoretical and computational modelling of\r\nUnconventional Superconductors, a field which he also investigated in his PhD."
            }
        ],
        "popularity": 3,
        "slot_time": "thu-11:55",
        "slot_date": "thursday 11:55",
        "slot_code": "thu-11:55-cubus",
        "room": "cubus",
        "slug": "interactive-visualization-of-traffic-data-using-bokeh"
    },
    "thu-11:55-media": {
        "code": "EZC8FK",
        "title": "Data science complexity and solutions in real industrial projects",
        "state": "confirmed",
        "abstract": "Due to the complexity associated with data, using machine learning in real-world scenarios is difficult. I\u2019d like to give an insight into how we tackle this task based on examples of real projects in an industrial environment.",
        "description": "As data scientists we usually like to apply fancy machine learning models to well-groomed datasets. Everyone working on industrial problems will eventually learn, that this does not reflect reality. The amount of time spent on modeling is small compared to data gathering, -warehousing and -cleaning. Even after training and deployment of the model, the work is not done. Continuous monitoring of the performance and input data is still necessary.\r\n\r\nIn this talk I discuss how important data handling is for successful data science projects. Each milestone, from finding the business case to continuously monitoring the performance of the solution, is addressed. This is exemplary shown on a project, with the goal of improving a productive system.",
        "duration": "00:30",
        "speakers": "Artur Miller",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Algorithms, Big Data, Data Science, Infrastructure, Machine Learning",
        "domain": "some",
        "tweet": "Due to the complexity associated with data, using machine learning in real-world scenarios is difficult. I\u2019d like to give an insight into how we tackle this task based on examples of real projects in an industrial environment.",
        "the_speakers": [
            {
                "name": "Artur Miller",
                "biography": "Artur Miller works as Data Scientist for ROSEN Technology & Research Center in Lingen, Germany. He has a M.Sc. in Electrical Engineering. His main tasks are solving machine learning-, optimization- and robotics problems. He likes spending his time writing Python code and blogging about real world machine learning problems.",
                "homepage": "http://www.rosen-group.com",
                "@twitter": "@arturmillerblog"
            }
        ],
        "popularity": 3,
        "slot_time": "thu-11:55",
        "slot_date": "thursday 11:55",
        "slot_code": "thu-11:55-media",
        "room": "media",
        "slug": "data-science-complexity-and-solutions-in-real-industrial-projects"
    },
    "thu-11:55-lounge": {
        "code": "NP9PRT",
        "title": "Measuring the hay in the haystack: quantifying hidden variables using Bayesian Inference",
        "state": "confirmed",
        "abstract": "Monitoring performance metrics in network traffic is important in technology-driven trading, but not every metric is visible to measure. Here I discuss how to tackle this problem via Bayesian inference using PyMC3 package.",
        "description": "Technology-driven trading is a field with many challenges, and performance and availability of the network communication is essential to the business. To have a good understanding on the performance and availability, we monitor certain metrics - however not every interesting metric is readily available to measure. Some of these have to be inferred from the data we see in production by incorporating our own knowledge. What complicates this further is that the relationship between the hidden variables and the output data is not a deterministic one, as we are often dealing with a stochastic system. \r\n\r\nBayesian inference is a suitable way to tackle this issue - it allows encoding our knowledge as a prior distribution of the model parameters. Here we will go through real-world uses of Bayesian inference at IMC, using PyMC3 to make an estimate for the hidden metrics in the network traffic. \r\n\r\nKnowledge: No prior knowledge of PyMC3 is required. Since this is a short presentation, the talk with approach the problem and the solution at a high level instead of implementation details.",
        "duration": "00:15",
        "speakers": "Omer Yuksel",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Data Science, Networks",
        "domain": "some",
        "tweet": "Measuring the hay in the haystack - how to monitor metrics that are interesting but not directly measurable?",
        "the_speakers": [
            {
                "name": "Omer Yuksel",
                "biography": "Omer Yuksel is a Data Scientist in IMC, responsible for developing models and Python tools for monitoring and predicting network performance metrics. He has a background in applied mathematics and computer engineering, and worked on research projects in social network analysis and data-driven methods for network security.",
                "homepage": "http://",
                "@twitter": ""
            }
        ],
        "popularity": 3,
        "slot_time": "thu-11:55",
        "slot_date": "thursday 11:55",
        "slot_code": "thu-11:55-lounge",
        "room": "lounge",
        "slug": "measuring-the-hay-in-the-haystack-quantifying-hidden-variables-using-bayesian-inference"
    },
    "thu-11:55-lecture": {
        "code": "LRM8PG",
        "title": "What's new in Python 3.7?",
        "state": "confirmed",
        "abstract": "Scheduled for release in mid-June after the conference, Python 3.7 is shaping up to be a feature-packed release! This talk will cover all the new features of note that will be making their debut in Python 3.7.",
        "description": "Scheduled for release in mid-June after the conference, Python 3.7 is shaping up to be a feature-packed release! \r\nThis talk will cover all the new features of Python 3.7, including the Data Classes and the Context Variables for the asynchronous programming with asyncio.",
        "duration": "00:30",
        "speakers": "Stephane Wirtel",
        "submission_type": "Talk",
        "pyskill": "expert",
        "domains": "Community, Django, DevOps",
        "domain": "expert",
        "tweet": "What's new in Python 3.7?",
        "the_speakers": [
            {
                "name": "Stephane Wirtel",
                "biography": "Contributor of CPython, Stephane is a member of the EuroPython team since 2015, he likes to improve his skills with best practices, TDD, CI, etc.... Python developer since 2001 and member of the PSF, EuroPython and AFPy, he is also a freelancer in the wonderful Python world.",
                "homepage": "http://",
                "@twitter": "@matrixise"
            }
        ],
        "popularity": 5,
        "slot_time": "thu-11:55",
        "slot_date": "thursday 11:55",
        "slot_code": "thu-11:55-lecture",
        "room": "lecture",
        "slug": "what-s-new-in-python-3-7"
    },
    "thu-14:00-cubus": {
        "code": "KCTMMV",
        "title": "Advanced Analytics Today: From Open Source Integration to the Operationalization of the Analytic Lifecycle",
        "state": "confirmed",
        "abstract": "The times when a single statistician in companies created analytical models and put them into production in person as a whole, have long been a thing of the past. Today, our customers are facing important challenges: the interaction of SAS data scientists and open source programmers in solving a technical problem and the orderly and regulated transfer of analytical models to production.\r\nThe talk will show how SAS 'analytical platform covers the entire spectrum, from modeling through different types of users to the direct operationalization of models.",
        "description": "The times when a single statistician in companies created analytical models and put them into production in person as a whole, have long been a thing of the past. Today, our customers are facing important challenges: the interaction of SAS data scientists and open source programmers in solving a technical problem and the orderly and regulated transfer of analytical models to production.\r\nThe talk will show how SAS 'analytical platform covers the entire spectrum, from modeling through different types of users to the direct operationalization of models.",
        "duration": "00:45",
        "speakers": "Martin Sch\u00fctz",
        "submission_type": "Talk",
        "the_speakers": [
            {
                "name": "Martin Sch\u00fctz",
                "biography": "tba"
            }
        ],
        "popularity": null,
        "slot_time": "thu-14:00",
        "slot_date": "thursday 14:00",
        "slot_code": "thu-14:00-cubus",
        "room": "cubus",
        "slug": "advanced-analytics-today-from-open-source-integration-to-the-operationalization-of-the-analytic-lifecycle"
    },
    "thu-14:00-media": {
        "code": "Z7FXM3",
        "title": "PyTorch as a scientific computing library: past, present and future",
        "state": "confirmed",
        "abstract": "Python is very well known for its ecosystem of mature scientific computing packages. Despite that, the rapidly rising popularity of deep learning resulted in creation of a number of new libraries, including PyTorch. Although originally they were meant to provide better support for those domain specific use cases, one can come to a conclusion, that they can actually have wider applications.",
        "description": "Python is very well known for its ecosystem of mature scientific computing packages. Despite that, the rapidly rising popularity of deep learning resulted in creation of a number of new libraries, including PyTorch. Although originally they were meant to provide better support for those domain specific use cases, one can come to a conclusion, that they can actually have wider applications.\r\n\r\nIn this talk, I\u2019ll showcase the main ideas behind PyTorch - a relatively new library focusing on usability and good integration with other Python packages. I\u2019ll cover some interesting use cases, ranging from ones more specific to machine learning, to those more generally applicable in other scientific computing areas. I\u2019ll also cover some recently added features, and talk a bit about our future roadmap.",
        "duration": "00:45",
        "speakers": "Adam Paszke",
        "submission_type": "Talk",
        "the_speakers": [
            {
                "name": "Adam Paszke",
                "biography": "Author of PyTorch. Machine Learning, Algorithmics, FP, Math. CS & Mathematics student at MIMUW."
            }
        ],
        "popularity": 5,
        "slot_time": "thu-14:00",
        "slot_date": "thursday 14:00",
        "slot_code": "thu-14:00-media",
        "room": "media",
        "slug": "pytorch-as-a-scientific-computing-library-past-present-and-future"
    },
    "thu-14:00-lounge": {
        "code": "C9VEMT",
        "title": "Beyond Jupyter Notebooks - Building your own Data Science platform with Python & Docker",
        "state": "confirmed",
        "abstract": "This talk will interactively show a potential reference architecture for a self-build data science platform using Python & Docker. The presented services tackle common data science pain points such as model persistence, concurrent model training, scheduled model retraining, model exposure, etc.",
        "description": "Interactive notebooks like Jupyter have become more and more popular in the recent past and build the core of many data scientist's workplace. Being accessed via web browser they allow scientists to easily structure their work by combining code and documentation. \r\n\r\nYet notebooks often lead to isolated and disposable analysis artefacts. Keeping the computation inside those notebooks does not allow for convenient concurrent model training, model exposure or scheduled model retraining. \r\n\r\nThose issues can be addressed by taking advantage of recent developments in the discipline of software engineering. Over the past years containerization became the technology of choice for crafting and deploying applications. Building a data science platform that allows for easy access (via notebooks), flexibility and reproducibility (via containerization) combines the best of both worlds and addresses Data Scientist's hidden needs.",
        "duration": "00:45",
        "speakers": "Joshua G\u00f6rner",
        "submission_type": "Talk",
        "pyskill": "professional",
        "domains": "Artificial Intelligence, Algorithms, Data Science, DevOps, Infrastructure, Jupyter, Machine Learning, Programming, Python",
        "domain": "not required",
        "tweet": "#beyondjupyter",
        "the_speakers": [
            {
                "name": "Joshua G\u00f6rner",
                "biography": "After 5 years of experience in the pharmaceutical industry, Joshua Goerner switched into the automotive industry as a Data Scientist for BMW AG. In his current position he is specialised on working with sensor data extracted from connected vehicles. His major research interests cover the reproducibility of data science projects and the fusion of data science and modern software engineering.",
                "homepage": "http://",
                "@twitter": ""
            }
        ],
        "popularity": 3,
        "slot_time": "thu-14:00",
        "slot_date": "thursday 14:00",
        "slot_code": "thu-14:00-lounge",
        "room": "lounge",
        "slug": "beyond-jupyter-notebooks-building-your-own-data-science-platform-with-python-docker"
    },
    "thu-14:00-lecture": {
        "code": "VJ7SY7",
        "title": "Python on the blockchain: Triumphs and tribulations in a crypto startup",
        "state": "confirmed",
        "abstract": "In this presentation, we outline the utility of Python and Django as backend components in a blockchain/cryptocurrency startup company. After a brief explanation of blockchain systems in general, we provide an overview of and justification for the core components of our stack, discuss encountered problems, and highlight emerging trends in the Python/crypto space.",
        "description": "While many cryptographic components of blockchain protocols can be extremely complex, blockchain systems themselves are relatively easy to understand when viewed from a distance. To take the example of Bitcoin, users store digital currency in hardware or software wallets, and use private keys to sign and broadcast transactions. Broadcasted transactions are grouped together into a block through a cryptographic process known as mining, with miners rewarded through the collection of transaction fees and the issuance of new coins. The mined block of transactions is appended to the existing chain, and verified by a global network of nodes. This process repeats in perpetuity, with each newly added block adding to the trustedness and security of data stored on the chain.\r\n\r\nIncreased interest in and demand for cryptocurrencies has brought about a need for places where digital assets can easily be bought, sold or traded. Our platform, Bitpanda, accomplishes this with a backend written in Python, and relying heavily on Django and MySQL databases. In our presentation, we begin by providing a brief overview of how blockchains work. Following this, we describe the Python architecture that (e.g.) generates cryptocurrency wallets, builds, signs and sends transactions, and monitors blockchains for new, relevant data. Key challenges, solutions and failures encountered during the development of the system, and growth of our team, are presented.\r\n\r\nThroughout our talk, we also highlight a number of broader social implications of blockchains, and our work with them. More specifically, we describe the need for open-innovation based approaches to blockchain development, the value of open-source within the blockchain community, and the current lack of critical discourse surrounding the potential uses of blockchains as mechanisms of surveillance and control.",
        "duration": "00:45",
        "speakers": "Daniel and Lorb",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Business & Start-Ups, Community, Django, Infrastructure, Python",
        "domain": "not required",
        "tweet": "How we use Python to manage blockchains, cryptocurrencies, wallets and transactions",
        "the_speakers": [
            {
                "name": "Daniel and Lorb",
                "biography": "I'm Daniel, originally from Australia. I'm a functional linguist by training, but my enthusiasm for programming and the blockchain led me to leave academia for a position as a developer at Bitpanda, in Vienna. I also maintain a guest researcher position at the Austrian Academy of Sciences, working at the intersection of language, computing and innovation research. I'm co-presenting with my colleague and dear friend, Lorb, Austrian born, who also has a background in the humanities and social sciences, and who was responsible for our company's much-needed backend upgrade from PHP to Python.",
                "homepage": "http://bitpanda.com",
                "@twitter": "@interro_gator"
            }
        ],
        "popularity": 4,
        "slot_time": "thu-14:00",
        "slot_date": "thursday 14:00",
        "slot_code": "thu-14:00-lecture",
        "room": "lecture",
        "slug": "python-on-the-blockchain-triumphs-and-tribulations-in-a-crypto-startup"
    },
    "thu-14:00-openhub": {
        "code": "YKNWUB",
        "title": "Designing RESTful APIs",
        "state": "confirmed",
        "abstract": "APIs are all around. Everyone talks about RESTful APIs, but what does \u201cRESTful\u201d really mean?\r\n\r\nThis hands-on workshop takes you though everything that you need to know to design great RESTful APIs.",
        "description": "During the workshop, the participants will understand the key concepts behind RESTful APIs, critically examine some of the popular APIs, design an API from scratch and see how APIs evolve. We\u2019ll also take couple of popular APIs, rip them apart and design a better version of them. \r\n\r\nBy the end of this course, you\u2019ll have good understanding of RESTful APIs and start judging every API that you encounter.",
        "duration": "01:30",
        "speakers": "Anand Chitipothu",
        "submission_type": "Workshop",
        "pyskill": "expert",
        "domains": "Python, Web",
        "domain": "expert",
        "tweet": "APIs are all around. Everyone talks about RESTful APIs, but what does \u201cRESTful\u201d really mean?",
        "the_speakers": [
            {
                "name": "Anand Chitipothu",
                "biography": "Anand has been crafting beautiful software since a decade and half. He\u2019s now building a data science platform, rorodata, which he recently co-founded. He regularly conducts advanced programming courses through Pipal Academy. He is co-author of web.py, a micro web framework in Python. He has worked at Strand Life Sciences and Internet Archive.",
                "homepage": "http://https://anandology.com/",
                "@twitter": "anandology"
            }
        ],
        "popularity": null,
        "slot_time": "thu-14:00",
        "slot_date": "thursday 14:00",
        "slot_code": "thu-14:00-openhub",
        "room": "openhub",
        "slug": "designing-restful-apis"
    },
    "thu-14:50-cubus": {
        "code": "JPCMQV",
        "title": "Processing Geodata using Python",
        "state": "confirmed",
        "abstract": "In this talk it is shown how to analyze, manipulate and visualize geospatial data using Python and the Jupyter Notebook.",
        "description": "There is a large amount of Python modules available suitable for spatial data processing. In this talk, it is shown how to analyze, manipulate and visualize geospatial data by using open source modules.\r\nThe following modules will be introduced:\r\n- **Shapely** - Manipulation and analysis of geometric objects\r\n- **Fiona** - The pythonic way to handle vector data\r\n- **rasterio** - The pythonic way to handle raster data\r\n- **pyproj** - transforming spatial reference systems\r\n- **GeoPandas** - Geospatial analysis\r\n- **Folium** - Creating maps",
        "duration": "00:30",
        "speakers": "Martin Christen",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Big Data, Data Science, Jupyter, Python, Visualisation",
        "domain": "not required",
        "tweet": "#GeoProcessing using #Python and open source modules #bigdata #geodata #geopython",
        "the_speakers": [
            {
                "name": "Martin Christen",
                "biography": "Martin Christen is a professor of Geoinformatics and Computer Graphics at the Institute of Geomatics at the University of Applied Sciences Northwestern Switzerland (FHNW). His main research interests are geospatial Virtual- and Augmented Reality, 3D geoinformation, and interactive 3D maps. \r\nMartin Christen is very active in the Python community. He teaches various Python-related courses and uses Python in most research projects. He organizes the PyBasel meetup - the local Python User Group Northwestern Switzerland. He also organizes the yearly GeoPython conference. He is a board member of the Python Software Verband e.V.",
                "homepage": "http://www.fhnw.ch/habg/igeo",
                "@twitter": "@MartinChristen"
            }
        ],
        "popularity": 4,
        "slot_time": "thu-14:50",
        "slot_date": "thursday 14:50",
        "slot_code": "thu-14:50-cubus",
        "room": "cubus",
        "slug": "processing-geodata-using-python"
    },
    "thu-14:50-media": {
        "code": "PSG873",
        "title": "Fulfilling Apache Arrow's Promises: Pandas on JVM memory without a copy",
        "state": "confirmed",
        "abstract": "Apache Arrow's promise was to reduce the (serialization & copy) overhead of working with columnar data between different systems. Using the latest Pandas release and Arrow's ability to share memory between the JVM and Python as ingredients, we demonstrate that Arrow can fulfill this bold statement. The performance benefits of this will be shown using a typical data engineering use-case that produces data in the JVM and then passes it on to a Python-based machine learning model.",
        "description": "Apache Arrow established a standard for columnar in-memory analytics to redefine the performance and interoperability of most Big Data technologies in early 2016. Since then implementations in Java, C++, Python, Glib, Ruby, Go, JavaScript and Rust have been added. Although Apache Arrow (`pyarrow`) is already known to many Python/Pandas users for reading Apache Parquet files, its main benefit is the cross-language interoperability. With feather and PySpark, you can already benefit from this in Python and R/Java via the filesystem or network. While they improve data sharing and remove serialization overhead, data still needs to be copied as it is passed between processes.\r\n\r\nIn the 0.23 release of Pandas, the concept of ExtensionArrays was introduced. They allow the extension of Pandas DataFrames and Series with custom, user-defined typed. The most prominent example is `cyberpandas` which adds an IP dtype that is backed by the appropriate representation using NumPy arrays. These ExtensionArrays are not limited to arrays backed by NumPy but can take an arbitrary storage as long as they fulfill a certain interfaces. Using Apache Arrow we can implement ExtensionArrays that are of the same dtype as the built-in types of Pandas but memory management is not tied to Pandas' internal BlockManager. On the other hand Apache Arrow has a much more wider set of efficient types that we can also expose as an ExtensionArray. These types include a native string type as well as a arbitrarily nested types such as `list of \u2026` or `struct of (\u2026, \u2026, \u2026)`.\r\n\r\nTo show the real-world benefits of this, we take the example of a data pipeline that pulls data from a relational store, transforms it and then passes it into a machine learning model. A typical setup nowadays most likely involves a data lake that is queried with a JVM based query engine. The machine learning model is then normally implemented in Python using popular frameworks like CatBoost or Tensorflow.\r\n\r\nWhile sometimes these query engines provide Python clients, their performance is normally not optimized for large results sets. In the case of a machine learning model, we will do some feature transformations and possibly aggregations with the query engine but feed as many rows as possible into the model. This will lead then to result sets that have above a million rows. In contrast to the Python clients, these engines often come with efficient JDBC drivers that can cope with result sets of this size but then the conversion from Java objects to Python objects in the JVM bridge will slow things down again. In our example, we will show how to use Arrow to retrieve a large result in the JVM and then pass it on to Python without running into these bottlenecks.",
        "duration": "00:30",
        "speakers": "Uwe L. Korn",
        "submission_type": "Talk",
        "pyskill": "expert",
        "domains": "Algorithms, Big Data, Data Science, Parallel Programming",
        "domain": "some",
        "tweet": "Apache Arrow fulfills its promise with Pandas working on data from the JVM",
        "the_speakers": [
            {
                "name": "Uwe L. Korn",
                "biography": "Uwe Korn is a Senior Data Scientist at the German RetailTec company Blue Yonder. His expertise is on building scalable architectures for machine learning services. Nowadays he focuses on the data engineering infrastructure that is needed to provide the building blocks to bring machine learning models into production. As part of his work to provide an efficient data interchange he became a core committer to the Apache Parquet and Apache Arrow projects.",
                "homepage": "http://https://uwekorn.com/",
                "@twitter": "@xhochy"
            }
        ],
        "popularity": 4,
        "slot_time": "thu-14:50",
        "slot_date": "thursday 14:50",
        "slot_code": "thu-14:50-media",
        "room": "media",
        "slug": "fulfilling-apache-arrow-s-promises-pandas-on-jvm-memory-without-a-copy"
    },
    "thu-14:50-lounge": {
        "code": "SP8TP9",
        "title": "Introduction to Docker for Pythonistas",
        "state": "confirmed",
        "abstract": "Docker is a major driver of container virtualization and comes in very handy for your day-to-day work with Python.",
        "description": "My Talk aims to introduce you to Docker and how it works, how you can use prebuild Images from the Docker-Hub and how you can make your own Images.  \r\nIn more Detail, the following Points will be covered:  \r\n\r\n* What is Docker\r\n* Classic Virtualization vs. Container Virtualization\r\n* The Docker-Hub / Using prebuild Images\r\n* You need a local MongoDB/MySQL/whatever? No Problem!\r\n* Make your own Image a.k.a. \u201cThe Dockerfile\u201d\r\n* Build and Ship your Flask-App without \u201cBut it works on my machine!\u201d \u2013 Problems\r\n* Build and Ship your Data Science \u2013 Environment, reproducibility for the win! \r\n* \u2026 The other Side of the Medal \u2026\r\n* More to read and watch!",
        "duration": "00:30",
        "speakers": "Jan Wagner",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Big Data, Data Science, DevOps, Jupyter, Machine Learning",
        "domain": "some",
        "tweet": "Whale meets Snake \u2013 Introduction to Docker for Pythonistas",
        "the_speakers": [
            {
                "name": "Jan Wagner",
                "biography": "Currently Student (M.Sc. in Data Science) at the University of Mannheim. Got a B.Sc. in Business Informatics and a Apprenticeship as IT Management Assistant (Informatikkaufmann). I have been working Full-Time in several different Companies and Industries, mainly at IT-Projectmanagement-Level as Softwaretest-Coordinator but now aiming for a Python-Developer and/or Data Science Position.",
                "homepage": "http://",
                "@twitter": "@generic_data"
            }
        ],
        "popularity": 3,
        "slot_time": "thu-14:50",
        "slot_date": "thursday 14:50",
        "slot_code": "thu-14:50-lounge",
        "room": "lounge",
        "slug": "introduction-to-docker-for-pythonistas"
    },
    "thu-14:50-lecture": {
        "code": "PYBGWL",
        "title": "Data Science meets Data Protection: Keeping your data secure while learning from it.",
        "state": "confirmed",
        "abstract": "We'll look at some easy and hopefully smart ways to keep your data secure and well-protected while working with it: We will investigate techniques such as pseudonymization and anonymization and show you how you can apply them to your data and still get some useful insights from it.",
        "description": "We will discuss anonymization and pseudonymization techniques that you can apply to your data to keep it secure and comply with the law(s) while still being able to gain useful insights from it.\r\n\r\n* Why protect data?\r\n* Pseudonymization vs. anonymization: What's the difference?\r\n* Pseudonymization: Techniques & real-world examples\r\n* Problems and risks when pseudonymizing data\r\n* Anonymization: Approaches & real-world examples\r\n* Problems and risks when anyonymizing data\r\n* Takeaways and summary\r\n\r\nWe will show concrete Python implementations of various techniques and use example data sets to show how applying pseudonymization and anonymization will affect our ability to do machine learning / data science.",
        "duration": "00:30",
        "speakers": "Andreas Dewes, Katharine Jarmul",
        "submission_type": "Talk",
        "pyskill": "expert",
        "domains": "Artificial Intelligence, Business & Start-Ups, Big Data, Data Science",
        "domain": "expert",
        "tweet": "Data Science meets Data Protection: Keeping your data secure while learning from it.",
        "the_speakers": [
            {
                "name": "Katharine Jarmul",
                "biography": "Katharine Jarmul is a data scientist and engineer based in Berlin, Germany. She runs a consulting company Kjamistan where she works with large and small companies to investigate, build and evaluate solutions to data problems. She enjoys teaching, and has made several data-focused online courses and co-authored a book for O'Reilly Media. As a co-founder of PyLadies, she is passionate about diversity in technology and the (Py)Data community. When she's not Pythoning or data wrangling, she's likely cooking, taking photos or plotting how to change the world (on|off)line."
            },
            {
                "name": "Andreas Dewes",
                "biography": "Data Scientist & Founder @ 7scientists. PhD in Experimental Quantum Physics.",
                "homepage": "http://https://www.kiprotect.com",
                "@twitter": "kiprotect"
            }
        ],
        "popularity": 4,
        "slot_time": "thu-14:50",
        "slot_date": "thursday 14:50",
        "slot_code": "thu-14:50-lecture",
        "room": "lecture",
        "slug": "data-science-meets-data-protection-keeping-your-data-secure-while-learning-from-it"
    },
    "thu-15:25-cubus": {
        "code": "GFFGSG",
        "title": "Stretchy - NoSQL Database behind REST API",
        "state": "confirmed",
        "abstract": "A dynamic REST API database. Think of \"elasticsearch\", but brutally simple.",
        "description": "Stretchy is built as a microservice that provides a simple and intuitive REST API with a NoSQL database as backend. No need for database migrations or upfront schema design. The basic CRUD (create, read, update, delete) operations are available for getting data in and out from the database.\r\n\r\nStretchy is free and open source software built with Python 3, using Flask web framework. It currently uses MongoDB as its backend database. Since it is interfaced through the REST API however, Stretchy is technology agnostic and developers can create bindings to other databases, including SQL databases.\r\n\r\nThis presentation reviews the reasons for creating Stretchy, its current applications, an short tutorial on how to use it, and tips on how to deploy it.",
        "duration": "00:15",
        "speakers": "Artur Scholz",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Big Data, Data Science, Web",
        "domain": "not required",
        "tweet": "A dynamic REST API database. Think of \"elasticsearch\", but brutally simple",
        "the_speakers": [
            {
                "name": "Artur Scholz",
                "biography": "My interest is in Space, Programming, CubeSats, Open Source, and Standardization. My mission is to combine those to enable truly open space exploration to all humanity."
            }
        ],
        "popularity": 3,
        "slot_time": "thu-15:25",
        "slot_date": "thursday 15:25",
        "slot_code": "thu-15:25-cubus",
        "room": "cubus",
        "slug": "stretchy-nosql-database-behind-rest-api"
    },
    "thu-15:25-media": {
        "code": "XGUAE3",
        "title": "How to make your (digital) Communication strong & future ready",
        "state": "confirmed",
        "abstract": "Startups and small companies need a smart communication strategy.",
        "description": "Your business is all set up and ready. You want to enter the market with a new product or service. How to make your (digital) Communication strong & future ready.",
        "duration": "00:15",
        "speakers": "Simon Daubermann",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Business & Start-Ups, Community",
        "domain": "some",
        "tweet": "The importance of a smart communication strategy for a successful digital transformation.",
        "the_speakers": [
            {
                "name": "Simon Daubermann",
                "biography": "Simon Daubermann"
            }
        ],
        "popularity": 5,
        "slot_time": "thu-15:25",
        "slot_date": "thursday 15:25",
        "slot_code": "thu-15:25-media",
        "room": "media",
        "slug": "how-to-make-your-digital-communication-strong-future-ready"
    },
    "thu-15:25-lounge": {
        "code": "K7VAR3",
        "title": "Python Birdies: Codegolfing for better understanding (and fun)",
        "state": "confirmed",
        "abstract": "Codegolfing, the art of condensing ones code, seems counterproductive to good programming standards. In this talk, I will argue, with the use of examples, that this need not be the case, that playing with a language can give you better understanding of its internals, and in turn make you a better programmer.",
        "description": "Codegolfing means taking a programming task and trying to answer it with a byte-minimal correct solution. Such an answer often takes shortcuts, is horribly inefficient, and definitely violates almost 100% of PEP 8. Like any playful interaction with a subject, it can however improve your understanding of it, as well as teach you about weird interactions regarding operator precedence, lexer quirks and more.\r\n\r\nAfter going over basic definitions, I will take a small number of well-known or straightforward programming tasks and go through the act of golfing an answer together step by step.",
        "duration": "00:15",
        "speakers": "Jonathan Oberl\u00e4nder",
        "submission_type": "Talk",
        "pyskill": "expert",
        "domains": "Algorithms",
        "domain": "some",
        "tweet": "Golfing your Python code for (mostly) fun",
        "the_speakers": [
            {
                "name": "Jonathan Oberl\u00e4nder",
                "biography": "Jonathan started programming at the tender age of 12, after accidentally buying a book about C++. He quickly moved on to other languages (VBScript, AutoIt, PHP, Javascript, Perl, ...), but it wasn't until his Bachelor studies in Computational Linguistics (Saarland University) that he started learning Python, after having to choose between it and a Java course. Since then, he has mostly stayed true to Python, except for the occasional affair with esoteric programming languages. After finishing his Master's in Cognitive Science (Trento) and Computer Science (Prague), he started working as a full-time Python developer at the German price comparison website billiger.de",
                "@twitter": "L3viathan2142",
                "homepage": "http://"
            }
        ],
        "popularity": 2,
        "slot_time": "thu-15:25",
        "slot_date": "thursday 15:25",
        "slot_code": "thu-15:25-lounge",
        "room": "lounge",
        "slug": "python-birdies-codegolfing-for-better-understanding-and-fun"
    },
    "thu-15:25-lecture": {
        "code": "J9NSJE",
        "title": "Put your data on a map",
        "state": "confirmed",
        "abstract": "Quick overview of tools to render your geo data on a map from jupyter notebook with examples.",
        "description": "When you're working with geo data I found that putting it all on a map helps a lot to see and understand it. I will go over few of the tools I use on a day to day basis that allow you to draw a map. \r\nIt will include few common scenarios and examples.",
        "duration": "00:15",
        "speakers": "Alex Vykaliuk",
        "submission_type": "Talk",
        "the_speakers": [
            {
                "name": "Alex Vykaliuk",
                "biography": "26 years old programmer from Kyiv, Ukraine.\r\nWhen I am at work I lead a team responsible for data and search in a hotel booking tool.\r\nWhen I am not at work I travel, build small projects with maps and eat sandwiches."
            }
        ],
        "popularity": 2,
        "slot_time": "thu-15:25",
        "slot_date": "thursday 15:25",
        "slot_code": "thu-15:25-lecture",
        "room": "lecture",
        "slug": "put-your-data-on-a-map"
    },
    "thu-16:00-cubus": {
        "code": "XDLE7M",
        "title": "Observe all your applications",
        "state": "confirmed",
        "abstract": "This talk will give you an overview how we monitor our full stack from the 2000 physical machines up to the 10,000 parallel running Python application processes, micro-service instances and batch processing jobs.",
        "description": "You just deployed your new version of an application or micro-service; how do you know everything works as expected? You run your comprehensive test suite to verify functional correctness for known scenarios and performance tests before deploying, but does your application really work at the moment or is it just responding with error messages to all incoming requests?\r\n\r\nI\u2019m part of the team that runs a huge infrastructure for the SAP HANA development. This infrastructure is vital for nearly all development & testing activities of SAP HANA. As this infrastructure is powered by multiple in-house developed applications, we immediately want to know if an application starts to fail and we need to be able to quickly diagnose what caused the failure.\r\n\r\nThis talk will give you an overview how we monitor our full stack from the 2000 physical machines up to the 10,000 parallel running Python application processes, micro-service instances and batch processing jobs. It includes a review about the used tools, bad and good examples of instrumentation in Python code, the resulting visualisation and an outlook on upcoming improvements.",
        "duration": "00:30",
        "speakers": "Christoph Heer",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "DevOps, Infrastructure, Networks, Programming, Python",
        "domain": "some",
        "tweet": "Observe your applications to get a good feeling",
        "the_speakers": [
            {
                "name": "Christoph Heer",
                "biography": "I\u2019m an Infrastructure Engineer in the team behind SAP\u2019s huge test infrastructure for SAP HANA. In my spare time, I develop web applications with Django or playing around with other programming languages like Rust.",
                "homepage": "http://",
                "@twitter": "@ChristophHeer"
            }
        ],
        "popularity": 4,
        "slot_time": "thu-16:00",
        "slot_date": "thursday 16:00",
        "slot_code": "thu-16:00-cubus",
        "room": "cubus",
        "slug": "observe-all-your-applications"
    },
    "thu-16:00-media": {
        "code": "3ZBCVR",
        "title": "How to teach space invaders to your computer",
        "state": "confirmed",
        "abstract": "A very brief introduction to reinforcement learning theory followed by a hands on section in which it is demonstrated how to train an algorithm to play space invaders. After this talk you should now what reinforcement learning is and where to dig deeper if you like it.",
        "description": "First things first: playing good old Atari games might be cool but why should I write a program for doing it? Well teaching a computer to play a game means teaching it to develop strategies and use foresight planning to solve a certain problem. The tools you gather while solving i.e. space invaders are the same you may use to solve any problem which requires a sequential set of decisions in order to find an optimal solution to some problem, like i.e. controlling a robot that collects garbage. Furthermore, there is a lot of scientific research on reinforcement learning that focuses on solving Atari games which makes it a good starting point, as large amounts of publications and open source code already exists.\r\n\r\nWhat to expect from this talk? At first there will be a very short introduction to reinforcement learning theory, just the very basics, common applications and some references for further reading. Next points are, how to run Atari games from inside python for a learning task (with OpenAI's gym), and where to find an algorithm for the actual learning problem. Finally it will be shown how to build it all together in a jupyter notebook and let the algorithm play the game. Et voil\u00e0 that's your computer beating you in space invaders.",
        "duration": "00:30",
        "speakers": "David W\u00f6lfle",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Deep Learning & Artificial Intelligence, Jupyter, Python, Science",
        "domain": "some",
        "tweet": "A very brief introduction to reinforcement learning theory followed by a hands on section in which it is demonstrated how to train an algorithm to play space invaders.",
        "the_speakers": [
            {
                "name": "David W\u00f6lfle",
                "biography": "David W\u00f6lfle studied mechanical engineering in Karlsruhe. During his master studies he focused on wind energy technology and researched data sources for wind resource estimation. After graduating as Master of Science from Flensburg University of applied Applied Sciences in 2015, David worked as a R&D Scientist scientist at EWC Weather Consult GmbH (now UBIMET GmbH), where he designed and implemented software components for the estimation and predication of renewable energy power production. In 2016 David has been promoted to a team manager at EWC Weather Consult where he was responsible for the software engineering within the product development and as well as the design and execution of the project management. Besides these duties, he also developed innovative methods for estimating power production of airborne wind energy converters, using high high-resolution meteorological data and machine learning methods.\r\n\r\nSince early 2018 David works as a research scientist at FZI Research Center for Information Technology in the field of smart energy. His research focuses thereby on self-learning energy management systems using reinforcement learning techniques.",
                "homepage": "http://https://www.fzi.de/en/about-us/organisation/detail/address/woelfle/",
                "@twitter": ""
            }
        ],
        "popularity": 5,
        "slot_time": "thu-16:00",
        "slot_date": "thursday 16:00",
        "slot_code": "thu-16:00-media",
        "room": "media",
        "slug": "how-to-teach-space-invaders-to-your-computer"
    },
    "thu-16:00-lecture": {
        "code": "78ANWT",
        "title": "Creating an inclusive corporate culture",
        "state": "confirmed",
        "abstract": "Having a tech career as a minority is challenging. It could mean being the only one to speak against the popular opinion, or becoming more visible to get the same level of recognition. What can we do on the corporate level to make sure everyone feels welcome and retain these talents?\r\nCreating an inclusive corporate culture helps us achieve just that. This talk shares concrete steps that employees and employers can take to improve minorities in tech\u2019s sense of belonging and engagement.",
        "description": "Having a tech career as a minority is challenging. It could mean being the only one to speak against the popular opinion, or becoming more visible to get the same level of recognition. What can we do on the corporate level to make sure everyone feels welcome and retain these talents?\r\nCreating an inclusive corporate culture helps us achieve just that. This talk shares concrete steps that employees and employers can take to improve minorities in tech\u2019s sense of belonging and engagement:\r\n\r\n* Top down approach: establish an Executive Diversity Council that makes up of influential company leaders who deeply care about diversity and inclusion\r\n* Bottom up approach: get involved in a minority in engineering employee resource group \r\n* Start mentorship programs\r\n* Form diverse recruitment panels\r\n* Organize ally trainings",
        "duration": "00:30",
        "speakers": "Yenny Cheung",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Business & Start-Ups, Community",
        "domain": "some",
        "tweet": "The talk \"Creating an inclusive corporate culture\" shares concrete steps that employees and employers can take to improve minorities in tech\u2019s sense of belonging and engagement.",
        "the_speakers": [
            {
                "name": "Yenny Cheung",
                "biography": "Yenny is a full-stack software engineer at Yelp in Hamburg. She is on the Biz National team, where she is scaling advertising tools and reporting for national businesses. She leads the Women in Engineering Group at Yelp in Germany. Yenny has spoken about pythonic refactoring at conferences like PyCon.DE, European Women in Tech, and PyDays Vienna. She was also a guest speaker on a Talk Python podcast episode, which had around 50,000 downloads.",
                "homepage": "http://",
                "@twitter": "yennycheung"
            }
        ],
        "popularity": 3,
        "slot_time": "thu-16:00",
        "slot_date": "thursday 16:00",
        "slot_code": "thu-16:00-lecture",
        "room": "lecture",
        "slug": "creating-an-inclusive-corporate-culture"
    },
    "thu-16:35-cubus": {
        "code": "HDMT9B",
        "title": "Solving Data Science Problems using a Jupyter Notebook and SAP HANA's in-database Machine Learning Libraries",
        "state": "confirmed",
        "abstract": "During this talk we will present how a Data Scientist can work on datasets stored in SAP HANA's Database leveraging in-database machine learning libraries. Data will reside in the database and calcuations with be pushed down to the DB minimizing data transfer to the client.",
        "description": "Companies store their data in databases with highly restricted access regulations. The latest regulatorily changes enforces the need to work on the datasets in this controlled environment without created additional external copies. However Data Scientists prefer to work with tools they are most familiar like Python, R  and Jupyter Notebooks using to a  large amount of open-source packages (numpy, matplotlib, pandas, ..).  SAP HANA provides highly optimized in-database machine learning libraries. In this talk we will present how a Data Scientist can work in an environment he/she is most familiar with and access the data stored in SAP HANA using SAP HANA machine learning libraries with a scikit-learn type interface.  Data will remain in the database and will be exposed as dataframes (similar to Pandas dataframes).  We will explain the software architecture and present a complete end-to-end use case by using a Jupyter Notebook.",
        "duration": "00:30",
        "speakers": "Dr Frank Gottfried",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Big Data, Data Science, Jupyter, Machine Learning, Visualisation",
        "domain": "some",
        "tweet": "Addressing Data Science Problems with Jupyter and SAP HANA's ML libraries",
        "the_speakers": [
            {
                "name": "Dr Frank Gottfried",
                "biography": "Frank Gottfried is a Development Architect at SAP SE. He holds a Ph.D. in Physics from\r\nthe University of Heidelberg and a master\u2019s degree in IP Management and Law from the University of Strasbourg. He joined SAP in 1996 as a software developer and since then has gained experience in\r\nvarious technical and management functions and consulting. For the last few years he\u2019s been \r\nworking on machine learning topics using SAP HANA\u2019s ML libraries and Deep Learning  frameworks (TensorFlow).",
                "homepage": "http://www.sap.com",
                "@twitter": ""
            }
        ],
        "popularity": 1,
        "slot_time": "thu-16:35",
        "slot_date": "thursday 16:35",
        "slot_code": "thu-16:35-cubus",
        "room": "cubus",
        "slug": "solving-data-science-problems-using-a-jupyter-notebook-and-sap-hana-s-in-database-machine-learning-libraries"
    },
    "thu-16:35-media": {
        "code": "3N7DNH",
        "title": "Driving simulation and data analysis of magnetic nanostructures through Jupyter Notebook",
        "state": "confirmed",
        "abstract": "We present ongoing work from a project that makes a particular computer simulation (implemented in C++ and Tk/Tcl) accessible through a Python interface, and through the Jupyter Notebook. The talk describes the motivation, benefits and current status of the project.",
        "description": "We present ongoing work from a project that makes a particular computer\r\nsimulation (implemented in C++ and Tk/Tcl) accessible through a Python\r\ninterface, and through the Jupyter Notebook. The talk describes the\r\nmotivation and current status of the project.\r\n\r\nIn more detail, the computer simulation in question is the Object\r\nOriented Micromagnetic Modelling Framework ([OOMMF](http://math.nist.gov/oommf/)) \r\nwhich is likely the most widely used micromagnetic simulation package. It can be\r\ndriven through a graphical (Tk) user interface or through a configuration\r\nfile that defines a simulation run.\r\n\r\nIn this talk, we first show a Python interface to OOMMF that allows\r\nthe driving of OOMMF simulations from a Python program or interpreter\r\nprompt. This way we embed a widely used scientific code from 1990s in\r\na general purpose programming language [[1](https://doi.org/10.1063/1.4977225)] \r\nand enable the full use of\r\nthe ecosystem of scientific libraries available for Python. For\r\nexample, design optimisation, specialised post-processing, and the\r\ncreation of figures can all be carried out using a single script;\r\nmaking the work more easily reproducible.\r\n\r\nSecond, we integrate the Python interface to OOMMF into a Jupyter\r\nnotebook, so that all existing benefits of using Jupyter are inherited\r\nfor the use in computational micromagnetics, which is the reason we\r\nnamed our code Jupyter-OOMMF ([JOOMMF](http://joommf.github.io/)). \r\nA [JupyterHub installation](https://tryjoommf.soton.ac.uk/) of the \r\ntool reduces barriers in uptake, and all the code is \r\n[on github](https://github.com/joommf). \r\n\r\nWe discuss the benefits of driving computer simulation and data\r\nanalysis through Jupyter Notebooks.\r\n\r\nThis project is a part of the Jupyter-OOMMF (JOOMMF) activity in the\r\n[OpenDreamKit](http://opendreamkit.org/) project and we acknowledge \r\nfinancial support from\r\nHorizon 2020 European Research Infrastructures project (676541). The\r\nwork is also supported by the EPSRC CDT in Next Generation\r\nComputational Modelling EP/L015382/1, and the EPSRC grants\r\nEP/M022668/1 and EP/N032128/1.\r\n\r\nFor additional context: micromagnetic modelling is a key research\r\nmethod in academia and industry to support development of\r\nhigh-capacity magnetic storage devices that are cheap, fast, and\r\nreliable, and to enable research into future alternative storage and\r\nprocessing technologies such as spintronics. The OOMMF modelling\r\npackage has been used in \r\n[over 2500 publications](https://math.nist.gov/oommf/oommf_cites.html) since 1999.\r\n\r\n[1] Beg, M., Pepper, R. A., and Fangohr, H. User interfaces for\r\ncomputational science: A domain specific language for OOMMF embedded\r\nin Python. AIP Advances 7, 056025 (2017), https://doi.org/10.1063/1.4977225",
        "duration": "00:30",
        "speakers": "Hans Fangohr",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Data Science, Jupyter, Programming, Python, Science",
        "domain": "not required",
        "tweet": "Driving simulation and data analysis of tiny magnetic things through Jupyter Notebook",
        "the_speakers": [
            {
                "name": "Hans Fangohr",
                "biography": "Hans Fangohr is a Data Analysis Scientist at the European XFEL GmbH (which hosts the worlds most brilliant X-Ray Free Electron Laser, http://xfel.eu) and Professor of Computational Modelling at the University of Southampton. He received his undergraduate degree \"Diplomphysiker\" in physics from the University of Hamburg (Germany) and completed his PhD studies in the High Performance Computing Group at the department of Computer Science in Southampton. He is a full professor since 2010, and specialised in computational science, data analysis and software engineering for science.",
                "homepage": "http://fangohr.github.io",
                "@twitter": "@ProfCompMod"
            }
        ],
        "popularity": 3,
        "slot_time": "thu-16:35",
        "slot_date": "thursday 16:35",
        "slot_code": "thu-16:35-media",
        "room": "media",
        "slug": "driving-simulation-and-data-analysis-of-magnetic-nanostructures-through-jupyter-notebook"
    },
    "thu-16:35-lecture": {
        "code": "FVLSNE",
        "title": "Designing better drugs with machine learning",
        "state": "confirmed",
        "abstract": "We will show how we use our scikit-learn based toolkit cream to build and validate predictive models for important properties in drug design, such as optimization of biochemical/cellular activity and ADME properties.",
        "description": "Developing safe and efficacious drugs is a multi-parameter optimization process. A good drug needs besides affinity to the receptor or protein target also good ADME (Absorption, Distribution, Metabolism, and Excretion) properties to be successful. Machine learning models are used to assess compound ideas early-on in the design process and give guidance what to synthesize next. We will present our scikit-learn based toolkit cream to build and validate predictive models. Results from validation studies using existing data as well as prospective prediction results will be shared. Special focus will be placed on how confidence in predictions can be increased using cross-validation procedures and by consideration of the domain of applicability of the predictions.",
        "duration": "00:30",
        "speakers": "Daniel Kuhn",
        "submission_type": "Talk",
        "pyskill": "expert",
        "domains": "Artificial Intelligence, Deep Learning & Artificial Intelligence, Data Science, Jupyter, Machine Learning, Science",
        "domain": "expert",
        "tweet": "How machine learning can help to design better drugs in pharmaceutical research",
        "the_speakers": [
            {
                "name": "Daniel Kuhn",
                "biography": "Daniel Kuhn is computational chemist and discovery project leader at Merck in Darmstadt, Germany. After studying pharmacy in Marburg he obtained his PhD from Philipps-University, Marburg in computational chemistry working with Gerhard Klebe on the classification of protein binding sites. In 2004 he joined Boehringer Ingelheim in Vienna as computational chemist working in oncology research. In 2010 he joined Merck as principal scientist contributing to projects in early and late drug discovery stages. He is also leading a lead optimization project towards preclinical development. His research interests include protein kinase drug discovery, structure-based design and machine learning approaches to hit identification and lead-optimization.",
                "homepage": "http://https://www.merckgroup.com/en",
                "@twitter": ""
            }
        ],
        "popularity": 3,
        "slot_time": "thu-16:35",
        "slot_date": "thursday 16:35",
        "slot_code": "thu-16:35-lecture",
        "room": "lecture",
        "slug": "designing-better-drugs-with-machine-learning"
    },
    "thu-17:10-cubus": {
        "code": "LJEWML",
        "title": "Microservices from the trenches: how we delivery fancy sofas across Europe",
        "state": "accepted",
        "abstract": "At made.com we use micro-services written in Python to power our entire backend system and deliver an incredible amount of orders each week.\r\nDuring this presentation I will explain what are micro-services and compare them to monolith applications.",
        "description": "At made.com we use micro-services written in Python to power our entire backend system and deliver an incredible amount of orders each week.\r\nDuring this presentation I will explain what are micro-services and compare them to monolith applications.\r\nBy analysing what are the differences between micro-services and monolith applications I will show why you need them and for which situations they are extremely useful.\r\nI will also talk about what you need on different levels regarding infrastructure, knowledge and experience to get the most out of a micro-services architecture.\r\nThe last part of the presentation is dedicated to the drawbacks of running a micro-services architecture and sharing some solutions.\r\nI will conclude sharing some useful resources about micro services and taking some questions.\"",
        "duration": "00:30",
        "speakers": "Christian Barra",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Data Science, Jupyter",
        "domain": "some",
        "tweet": "Mom stop using Excel! I will teach you Jupyter \ud83c\udf0c",
        "the_speakers": [
            {
                "name": "Christian Barra",
                "biography": "I do Python, conferences and often play with data.",
                "homepage": "http://chrisbarra.xyz",
                "@twitter": "@christianbarra"
            }
        ],
        "popularity": null,
        "slot_time": "thu-17:10",
        "slot_date": "thursday 17:10",
        "slot_code": "thu-17:10-cubus",
        "room": "cubus",
        "slug": "microservices-from-the-trenches-how-we-delivery-fancy-sofas-across-europe"
    },
    "thu-17:10-media": {
        "code": "HW3RYU",
        "title": "Cloud chat bot for lazy people",
        "state": "confirmed",
        "abstract": "These days with chat applications everywhere people tend to ask other people about things there although a direct answer would not be far on a website or it would be the first Google search result. It is understandable because mankind is lazy, so I am. Therefore, I want to let bots answer the easy questions for me.\r\nIn my situation as a DevOps person these questions can be about the health of a service I am responsible for. I will show you how you can run a Python bot with [Azure bot service]( https://azure.microsoft.com/en-us/services/bot-service/), reachable by [Slack]( https://slack.com/) which gives you a service health status based on information from a [Prometheus](https://prometheus.io/) monitoring system.",
        "description": "At work we established Slack years ago as our chat application and by now quite a percentage of communication goes through it. As a result it got much easier to contact one person or a group simultaneously. And this is good as we can share our knowledge save each other time. But it also introduced a category of questions in the chat which only require simple tedious tasks to get the answer and then post it as a response.\r\nOne possibility is to educate and point others to the place where they can find the answer or what tasks they have to do. The other one is use a chat bot for this. Both ways have advantages and for the bot it is that you can import a specific type of response more easily into a conversation without first gathering the information and copy and paste it.\r\nI am a developer and service operator and one category of questions which fits this is the category of service health questions, like \"Does service X has a problem right now?\". Hence, I will use a bot to answer them.\r\nFirst I will show you how you can create a python bot for the Azure bot service. With it the questioner then can either directly use the bot to answer his question or you can just create the response for him without going to the service health monitoring. In this case the service health information has to be obtained from a Prometheus monitoring service and then transformed into a chat message.",
        "duration": "00:30",
        "speakers": "Bj\u00f6rn Meier",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "DevOps, Infrastructure",
        "domain": "some",
        "tweet": "Intro into answering service status questions in Slack with an Azure bot service which utilizes Prometheus health metrics",
        "the_speakers": [
            {
                "name": "Bj\u00f6rn Meier",
                "biography": "Bjoern is a software engineer at Blue Yonder GmbH since 2016 after graduating in Computer Science. More correctly you could say he is a DevOps engineer at Blue Yonder where he is developing and operating - among other things - the services for the external data interfaces, preprocessing and data storage to enable the data scientists to run their prediction models. He loves the versatility and ecosystem of python to write e.g. production web apps, data analysis tools or operational scripts. If there was more free time he would like to spent it to dive deeper into functional programming languages like elixir to have a different view on things.",
                "homepage": "http://https://www.blue-yonder.com",
                "@twitter": "@da_bjoerni"
            }
        ],
        "popularity": 4,
        "slot_time": "thu-17:10",
        "slot_date": "thursday 17:10",
        "slot_code": "thu-17:10-media",
        "room": "media",
        "slug": "cloud-chat-bot-for-lazy-people"
    },
    "thu-17:10-lecture": {
        "code": "YV9WJ3",
        "title": "Where the heck is my memory?",
        "state": "confirmed",
        "abstract": "In this talk I want to introduce you to python memory management and want to convince you that it is worth knowing a few details about it even while writing pure python code. In the end, I want you to leave with a better knowledge about what\u2019s going on and equip you with a few tools and best practices to face the harsh world of memory management.",
        "description": "Memory management is something the common Python user doesn\u2019t need to bother with because the gory details of it are hidden deep within the interpreter itself. The garbage collector takes out the trash and we can spend our precious time bothering with more important things on our minds.\r\nLiving in this encapsulated utopia is nice but sometimes it is worth it to peak behind the curtains to unleash the full power of your application.\r\nIn this talk I want to show you when it is necessary to face this harsh world and convince you that it is in fact not as scary as it may seem. Using real life examples, I\u2019m going to show you how to use the garbage collector and open source tooling to get control over the memory you might not even know you had at your disposal.",
        "duration": "00:30",
        "speakers": "Florian Jetter",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Big Data, DevOps, Infrastructure",
        "domain": "some",
        "tweet": "Where the heck is my memory? - Python memory management in a nutshell",
        "the_speakers": [
            {
                "name": "Florian Jetter",
                "biography": "Florian Jetter is a Data Scientist working at the RetailTech company BlueYonder based in Karlsruhe, Germany. He spends most of his time bridging the gap between machine learning applications and raw data.",
                "homepage": "http://https://tech.blue-yonder.com",
                "@twitter": ""
            }
        ],
        "popularity": 2,
        "slot_time": "thu-17:10",
        "slot_date": "thursday 17:10",
        "slot_code": "thu-17:10-lecture",
        "room": "lecture",
        "slug": "where-the-heck-is-my-memory"
    },
    "fri-10:30-cubus": {
        "code": "GJ9AWL",
        "title": "Build text classification models ( CBOW and Skip-gram) with FastText in python",
        "state": "confirmed",
        "abstract": "NLP is an exciting way to interpret the textual data especially when we know that computer neither speak nor understand any kind of human language. So, how do we represent each word of a language in such a unique numerical pattern and process it in quickest way possible. Answer is FastText library.",
        "description": "FastText has been open-sourced by Facebook in 2016 and with its release, it became the fastest and most accurate library in Python for text classification and word representation. It is to be seen as a substitute for gensim package's word2vec.  It includes the implementation of two extremely important methodologies in NLP i.e Continuous Bag of Words and Skip-gram model. Fasttext performs exceptionally well with supervised as well as unsupervised learning. \r\n\r\nThe tutorial will be divided in following four segments :\r\n\r\n1. 0-10 minutes: The talk will begin with explaining the difference between word embeddings generated by word2vec, Glove, Fasttext and how FastText beats all the other libraries with better accuracy and in lesser time.\r\n\r\n2. 10-25 minutes: The code will be shown and explained line by line for both the models (CBOW and Skip-gram) on a standard textual labeled dataset with the tips on hyper-parametric tuning to get the best possible results.\r\n\r\n3. 25-40 minutes: How to use the pre-trained word embeddings released by FastText on various languages and where to use them. Various use cases of what kind of problems can be solved using FastText in python.\r\n\r\n4. 40-45 minutes: For QA session.",
        "duration": "00:45",
        "speakers": "Kajal Puri",
        "submission_type": "Talk",
        "pyskill": "expert",
        "domains": "Artificial Intelligence, Deep Learning & Artificial Intelligence, Data Science, NLP, Machine Learning",
        "domain": "expert",
        "tweet": "Build text classification models ( CBOW and Skip-gram) with FastText in python",
        "the_speakers": [
            {
                "name": "Kajal Puri",
                "biography": "Kajal Puri is working as a Data Scientist in Fractal Analytics. Before this, she has been dabbling with numbers and statistical models through personal projects and industrial internships (All thanks to Startups!). She has trained models to make them understand human language (Natural Language Processing) and categorise objects (Computer Vision). In her spare time, when she is not reading about AI Apocalypse, she can be found writing poetry on https://www.yourquote.in/kajal-puri-cbi/quotes/",
                "homepage": "http://https://kajal-puri.github.io/",
                "@twitter": "Agirlhasnofame"
            }
        ],
        "popularity": 3,
        "slot_time": "fri-10:30",
        "slot_date": "friday 10:30",
        "slot_code": "fri-10:30-cubus",
        "room": "cubus",
        "slug": "build-text-classification-models-cbow-and-skip-gram-with-fasttext-in-python"
    },
    "fri-10:30-media": {
        "code": "QU7PLA",
        "title": "Building your own conversational AI with open source tools",
        "state": "confirmed",
        "abstract": "Point-and-click tools like Dialogflow and LUIS are great for building simple prototypes of conversational agents. But they don\u2019t scale well beyond answering simple questions. In this live-coding talk, you will learn the fundamentals of conversational AI and how to build your own that can handle more complex back-and-forth conversations using open source libraries.",
        "description": "Conversational AI is far from being a solved problem, but you don\u2019t need to rely on third-party APIs to build great chat and voice apps. \r\n\r\nIn this talk we will live-code a useful, engaging conversational AI bot based entirely on machine learning. We\u2019ll be using Rasa NLU & Rasa Core, which are open source libraries for building machine learning-based chatbots and voice assistants. We will teach our system how to hold multi-turn conversations by creating some initial training data, and then refine its behaviour by interacting with the system and providing feedback. We will cover the fundamentals of conversational AI, including the most important algorithms for intent classification, entity extraction, and dialogue management. \r\n\r\nWhat will attendees learn: \r\n\r\n- fundamentals of natural language understanding and dialogue management for building intelligent assistants. \r\n- deep dive into the open source libraries Rasa NLU and Rasa Core.\r\n- open challenges.",
        "duration": "00:45",
        "speakers": "Justina Petraityt\u0117",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Artificial Intelligence, NLP, Machine Learning, Python",
        "domain": "some",
        "tweet": "Building your own conversational AI with open source tools",
        "the_speakers": [
            {
                "name": "Justina Petraityt\u0117",
                "biography": "Justina has a background in Econometrics and Data Analytics. Her curiosity for Data Science and human behaviour analytics has taken her to many places and industries \u2013 over the past three years she has been doing Data Science work across video gaming, fintech, insurance industries. Her interest in chatbots, natural language processing and open source has led her to Rasa, a Berlin-based conversational AI startup where she works as a Developer Advocate focusing on improving developer experience in using open source software for conversational AI.",
                "homepage": "http://https://rasa.com/"
            }
        ],
        "popularity": 4,
        "slot_time": "fri-10:30",
        "slot_date": "friday 10:30",
        "slot_code": "fri-10:30-media",
        "room": "media",
        "slug": "building-your-own-conversational-ai-with-open-source-tools"
    },
    "fri-10:30-lounge": {
        "code": "NZJGSH",
        "title": "Write your Own Decorators",
        "state": "confirmed",
        "abstract": "Decorators are really useful. Using them is simple. Writing your own is bit more involved. Learn in this hands-on workshop how to write decorators for many different purposes. The emphasis is on best practices and practical examples.",
        "description": "Writing a function that takes a function and returns new, modified function requires solid knowledge how Python functions and closures work. This  hands-on workshop takes  you step-by-step through the process of writing decorators. Learn how to apply best practices from the start.\r\n\r\nYou will learn the important principles that you need to fully understand how and why a decorator works the way it does. But this training does not end there. It provides many practical examples that can be used as blueprints for your own decorators.\r\n\r\nI have been teaching this material as part of my professional Python trainer career for many years. Therefore, I know the main stumbling blocks that many participants encounter  and have refined the material and my training method accordingly.",
        "duration": "01:30",
        "speakers": "Mike M\u00fcller",
        "submission_type": "Workshop",
        "pyskill": "expert",
        "domains": "Algorithms, Programming, Python",
        "domain": "expert",
        "tweet": "Learn how to write good decorators from the expert",
        "the_speakers": [
            {
                "name": "Mike M\u00fcller",
                "biography": "Mike M\u00fcller has been using Python as his primary programming language since 1999. He is a Python trainer and the CEO at Python Academy (www.python-academy.com).\r\n\r\nHe teaches a wide variety of Python topics including \"Introduction to Python\", \"Python for Scientists and Engineers\", \"Advanced Python\" as well as \"Optimization and Extensions of Python Programs\".\r\n\r\nHe is Python Software Verband chairman, a PSF fellow, a PSF community service award holder, and User Group co-founder. He chaired EuroSciPy 2008/2009, PyCon.DE 2011/2012 and EuroPython 2014. He is an organizer of current PyCon.DE and EuroSciPy conferences.",
                "homepage": "http://https://www.python-academy.com",
                "@twitter": "@pyacademy"
            }
        ],
        "popularity": null,
        "slot_time": "fri-10:30",
        "slot_date": "friday 10:30",
        "slot_code": "fri-10:30-lounge",
        "room": "lounge",
        "slug": "write-your-own-decorators"
    },
    "fri-10:30-lecture": {
        "code": "N8MZV8",
        "title": "Concurrency in Python - concepts, frameworks and best practices",
        "state": "confirmed",
        "abstract": "This talk discusses:\r\n\r\n- concurrency concepts (e. g. atomicity, race conditions and deadlocks)\r\n\r\n- frameworks for concurrency and their use (`threading`,\r\n  `multiprocessing`, `concurrent.futures`, `asyncio`)\r\n\r\n- higher abstractions, e. g. queues and active objects\r\n\r\n- best practices for writing concurrent code",
        "description": "Have you run in situations where concurrent execution could speed up\r\nyour Python code? Are you using a GUI toolkit?\r\n\r\nThis talk gives you the background to use concurrency in your code\r\nwithout shooting yourself in the foot - which is quite easy if you\r\ndon't understand how concurrent execution differs from linear\r\nexecution!\r\n\r\nThe presentation starts with explaining some concepts like\r\nconcurrency, parallelism, resources, atomic operations, race\r\nconditions and deadlocks.\r\n\r\nThen we discuss the commonly-used approaches to concurrency:\r\nmultithreading with the `threading` module, multiprocessing with the\r\n`multiprocessing` module, and event loops (which include the\r\n`asyncio` framework). Each of these approaches has its typical use\r\ncases, which are explained.\r\n\r\nYou can implement concurrency on a number of abstraction levels. The\r\nlowest level consists of primitives like locks, events, semaphores and\r\nso on. A higher abstraction level is using queues, typically with\r\nworker threads or processes. Even higher abstraction levels are active\r\nobjects (hiding primitives or queues behind an API; this includes\r\n\"actors\" if you heard of them), the thread and process pools in\r\n`concurrent.futures` and the `asyncio` framework. Finally, you can\r\n\"outsource\" concurrency by leaving it to a message broker, which is a\r\ndistinct process that receives and distributes messages.\r\n\r\nThe talk closes with some tips and best practices, mainly:\r\n\r\n- Don't use concurrency if you don't have to.\r\n\r\n- Keep it simple. \"Simple\" usually doesn't mean using primitives like\r\n  locks, but rather using higher abstractions if you can.\r\n\r\n- Operations that look atomic may not be atomic. For example, if\r\n  descriptors are used, an \"attribute access\" may do arbitrarily\r\n  complex things. If there's any doubt, assume an operation is _not_\r\n  atomic.\r\n\r\n- Try to hide concurrency behind an API. In particular, serialize\r\n  accesses to a resource by using a single thread or process (if you\r\n  use threads and processes) for this resource.\r\n\r\n- Defects in concurrent code are often difficult to expose. If your\r\n  code _seems_ to work, it doesn't mean it will work on a different\r\n  computer, on a complex network, under high load etc. Think about\r\n  what you're doing and what could go wrong. (Of course, this applies\r\n  to coding in general, but even more so to concurrent code.)",
        "duration": "00:45",
        "speakers": "Stefan Schwarzer",
        "submission_type": "Talk",
        "pyskill": "professional",
        "domains": "Parallel Programming, Programming, Python",
        "domain": "not required",
        "tweet": "Concurrency in Python - learn about concepts, frameworks (and their use) and best practices",
        "the_speakers": [
            {
                "name": "Stefan Schwarzer",
                "biography": "Stefan has been using Python professionally for more than 15 years, the last 10+ years as a freelancing software developer and consultant (https://sschwarzer.com/en/). He has published articles on Python and given talks on Python at several conferences. He's also the maintainer of the ftputil library (https://pypi.org/project/ftputil/).",
                "homepage": "http://https://sschwarzer.com"
            }
        ],
        "popularity": 5,
        "slot_time": "fri-10:30",
        "slot_date": "friday 10:30",
        "slot_code": "fri-10:30-lecture",
        "room": "lecture",
        "slug": "concurrency-in-python-concepts-frameworks-and-best-practices"
    },
    "fri-10:30-openhub": {
        "code": "C9H7FH",
        "title": "How to develop your project from an idea to architecture design in 50 minutes",
        "state": "confirmed",
        "abstract": "Have a fresh idea that needs to be shaped? This tutorial will show how to start with a raw idea, visualize it, and then use the object-oriented approach to design the architecture.",
        "description": "Have you ever asked yourself:\u2028- how and where to start developing a new project from the scratch,\u2028- how to choose main components of a new software or how to develop a new feature,\u2028- how to decide whether your project needs refactoring\u00a0\u2028- how to avoid repeating the same issues?\u2028This talk will answer all these questions.\r\nYou will learn:\u2028- how to develop the idea from scratch,\u2028- how to develop and visualize the idea, create basic visual design without any specific knowledge,\u2028- hot to use principles of Object Oriented Analysis and Design,\u00a0\u2028- how to develop and show amazing architecture design.\r\nYou will learn to better understand where and how to start, analyze and decouple the system, create a clean, clear and extendable architecture.",
        "duration": "01:30",
        "speakers": "Anastasiia Tymoshchuk, Elena Volovicheva",
        "submission_type": "Workshop",
        "pyskill": "basic",
        "domains": "Programming",
        "domain": "some",
        "tweet": "How to develop your project from an idea to architecture design in 50 minutes",
        "the_speakers": [
            {
                "name": "Elena Volovicheva",
                "biography": "Elena Volovicheva is a Digital Designer from Ukraine, currently working in Berlin for a company named Fonpit. Through the years of hands-on experience\u00a0she made her way from an artist to digital designer, she tried herself as an entrepreneur, events host and teacher. \r\n\r\nShe believes that principles of design are reflected in the most natural aspects of our daily lives and that the design techniques should be applied also in the work of non-designers to create harmony in their workflow and vision."
            },
            {
                "name": "Anastasiia Tymoshchuk",
                "biography": "Anastasiia originally came from Ukraine, working in Berlin since April 2016. Her first Python project was - to create automation testing framework from scratch for LG Smart TV (using Python 2.7 and Django).\r\nWorking in the development for almost of 7 years (around 5 years in Python), including experience in e-commerce as well as game development. Every day she is dealing with lots of challenges when she has to consider software or library to start with, starting from the question how to build architecture and finishing with a deployment. She has also a great experience in the design and development of the project from scratch to production.  \r\n\r\nElena is a Digital Designer from Ukraine, currently working in Berlin for a company named Fonpit. Through the years of hands-on experience\u00a0she made her way from an artist to digital designer, she tried herself as an entrepreneur, events host and teacher.   \r\nShe believes that principles of design are reflected in the most natural aspects of our daily lives and that the design techniques should be applied also in the work of non-designers to create harmony in their workflow and vision.",
                "homepage": "http://atymo.me/",
                "@twitter": ""
            }
        ],
        "popularity": null,
        "slot_time": "fri-10:30",
        "slot_date": "friday 10:30",
        "slot_code": "fri-10:30-openhub",
        "room": "openhub",
        "slug": "how-to-develop-your-project-from-an-idea-to-architecture-design-in-50-minutes"
    },
    "fri-11:20-cubus": {
        "code": "8DL88C",
        "title": "Suggestions from Python and Solr",
        "state": "confirmed",
        "abstract": "Trying to guess what a user wants when she's typing something into the search box of our price comparison website is a surprisingly complex endevour. Our solution is based on the Solr SuggestComponent, heavily fortified with Python logic.",
        "description": "When a user types a query into the search box of our price comparison website, we try to figure out what they search, and provide suggestions as they type along. What product, what brand, from which categories? Solr provides a SuggestComponent that is a good start, but in a lot of situations we need fallback strategies: what should we show to a user searching for just a brand name? Or for a singular offer we can't actually show them? What alternatives can we dig up? And behind all this backfill logic lurks that dreaded question: what amount of irrelevant garbage is worse than the horror vacui of an empty result set?",
        "duration": "00:30",
        "speakers": "Jonathan Oberl\u00e4nder, Patrick Schemitz",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Algorithms",
        "domain": "some",
        "tweet": "Suggestions for price comparison search with Python and Solr",
        "the_speakers": [
            {
                "name": "Patrick Schemitz",
                "biography": "Patrick is a Senior Scientist at solute GmbH. An avid Pythonista since 2003, his main responsibility is the billiger.de search functionality, which he (co-) wrote using first Lucene, later Solr and now SolrCloud. Besides that, he wrote much of the SVM-based offer categorization at billiger.de and has a keen interest in machine learning. Patrick holds a Ph.D. in particle physics from Karlsruhe university."
            },
            {
                "name": "Jonathan Oberl\u00e4nder",
                "biography": "Jonathan started programming at the tender age of 12, after accidentally buying a book about C++. He quickly moved on to other languages (VBScript, AutoIt, PHP, Javascript, Perl, ...), but it wasn't until his Bachelor studies in Computational Linguistics (Saarland University) that he started learning Python, after having to choose between it and a Java course. Since then, he has mostly stayed true to Python, except for the occasional affair with esoteric programming languages. After finishing his Master's in Cognitive Science (Trento) and Computer Science (Prague), he started working as a full-time Python developer at the German price comparison website billiger.de",
                "@twitter": "L3viathan2142",
                "homepage": "http://"
            }
        ],
        "popularity": 3,
        "slot_time": "fri-11:20",
        "slot_date": "friday 11:20",
        "slot_code": "fri-11:20-cubus",
        "room": "cubus",
        "slug": "suggestions-from-python-and-solr"
    },
    "fri-11:20-media": {
        "code": "EC3UJW",
        "title": "Enabling the chip technologies of tomorrow \u2013 how Python helps us",
        "state": "confirmed",
        "abstract": "With the upraise of data science, more and more People from a non-programming background come to Python. We'd like to share our experience how we successfully use Python in a non-software development department.",
        "description": "Carl Zeiss SMT GmbH is the leading manufacturer of lithography optics. Our optics allow chipmakers to produce smaller, faster and more energy efficient computer chips. As we move to smaller and smaller structures, the necessary optics grow more and more complex. Customized simulations and data analytics by highly qualified technical domain experts are essential. These people are not experienced software developers. However, with Python and the right support, we can give them powerful tools to accomplish their task efficiently.\r\n\r\nPioneering Python in a larger enterprise can be challenging. At present, we use Python in selected areas of our product development and production processes. We'd like to share our challenges and solutions with using Python in a heterogeneous company environment. In particular, how can we make Python accessible to non-programmers? How do we ensure consistent development? How do we embed in the non-Python ecosystem of the company?",
        "duration": "00:30",
        "speakers": "Tim Hoffmann",
        "submission_type": "Talk",
        "pyskill": "expert",
        "domains": "Data Science",
        "domain": "some",
        "tweet": "You are using a lot of Python libraries. But have you published one? Here is how you do it.",
        "the_speakers": [
            {
                "name": "Tim Hoffmann",
                "biography": "After programming some other languages I fell in love with Python and use it professionally as well as privately nowadays. I got involved in open source to make the tools that I want to use even better."
            }
        ],
        "popularity": 3,
        "slot_time": "fri-11:20",
        "slot_date": "friday 11:20",
        "slot_code": "fri-11:20-media",
        "room": "media",
        "slug": "enabling-the-chip-technologies-of-tomorrow-how-python-helps-us"
    },
    "fri-11:20-lecture": {
        "code": "TTBMDL",
        "title": "Strongly typed datasets in a weakly typed world",
        "state": "confirmed",
        "abstract": "Strongly typed Parquet Datasets / Hive Tables are often used to exchange and preserve data in a Pandas-driven environment, where types are rather unstable. This results in multiple issues and these as well as potential solutions will be presented, together with an RFC directed to the community.",
        "description": "We at Blue Yonder use Pandas quite a lot during our daily data science and engineering work. This choice, together with Python as an underlying programming language gives us flexibility, a feature-rich interface, and access to a large community and ecosystem. When it comes to preserving the data and exchanging it with different software stacks, we rely on Parquet Datasets / Hive Tables. During the write process, there is a shift from a rather weakly typed world to a strongly typed one. For example, Pandas may convert integers to floats for many operations without asking, but parquet files and the schema information stored alongside them dictate very precise types. The type situation may get even more \"colorful\", when datasets are written by multiple code versions or different software solutions over time. This then results in important questions regarding type compatibility.\r\n\r\nThis talk will first represent an overview on types at different layers (like NumPy, Pandas, Arrow and Parquet) and the transition between this layers. The second part of the talk will present examples of type compatibility we have seen and why+how we think they should be handled. At the end there will be a Q+A, which can be seen as the start of a potentially longer RFC process to align different software stacks (like Hive and Dask) to handle types in a similar way.",
        "duration": "00:30",
        "speakers": "Marco Neumann",
        "submission_type": "Talk",
        "pyskill": "expert",
        "domains": "Algorithms, Big Data, Data Science, Parallel Programming",
        "domain": "expert",
        "tweet": "Problems+solutions for strongly typed Parquet Datasets / Hive Tables in rather weakly typed Pandas environment",
        "the_speakers": [
            {
                "name": "Marco Neumann",
                "biography": "Studied computer science at KIT (Karlsruhe, Germany), worked as a Tech Student at CERN, now a Data Scientist at Blue Yonder (Hamburg, Germany). Loves to travel and to exchange all kind of ideas.",
                "homepage": "http://https://www.blue-yonder.com/",
                "@twitter": "@crepererum"
            }
        ],
        "popularity": 4,
        "slot_time": "fri-11:20",
        "slot_date": "friday 11:20",
        "slot_code": "fri-11:20-lecture",
        "room": "lecture",
        "slug": "strongly-typed-datasets-in-a-weakly-typed-world"
    },
    "fri-11:55-cubus": {
        "code": "TTULUU",
        "title": "Satellite data is for everyone: insights into modern remote sensing research with open data and Python",
        "state": "confirmed",
        "abstract": "We give an overview about state-of-the-art land-use classification from satellite data with CNNs (in Python) based on an open dataset.",
        "description": "The largest earth observation programme Copernicus (http://copernicus.eu) makes it possible to perform terrestrial observations providing data for all kinds of purposes. One important objective is to monitor the land-use and land-cover changes with the Sentinel-2 satellite mission. These satellites measure the sun reflectance on the earth surface with multispectral cameras (13 channels between 440 nm to 2190 nm). Machine learning techniques like convolutional neural networks (CNN) are able to learn the link between the satellite image (spectrum) and the ground truth (land use class). In this talk, we give an overview about the state-of-the-art land-use classification with CNNs based on an open dataset.\r\n\r\nThe EuroSAT benchmark dataset (http://madm.dfki.de/downloads) is freely provided by German Research Center for Artificial Intelligence (DFKI). It consists of 27.000 image patches for ten different land use/cover classes, e.g. industrial and residential areas, different crop and vegetation types and forests. All samples have 64 by 64 pixel dimension and include either only the RGB images or all 13 bands. \r\n\r\nWe will use different out-of-box CNNs for the Keras deep learning library (https://keras.io/). All networks are either included in Keras itself or are available from Github repositories. We will show the process of transfer learning for the RGB datasets. Furthermore, the minimal changes required to apply commonly used CNNs to multispectral data are demonstrated. Thus, the interested audience will be able to perform their own classification of remote sensing data within a very short time. \r\nResults of different network structures are visually compared. Especially the differences of transfer learning and learning from scratch are demonstrated. This also includes the amount of necessary training epochs, progress of training and validation error and visual comparison of the results of the trained networks.\r\n\r\nFinally, we give a quick overview about the current research topics including recurrent neural networks for spatio-temporal land-use classification and further applications of multi- and hyperspectral data, e.g. for the estimation of water parameters and soil characteristics. Additionally, we provide links to the code and dataset used in this talk.",
        "duration": "00:30",
        "speakers": "Felix M. Riese, Jens Leitloff",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Artificial Intelligence, Computer Vision, Deep Learning & Artificial Intelligence, Data Science, Machine Learning, Science",
        "domain": "some",
        "tweet": "Overview about state-of-the-art land-use classification from satellite data with CNNs based on an open dataset",
        "the_speakers": [
            {
                "name": "Jens Leitloff",
                "biography": "* 1997 - 2003 Studying Geodesy at TU Berlin\r\n* 2003 - 2004 Work & Travel in Australia\r\n* 2004 - 2008 PhD at TU M\u00fcnchen\r\n* 2008 - 2011 Scientific Researcher at German Aerospace Center (DLR)\r\n* since 2011 Academic Researcher at Karlsruhe Institute of Technology (KIT)"
            },
            {
                "name": "Felix M. Riese",
                "biography": "I am a PhD student at KIT Karlsruhe with research interests in machine learning and hyperspectral remote sensing. Before that, I did my physics bachelor and particle-physics master in at the KIT. I love learning, which is why I am doing my MBA at the CDI in Paris at the same time. In my free time, I go hiking and running.",
                "homepage": "http://www.ipf.kit.edu/english/staff_riese_felix.php",
                "@twitter": "@felixriese"
            }
        ],
        "popularity": 3,
        "slot_time": "fri-11:55",
        "slot_date": "friday 11:55",
        "slot_code": "fri-11:55-cubus",
        "room": "cubus",
        "slug": "satellite-data-is-for-everyone-insights-into-modern-remote-sensing-research-with-open-data-and-python"
    },
    "fri-11:55-media": {
        "code": "3D3BDP",
        "title": "Grammar of Graphics in Python",
        "state": "confirmed",
        "abstract": "This talk will give an introduction to the concepts behind the grammar of graphics and how to use them in Python by means of the altair library and the vega specification.",
        "description": "A grammar is, according to Wikipedia, the set of structural rules governing the composition of clauses, phrases, and words in any given natural language. A grammar of graphics is then the set of structural rules governing the composition of visual elements. Transforming data into visual representations using composition is quite powerful and allows to create complex visualisations with simple building blocks. \r\n\r\nWhile the ideas behind the grammar of graphics date back well into the 80s, as a Python developer it is only quite recently that we can make use of it. Altair, backed by the vega specification, is one of the few plotting libraries in Python that provide such a declarative and compositional API. \r\n\r\nIn this talk I will give an introduction to the core concepts behind the grammar of graphics as well as practical examples how to use altair API in Python to create vega plots.",
        "duration": "00:30",
        "speakers": "Malte Harder",
        "submission_type": "Talk",
        "pyskill": "basic",
        "domains": "Visualisation",
        "domain": "some",
        "tweet": "Practical grammar of graphics in python using altair & vega",
        "the_speakers": [
            {
                "name": "Malte Harder",
                "biography": "Data Scientist at Blue Yonder doing data engineering with a passion for visualisations.",
                "homepage": "http://",
                "@twitter": ""
            }
        ],
        "popularity": 3,
        "slot_time": "fri-11:55",
        "slot_date": "friday 11:55",
        "slot_code": "fri-11:55-media",
        "room": "media",
        "slug": "grammar-of-graphics-in-python"
    },
    "fri-11:55-lecture": {
        "code": "ELAHUR",
        "title": "Prototyping to tested code",
        "state": "confirmed",
        "abstract": "Developing prototypes and their tests both in Jupyter notebooks.",
        "description": "Jupyter notebooks are a great environment to prototype solutions and explore their design. Turning these solutions into reusable components usually requires moving them out of the notebook environment into external python packages. Often, at this stage, the code is refactored and test are written.\r\n\r\nIn this talk, I will demo [`ipytest`](https://github.com/chmp/ipytest), a small tool to run tests inside notebooks. It supports [`pytest`](http://pytest.org/) as well as the standard [`unittest`](https://docs.python.org/3/library/unittest.html) framework. It allows to start prototypes in a notebook and to develop the tests with the code in an highly interactive environment. As the code grows, it can be transparently moved outside notebooks and transformed into reusable components. By bringing support for tests to the notebook environment, [`ipytest`](https://github.com/chmp/ipytest) bridges the artificial gap between notebooks and reusable components.",
        "duration": "00:15",
        "speakers": "Christopher Prohm",
        "submission_type": "Talk",
        "pyskill": "expert",
        "domains": "Data Science, Machine Learning",
        "domain": "expert",
        "tweet": "ipytest: developing prototypes and their tests directly in Jupyter notebooks",
        "the_speakers": [
            {
                "name": "Christopher Prohm",
                "biography": "I am a physicist by training and am now working as a data scientist. In my work, I focus on building robust software and putting code in production. Python has been my tool of choice for about 8 years.",
                "homepage": "http://cprohm.de",
                "@twitter": "@c_prohm"
            }
        ],
        "popularity": 3,
        "slot_time": "fri-11:55",
        "slot_date": "friday 11:55",
        "slot_code": "fri-11:55-lecture",
        "room": "lecture",
        "slug": "prototyping-to-tested-code"
    },
    "fri-14:00-cubus": {
        "code": "WESP9T",
        "title": "From exploration to deployment - combining PyTorch and TensorFlow for Deep Learning",
        "state": "confirmed",
        "abstract": "PyTorch vs. TensorFlow? Why not PyTorch + TensorFlow? Lets's combine deep learning frameworks and use their individual strengths for explorative and production-oriented tasks along a data science project.",
        "description": "Despite the many deep learning frameworks out in the wild few have achieved widespread adoption. Two of them are TensorFlow and PyTorch. Where PyTorch relies on a dynamic computation graph TensorFlow goes for a static graph. Where TensorFlow shows greater adoption and additional useful extensions with TensorFlow Serving and TensorBoard, Pytorch proves useful trough its easy and more pythonic API.\r\n\r\nData scientists are confronted with explorative challenges, but also need to be aware of model deployment and production. Do we need to single out frameworks until we end up with the only one or is there a case for joint usage of two deep learning frameworks? Can we leverage the strengths of the frameworks for different tasks along the path from exploration to production?\r\n\r\nIn my talk, I want to present a case combining the benefits of PyTorch and TensorFlow using the first for explorative and latter for deployment tasks. Therefore, I will choose a common deep learning challenge and discuss the strengths and weaknesses of both frameworks along a demo that brings a model from development into production.",
        "duration": "00:45",
        "speakers": "Marcel Kurovski",
        "submission_type": "Talk",
        "pyskill": "expert",
        "domains": "Artificial Intelligence, Deep Learning & Artificial Intelligence, Data Science, Machine Learning",
        "domain": "some",
        "tweet": "From exploration to deployment - combining PyTorch and TensorFlow for Deep Learning",
        "the_speakers": [
            {
                "name": "Marcel Kurovski",
                "biography": "Marcel Kurovski is a Data Scientist at inovex, a German IT project house focusing on\r\ndigital transformation. He earned a master's degree in Industrial Engineering and\r\nManagement from the Karlsruhe Institute of Technology (KIT) where he focused on\r\ncomputer science, machine learning and operations research.\r\n\r\nMarcel works on novel methods to exploit deep learning for recommender systems in\r\norder to better personalize content and improve user experience. He works for clients\r\nin e-commerce and retail where he bridges the gap between proof-of-concept and\r\nscalable AI systems.\r\n\r\nMarcel develops on the Python data science stack and also contributes to\r\nTensorFlow. His research spans recommender systems, deep learning as well as\r\nmethods for approximate nearest neighbor search.",
                "homepage": "http://https://www.inovex.de/en/our-services/data-science-deep-learning/",
                "@twitter": ""
            }
        ],
        "popularity": 4,
        "slot_time": "fri-14:00",
        "slot_date": "friday 14:00",
        "slot_code": "fri-14:00-cubus",
        "room": "cubus",
        "slug": "from-exploration-to-deployment-combining-pytorch-and-tensorflow-for-deep-learning"
    },
    "fri-14:00-media": {
        "code": "3EV7WB",
        "title": "Python Decorators: Gift or Poison?",
        "state": "confirmed",
        "abstract": "The talk will explain basics of Python Decorators, will show lots of examples and use cases, as well as for a beginner, as for an experienced developer.",
        "description": "Why would you ever need to use decorators in Python?\r\nHave you ever had the task when you need to use one function in few places and you really wanted to avoid of code duplicating? For example to add some logging into functions or timers, etc. Decorators in Python are super powerful with these tasks, but at the same time they are super complicated, sometimes even magical. When I started learning Python, Decorators were really like a magic: how to use them, how are they working, lots of questions. The goal is to make the things easier and clear to answer a question: to use or not to use Decorators in your project.",
        "duration": "00:30",
        "speakers": "Anastasiia Tymoshchuk",
        "submission_type": "Talk",
        "pyskill": "expert",
        "domains": "Python",
        "domain": "not required",
        "tweet": "Are Decorators in Python a gift or a poison?",
        "the_speakers": [
            {
                "name": "Anastasiia Tymoshchuk",
                "biography": "Anastasiia originally came from Ukraine, working in Berlin since April 2016. Her first Python project was - to create automation testing framework from scratch for LG Smart TV (using Python 2.7 and Django).\r\nWorking in the development for almost of 7 years (around 5 years in Python), including experience in e-commerce as well as game development. Every day she is dealing with lots of challenges when she has to consider software or library to start with, starting from the question how to build architecture and finishing with a deployment. She has also a great experience in the design and development of the project from scratch to production.  \r\n\r\nElena is a Digital Designer from Ukraine, currently working in Berlin for a company named Fonpit. Through the years of hands-on experience\u00a0she made her way from an artist to digital designer, she tried herself as an entrepreneur, events host and teacher.   \r\nShe believes that principles of design are reflected in the most natural aspects of our daily lives and that the design techniques should be applied also in the work of non-designers to create harmony in their workflow and vision.",
                "homepage": "http://atymo.me/",
                "@twitter": ""
            }
        ],
        "popularity": 3,
        "slot_time": "fri-14:00",
        "slot_date": "friday 14:00",
        "slot_code": "fri-14:00-media",
        "room": "media",
        "slug": "python-decorators-gift-or-poison"
    }
}
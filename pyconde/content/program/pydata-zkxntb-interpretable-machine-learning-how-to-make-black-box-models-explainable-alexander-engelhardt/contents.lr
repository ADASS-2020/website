_model: session 
---
code: ZKXNTB
---
title: Interpretable Machine Learning: How to make black box models explainable
---
description: In this talk, we'll find out how to interpret the predictions of otherwise black-box models.
---
twitter_image: /static/media/twitter/ZKXNTB.jpg
---
speakers: Alexander Engelhardt
---
submission_type: Talk
---
domains: Data Science • Machine Learning
---
biography: #### Alexander Engelhardt

Affiliation: Engelhardt Data Science GmbH



Statistician turned freelance data scientist, Munich based.

Caught the entrepreneurial bug. Now experimenting with product-based business and/or productized services.

[Twitter](https://twitter.com/eng_elhardt) • [Gthub](https://github.com/alexengelhardt) • [Homepage](http://www.engelhardt-ds.de)
---
affiliation: Engelhardt Data Science GmbH
---
track: PyData
---
meta_title: Interpretable Machine Learning: How to make black box models explainable @eng_elhardt #PyConDE #PyDataBerlin
---
body: We all love linear regression for its interpretability: Increase square
meters by 1, that leads to rent going up by 8 euros. A human can easily
understand why this model made a certain prediction.

Complex machine learning models like tree aggregates or neural networks
usually make better predictions, but this comes at a price: it's hard to
understand these models.

In this talk, we'll look at a few methods to pry open these models and gain
some insights 
   
Specifically, the topics covered are

- What makes a model interpretable?
  - Linear models, trees, decision rules
- The SIPA framework (Sampling, Intervention, Prediction, Aggregation) for making models interpretable again
- Model-agnostic methods for interpretability
  - Partial dependence plots (PDPs)
  - Individual conditional expectations (ICEs)
- Example-based explanations
- The future of machine learning


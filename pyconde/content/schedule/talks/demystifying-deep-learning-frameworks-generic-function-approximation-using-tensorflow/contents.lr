_model: page_markdown
---
title: Demystifying Deep Learning Frameworks : Generic Function Approximation using Tensorflow
---
body:

# Demystifying Deep Learning Frameworks : Generic Function Approximation using Tensorflow
<div class="avatar">
![](https://secure.gravatar.com/avatar/b783d342a504b68f376b5cc4893edfc9?s=500)
**[Siddhartha Rai](https://www.linkedin.com/in/siddrai/)**


Siddhartha is an innovate Data Scientist and Engineer with a strong background in managing technical teams. He is solution driven and goal focussed. He has built strong business partnerships and products in his 15 + year career in software development and research. He is passionate about using the predictive power of data to solve complex industry problem and create robust business processes. He is experienced in several machine learning techniques and algorithms and related data engineering pipelines. Siddhartha excels at analyzing copious amounts of data, communicating its significance and impact to the business, designing and developing impactful predictive models, tools and products to ensure spectacular business results. Siddhartha has led and mentored A-star teams in financial and retail sectors. He is passionate about technology and believes that current business processes can leverage and benefit significantly from the use of data.
</div>
## Abstract

*Tags:* analytics python data-science machine learning ai deep learning analytics python data-science machine learning

This talk attempts to demystify some aspects of Deep Learning using Tensorflow, a popular deep learning framework. Under the hood, Tensorflow is essentially a way to approximate functions. We show how Tensorflow can be used as a generic function approximation tool, and some approaches to tune it. 


## Description
Tensorflow as a function approximation tool
=====================================
Neural networks(and deep learning networks) are essentially tools for function approximation. A _function approximator_ is a tool that given training data X (consisting of multiple independent variables), and output data y (a single output variable), approximates the relationship between X and y as a mathematical function. In this talk, we show how Tensorflow can be used as a function approximation tool, and some approaches for tuning Tensorflow for this purpose. 

Modeling Function Approximation in Tensorflow
-------------------------------------------------------------------
Given training data X, and a output value y, we wish to learn the _best_  mathematical relationship  between X and y, The term _best_ here is a relative term, and depends on how _different_ the output from this function(y_hat) is as compared to the actual output y. In this talk we show how to use Tensorflow, a deep learning framework, to model arbitrary functions. 

For example, given a single training variable x, and an output variable y, the relationship between x and y _could_ be linear, e.g. _y + a x + b_, or it _could_ be more complex e.g. _y = sin(π x) + cos(π x/2) + sin(π x/4)  + cos (π x/8)_. One of the key goals while developing a generic function approximation model is to ensure that the model does not make any assumption on the relationship between X, and y, and instead _learns_ this relationship as part of the model training. 

Modeling Approach
-------------------------------
We use the following approach to learn the relationship between X and y in Tensorflow- 

1. We first define the structure of the deep learning network that we would like to use to model the generic relationship between X, and y. 
2. We define the activation functions that we would need to use in the model
3. We then define the criteria for measuring the difference between the predicted output and the actual output 
4. We define the approach used for solving the optimization problem defined in the above steps 
5. We compute the output of our function approximator using Tensorflow primitives

Tuning Choices 
----------------------
Each of the above stages presents several tuning choices, for example - 

1. How deep and how wide should the network be
2. Which activation function to use
3. Which optimization method to use for arriving at the best function approximation

Implementation 
-----------------------
The following is an implementation of a generic function approximator that takes inputs _in_x_, and _in_y_, and _learns_ a deep network to model the relationship between _in_x_ and _in_y_. The parameters _layer1_output, layer2_output,layer3_output_ define the structure of the deep network, and the tuning parameter _iterations_ is the number of iterations of the optimiztaion algorithm that are executed, before the output is evaluated. The optimization technique used can also be parameterized, for example,  it could be a simple _GradientDescentOptimizer_, or  _AdamOptimizer_, each of which have further tuning parameters. 

~~~~
def eval_multi_stage(layer1_output, layer2_output,layer3_output, iterations, in_x, in_y):
    in_x = np.reshape(in_x, (-1,1))
    in_y = np.reshape(in_y, (-1,1))
    sess = tf.InteractiveSession()
    x = tf.placeholder(tf.float32, shape=[None, 1], name = "x12")
    y_ = tf.placeholder(tf.float32, shape=[None, 1], name = "y12_")
    W_fc1 = weight_variable([1, layer1_output])
    b_fc1 = bias_variable([layer1_output])
    y_1 = tf.nn.relu(tf.matmul(x, W_fc1) + b_fc1)
    W_fc2 = weight_variable([layer1_output, layer2_output])
    b_fc2 = bias_variable([layer2_output])
    y_2 = tf.nn.relu(tf.matmul(y_1, W_fc2) + b_fc2)
    W_fc3 = weight_variable([layer2_output, layer3_output])
    b_fc3 = bias_variable([layer3_output])
    y_3 = tf.matmul(y_2, W_fc3) + b_f3
    cross_entropy = tf.reduce_mean(tf.square(in_y - y_3))
    train_step = tf.train.AdamOptimizer(epsilon = 1e-5, learning_rate=0.01).minimize(cross_entropy) 
    sess.run(tf.initialize_all_variables())
    for i in range(iterations):
        train_step.run(feed_dict={x:in_x, y_: in_y})
    y_hat = output.eval(feed_dict={x:in_x , y_: in_y})
    return np.reshape(y_hat, [-1])
~~~~

Results
-----------
The results of approximating a mathematical function are best described in terms of - 

1. How closely the approximated output matches the given input. 
2. The effect that the different tuning choices described above have on the quality of the approximation
3. The effect that the tuning choices have on the computation time spent before the solution converges 
 
The results from applying these functions on different types of functions are shown in the following links (please click for the plots) - 

*  <a href="https://drive.google.com/file/d/0B-oAF_WTZ7kGUG5rZExSMTVqY28/view?usp=sharing">  Cubic function approximated by a single layer neural network</a> 

The first, and the simplest approximation that we demonstrate is that of a cubic function approximated by a single stage neural network. The cubic function that captures the actual relationship in the training set is _y = (x - 2) * (x - 4) * (x-6)_. As expected, the approximation is linear (the green line). The input used for training, here is represented by the blue curve, a.k.a _Actual Data_. The results of approximating using a single layer neural network is represented by the line _Fitted Function_. The line represented by _Fitted Function_ is therefore an approximation of the above cubic function that is _learnt_ by the deep network based on the data that it was trained on. 
 
*  <a href="https://drive.google.com/file/d/0B-oAF_WTZ7kGdW1XREJDR01aOFk/view?usp=sharing"> Cubic Function Approximated by a 3 layer deep Neural Network </a> 

This curve shows the results of approximating our cubic function using a neural network that is 3 layers deep. Here we see that the blue curve of actual values is almost hidden by the approximation function - the red curve(deep learning network's oputput) of approximated values. The approximation here is done using a 3 layer network, whose first 2 stages have 32 nodes each.  This example suggests that the network defined here has done an _almost perfect_ job at learning the function using the training data that it was provided. 

* <a href="https://drive.google.com/file/d/0B-oAF_WTZ7kGZ3VNU194eFl4UUE/view?usp=sharing">Exponential Function Approximation</a>

We extend the example, by approximating an exponential function_(y = exp(x))_ using the same network as above.  The same 3 layer deep network, does a good job at approximating the exponential function as well. Here again, the number of nodes in the first 2 layers is 32. 

*  <a href="https://drive.google.com/file/d/0B-oAF_WTZ7kGaHZ1SHpwVmJzeU0/view?usp=sharing">Quassi-periodic Function Approximation (Trend)</a>  

We use the same network as in the second example, now to approximate a quassi-periodic function. The relationship in the training set is _ y = sin(π x) + cos(π x/2) + sin(π x/4)  + cos (π x/8). This quassi-periodic function here , for example, could represent a time-series function with _seasonality_ and _trend_ components - the _trend_ being defined as the overall variation across the different cycles,(or seasons) , and the _seasonality_ defined as the cyclical variation within the cycles. We notice from this plot, that the 3 layer network with 32 nodes in each of the 1st 2 layers represented by the red curve of fitted values, follows the overall trend in the actual value. We conclude therefore that the network, does a reasonably good job at capturing the _trend_. However the _seasonality_ is not captured well by this network, as there are significant differences between the blue and red curves within the cycles. 


*  <a href="https://drive.google.com/file/d/0B-oAF_WTZ7kGRXJoRlFJY1NXYm8/view?usp=sharing">Quassi-periodic Function Approximation (Seasonality - part 1)</a>  
We enrich the deep learning network used above by increasing the number of nodes in each of the 1st 2 stages from 32 to 64, and we notice that this improves the ability of the network to capture the seasonality of the data. This network captures the seasonality for more than half of the trained range. 

*  <a href="https://drive.google.com/file/d/0B-oAF_WTZ7kGTWtzaVJvOWtzQlk/view?usp=sharing">Quassi-periodic Function Approximation (Seasonality - part 2)</a>  
We extend the network further to increase the number of nodes in the 1st 2 layers from 64 to 128, and we observe that the seasonality component is captured in much better manner by the network. And finally by increasing the number of nodes in 1st 2 layers from 128 to 256, the network learns the behavior of the function in the entire trained range. The results for this are shown <a href="https://drive.google.com/file/d/0B-oAF_WTZ7kGTVJxSGlGTHpnWWM/view?usp=sharing">  here.</a>

In the last example(256 nodes in 1st 2 layers), the blue curve of actual values, is almost covered by the approximated values, except at the right boundary. This suggests that the approximation is good in the range of values for which it has been trained, and no guarantees can be made for the input values lying outside the trained range.  This is expected because in the absence of training data outside a given range, there is no reasonable way to approximate the output function.

  
We summarize that the same network that was used for approximating the cubic and exponential functions, with a very limited amount of tuning does a good job of approximating the quassi-periodic function, without any change to its other parameters. The tuning being talked about here is namely the increase in number of nodes in the first 2 layers of the deep network. Therefore the same structure of the network is capable of _leraning_ multiple types of relationships - cubic, exponential, and quassi-periodic. The last example, also suggests that more complex function require a higher number of nodes while retaining the same deep learning structure, and the network can be tuned to achieve the necessary performance by increasing the number of nodes. 


Summary 
--------------
This talk covered the following topics  -  

1. How to use Tensorflow to model an arbitrary function 
2. The model tuning choices 
3. The effect of various tuning choices on the quality and convergence of the optimization algorithm. 
4. Most importantly, this talk demonstrates that it is possible to use the same neural network to approximate different types of functions - cubic, quassi-periodic, and exponential. 

As practitioners and users of deep learning frameworks, this should give us the confidence and encouragement, that an appropriately  defined deep learning  network is capable of being a universal function approximator - i.e. even in the absence of any prior information between the relationship between the input and output, a deep learning network is capable of learning the relationship using the given training data. 



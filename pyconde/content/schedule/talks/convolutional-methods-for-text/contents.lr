_model: page_markdown
---
title: Convolutional Methods for Text
---
head_extra:

<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@pyconde" />
<meta name="twitter:title" content="Tal Perry: Convolutional Methods for Text" />
<meta name="twitter:description" content="Everyone knows the adage &#34;Good, fast, cheap: Choose two&#34;. In this talk I&#39;ll show that with convolutions we don&#39;t have to make uncomfortable choices like that. Using convolutional neural networks we can get the same results 10x faster and with a fraction of the data. " />
<meta name="twitter:image" content="https://de.pycon.org/files/logo.png" />
---
body:

# Convolutional Methods for Text
<div class="avatar">
![](https://secure.gravatar.com/avatar/bedfec06d273b18121337e837bddbba6?s=500)
**[Tal Perry]()** ([@thetalperry](http://twitter.com/thetalperry))


Hi,
I'm Tal Perry. I'm currently a data scientist at Citi where I work on applying deep learning to the analysis of financial chats and emails. 
I hold a B.Sc in mathematics from tel Aviv University, was CTO of Superfly (Alternative data for hedge funds) , founder of SmartScribe(Automated transcription) and Seventh Sigma (Algorithmic trading). I live in Berlin with my beloved partner, two dogs and a cat. 
</div>
## Abstract

*Tags:* nlp machine learning tensorflow

Everyone knows the adage "Good, fast, cheap: Choose two". In this talk I'll show that with convolutions we don't have to make uncomfortable choices like that. Using convolutional neural networks we can get the same results 10x faster and with a fraction of the data. 


## Description

Abstract 
The go to architecture for deep learning on sequences such as text is the RNN and particularly LSTM variants. While remarkably effective, RNNs are painfully slow due their sequential nature. Convolutions allow us to process a whole sequence in parallel greatly reducing the time required to train and infer by up to 9X. Convolutions can help us work much faster with large text sets both during exploration and training as well as inference. 

In this talk we'll review the key innovations in the DenseNet architecture and show how to adapt it to text. We'll go over "deconvolution" operators and dilated convolutions as means of handling long range dependencies. Finally we'll look at convolutions applied to translation  at the character level.

The goal of this talk is to demonstrate the practical advantages and relative ease with which these methods can be applied, as such we will focus on the ideas and implementations (in tensorflow) more than on the math.



